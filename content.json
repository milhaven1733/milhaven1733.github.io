[{"title":"Numpy笔记补充","date":"2017-05-07T06:39:59.000Z","path":"2017/05/07/Numpy笔记补充/","text":"创建ndarrayNumPy的empty方法可以用来创建数组（返回一些未初始化的值） 12345In [2]:np.empty((2,3))Out[2]:array([[ 0.00000000e+000, 7.48378625e-317, 6.92605451e-310], [ 6.92604876e-310, 0.00000000e+000, 0.00000000e+000]]) arange是Python内置函数range的array版： 123np.arange(15)Out[4]:array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) 数据类型可以通过ndarray的astype方法显示地转换dtype： 1234567891011121314In [6]:arr=np.arange(5)arr.dtypeOut[6]:dtype(&apos;int64&apos;)In [8]:float_arr=arr.astype(np.float64)float_arrOut[8]:array([ 0., 1., 2., 3., 4.])In [9]:float_arr.dtypeOut[9]:dtype(&apos;float64&apos;) 调用astype会创建一个新的数组。 索引、切片ndarray中的数据切片是原始数组的视图——即数据不会被复制，视图上的任何修改将直接反映到源数组上。 123456789101112In [13]:arr=np.arange(10)arr_slice=arr[5:8]arr_slice[0]=123arrOut[13]:array([ 0, 1, 2, 3, 4, 123, 6, 7, 8, 9])In [14]:arr_slice[:]=0arrOut[14]:array([0, 1, 2, 3, 4, 0, 0, 0, 8, 9]) 选取数据的子集也是视图,除非使用显式的复制操作： 12345678910111213141516In [25]:arr1d=arr2d[0]arr1d[:]=0arr2dOut[25]:array([[0, 0, 0], [4, 5, 6], [7, 8, 9]])In [27]:arr1d=arr2d[0].copy()arr1d[:]=0arr2dOut[27]:array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 布尔型索引1234567891011121314151617181920212223242526In [43]:from numpy.random import randndata=randn(7,4)dataOut[43]:array([[-0.08217433, 0.72757825, -0.17572698, 0.11375411], [ 0.14297017, 0.37876047, 1.69723883, -0.76684935], [ 0.19313528, 0.48533314, -0.55429606, 0.04060214], [ 0.72773502, -0.7296657 , -0.04213444, 0.21005464], [ 0.6785969 , 0.28481481, -0.34063399, 0.34345932], [-0.76815318, -0.60921259, 1.69758231, 0.51980305], [-1.97113938, -0.13498346, -1.79160841, 0.086466 ]])In [44]:bl=np.array([True,False,False,True,False,False,False])data[bl,2:]Out[44]:array([[-0.17572698, 0.11375411], [-0.04213444, 0.21005464]])In [45]:data[~bl,2:]Out[45]:array([[ 1.69723883, -0.76684935], [-0.55429606, 0.04060214], [-0.34063399, 0.34345932], [ 1.69758231, 0.51980305], [-1.79160841, 0.086466 ]]) Fancy index（花式索引）可以利用整数数组进行索引： 12345678910In [50]:arr=np.empty((8,4))for i in range(8): arr[i]=iarr[[4,3,0,6]]Out[50]:array([[ 4., 4., 4., 4.], [ 3., 3., 3., 3.], [ 0., 0., 0., 0.], [ 6., 6., 6., 6.]]) 使用np.ix_函数，将两个一维整数数组转换为一个用于选取矩形区域的索引器： 12345678910111213141516171819In [56]:arr=np.arange(32).reshape(8,4)arrOut[56]:array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]])In [58]:arr[np.ix_([1,5,7,2],[0,3,1,2])]Out[58]:array([[ 4, 7, 5, 6], [20, 23, 21, 22], [28, 31, 29, 30], [ 8, 11, 9, 10]]) 等同于： 1arr[[1,5,7,2]][:,[0,3,1,2]] 通用函数ufunc——一种对ndarray中数据执行元素级运算的函数 一元ufunc： 123456789101112131415161718In [19]:a=randn(10)aOut[19]:array([-0.31537852, 2.32341427, 0.56449346, 1.024455 , -0.32593332, 0.42131887, 0.25607879, -0.4078455 , 0.83566915, 0.10110313])In [20]:print np.sign(a) #求个元素正负号print np.ceil(a) #向上取整print np.floor(a) #向下取整print np.rint(a) #四舍五入print np.modf(a) #将元素的小数和整数部分以两个独立数组的形式返回[-1. 1. 1. 1. -1. 1. 1. -1. 1. 1.][-0. 3. 1. 2. -0. 1. 1. -0. 1. 1.][-1. 2. 0. 1. -1. 0. 0. -1. 0. 0.][-0. 2. 1. 1. -0. 0. 0. -0. 1. 0.](array([-0.31537852, 0.32341427, 0.56449346, 0.024455 , -0.32593332, 0.42131887, 0.25607879, -0.4078455 , 0.83566915, 0.10110313]), array([-0., 2., 0., 1., -0., 0., 0., -0., 0., 0.])) 二元ufunc： 123456789101112131415161718a=np.array([1.0,2.0,3.0])b=np.array([3,2,1])print np.add(a,b)print np.subtract(a,b)print np.multiply(a,b)print np.divide(a,b)print np.floor_divide(a,b) #整除print np.power(a,b) #元素级计算a的b次幂print np.maximum(a,b) #计算元素级最大值print np.greater(a,b) #元素级比较运算，产生布尔型数组[ 4. 4. 4.][-2. 0. 2.][ 3. 4. 3.][ 0.33333333 1. 3. ][ 0. 1. 3.][ 1. 4. 3.][ 3. 2. 3.][False False True] 比较运算函数还包括greater_equal,less,less_equal,equal,not_equal 矢量化数组运算用数组表达式替代循环 例如：在一组值上计算函数sqrt(x^2+y^2) 12points=np.arange(-5,5,0.01) #生成1000个间隔相等的点xs,ys=np.meshgrid(points,points) #接受一个一维数组，产生两个二维矩阵(对应所有（x，y）对) 如： 123456789101112131415161718192021222324points=np.arange(-5,5,1)xs,ys=np.meshgrid(points,points)print xsprint ys[[-5 -4 -3 -2 -1 0 1 2 3 4] [-5 -4 -3 -2 -1 0 1 2 3 4] [-5 -4 -3 -2 -1 0 1 2 3 4] [-5 -4 -3 -2 -1 0 1 2 3 4] [-5 -4 -3 -2 -1 0 1 2 3 4] [-5 -4 -3 -2 -1 0 1 2 3 4] [-5 -4 -3 -2 -1 0 1 2 3 4] [-5 -4 -3 -2 -1 0 1 2 3 4] [-5 -4 -3 -2 -1 0 1 2 3 4] [-5 -4 -3 -2 -1 0 1 2 3 4]][[-5 -5 -5 -5 -5 -5 -5 -5 -5 -5] [-4 -4 -4 -4 -4 -4 -4 -4 -4 -4] [-3 -3 -3 -3 -3 -3 -3 -3 -3 -3] [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [ 0 0 0 0 0 0 0 0 0 0] [ 1 1 1 1 1 1 1 1 1 1] [ 2 2 2 2 2 2 2 2 2 2] [ 3 3 3 3 3 3 3 3 3 3] [ 4 4 4 4 4 4 4 4 4 4]] 12345%matplotlib inlineimport matplotlib.pyplot as pltz=np.sqrt(xs**2+ys**2) #矢量化数组运算plt.imshow(z,cmap=plt.cm.winter) #绘制灰度图plt.colorbar() #添加colorbar 条件逻辑与数组运算np.where是三元表达式x if condition else y的矢量化版本 1234567xarr=np.array([1,2,3,4,5])yarr=np.array([6,7,8,9,0])cond=np.array([True,False,True,False,True])result=np.where(cond,xarr,yarr)resultOut：array([1, 7, 3, 9, 5]) 第二、三个参数也可以为标量值： 1234567891011121314151617181920In [101]:arrarr=randn(3,3)arrOut[101]:array([[-0.54424131, 0.62863098, 0.56891827], [ 1.34643462, 1.56380868, 0.58623083], [ 0.43735038, 0.44985456, -0.34077432]])In [102]:np.where(arr&gt;0,1,-1)Out[102]:array([[-1, 1, 1], [ 1, 1, 1], [ 1, 1, -1]])In [104]:np.where(arr&gt;0,1,arr)Out[104]:array([[-0.54424131, 1. , 1. ], [ 1. , 1. , 1. ], [ 1. , 1. , -0.34077432]]) 统计方法求元素累积和： 12345arr=np.array([[0,1,2],[3,4,5],[6,7,8]])In [115]:arr.cumsum()Out[115]:array([ 0, 1, 3, 6, 10, 15, 21, 28, 36]) 可以指定在某轴上累积： 12345678910arr.cumsum(axis=1) Out[110]:array([[ 0, 1, 3], [ 3, 7, 12], [ 6, 13, 21]]) arr.cumprod(0) #求累计积Out[114]:array([[ 0, 1, 2], [ 0, 4, 10], [ 0, 28, 80]]) 排序默认沿横轴升序排列： 12345678910111213arr=randn(3,5)arrOut[31]:array([[-0.22762911, -0.59528567, -0.44706287, -0.65593567, -1.24890433], [-0.77928573, -0.56997516, 0.37210553, 0.83965676, 0.54670066], [ 1.8469569 , 0.54329902, -1.19774885, 0.69641988, 0.43255057]])In [32]:arr.sort()arrOut[32]:array([[-1.24890433, -0.65593567, -0.59528567, -0.44706287, -0.22762911], [-0.77928573, -0.56997516, 0.37210553, 0.54670066, 0.83965676], [-1.19774885, 0.43255057, 0.54329902, 0.69641988, 1.8469569 ]]) 指定在某轴上排序： 12345678910111213141516arr=randn(3,5)arrarray([[ 0.76745725, 0.63957621, 0.54959643, -1.15530712, 1.49122891], [ 0.42202383, 0.2410772 , -0.4189283 , 0.94238709, 0.26250363], [-0.13254626, -1.25501108, -0.45683496, -1.52373513, 0.28402619]])arr.sort(0) #在纵轴上排序arrarray([[-0.13254626, -1.25501108, -0.45683496, -1.52373513, 0.26250363], [ 0.42202383, 0.2410772 , -0.4189283 , -1.15530712, 0.28402619], [ 0.76745725, 0.63957621, 0.54959643, 0.94238709, 1.49122891]])arr.sort(1) #在横轴上排序arrOut[79]:array([[-1.52373513, -1.25501108, -0.45683496, -0.13254626, 0.26250363], [-1.15530712, -0.4189283 , 0.2410772 , 0.28402619, 0.42202383], [ 0.54959643, 0.63957621, 0.76745725, 0.94238709, 1.49122891]]) 降序排列： 1234567arr.sort(0)arr=arr[::-1]arrOut[87]:array([[ 1.13964135, 0.88074008, 0.96757798, 1.42249469, 1.12209403], [-0.45274363, 0.00309309, 0.2147203 , 0.40488698, 0.17494692], [-0.81773678, -0.39659417, -0.38647118, -0.53779886, -1.46781707]]) 文件存取读取csv文本： 12345678910path=&apos;/home/milhaven1733/Desktop/Data Analyst/pydata-book-master/ch04/array_ex.txt&apos;arr=np.loadtxt(path,delimiter=&apos;,&apos;)arrOut[16]:array([[ 0.580052, 0.18673 , 1.040717, 1.134411], [ 0.194163, -0.636917, -0.938659, 0.124094], [-0.12641 , 0.268607, -0.695724, 0.047428], [-1.484413, 0.004176, -0.744203, 0.005487], [ 2.302869, 0.200131, 1.670238, -1.88109 ], [-0.19323 , 1.047233, 0.482803, 0.960334]]) 线性代数1234567x=randn(2,2)mat=x.T.dot(x) #x的转秩与x的点积运算inv(mat)mat.dot(inv(mat)) #mat点乘mat的逆Out[159]:array([[ 1., 0.], [ 0., 1.]]) 随机数与数组运算范例：随机漫步1234567%matplotlib inlineimport matplotlib.pyplot as pltnsteps=1000 draws=np.random.randint(0,2,size=nsteps) #产生1000个（0,2）之间的整数，即为0或为1steps=np.where(draws&gt;0,1,-1) #将1,0转化为1，-1walk=steps.cumsum() #计算累积和plt.plot(walk) #绘图 模拟多个随机漫步过程： 12345nwalks=100 #模拟100个随机过程nsteps=100draws=np.random.randint(0,2,size=(nwalks,nsteps))steps=np.where(draws&gt;0,1,-1)walks=steps.cumsum(1) 计算到达15或-15的最小穿越时间: 123hist=(np.abs(walks)&gt;=15).any(1) #检查每个漫步过程是否到达(-)15，返回布尔型array，1指定在每行上检查crossing_time=(np.abs(walks[hist])&gt;=15).argmax(1) #在到达指定值的漫步过程中检查第一次到达的索引值crossing_time.mean() #计算均值","tags":[]},{"title":"MySQL-学习笔记-7","date":"2017-05-06T06:54:18.000Z","path":"2017/05/06/MySQL-学习笔记-7/","text":"自定义函数关于自定义函数用户自定义函数（user-defined function UDF）是一种对MySQL扩展的途径，其用法和内置函数相同。 必要条件： 参数（可以有零个，一个、多个） 返回值（均具有） 函数可以返回任意类型的值，同样可以接受这些类型的参数 MySQL中的自定义函数最多支持1024个参数。。。 创建自定义函数： CREATEFUNCTION function_name RETURNS {STRING|INTEGER|REAL|DECIMAL} routine_body (函数体) 关于函数体： 由合法的SQL语句构成； 可以是简单的SELECT或INSERT语句； 函数体如果为复合结构则使用BEGIN…END语句； 复合结构可以包含声明、循环、控制结构； 创建不带参数的自定义函数设置客户端编码格式（方便显示汉字） 1mysql&gt; set NAMES utf8; 日期格式转化功能： 123456mysql&gt; SELECT DATE_FORMAT(NOW(),&apos;%Y年%m月%d日 %H点%i分%s秒&apos;) DATE;+-----------------------------------+| DATE |+-----------------------------------+| 2017年05月06日 17点09分27秒 |+-----------------------------------+ 将以上命令编写成函数： 12345678mysql&gt; CREATE FUNCTION f1() RETURNS VARCHAR(30) RETURN DATE_FORMAT(NOW(),&apos;%Y年%m月%d日 %H点%i分%s秒&apos;);Query OK, 0 rows affected (0.01 sec)mysql&gt; SELECT f1(); +-----------------------------------+| f1() |+-----------------------------------+| 2017年05月06日 17点07分11秒 |+-----------------------------------+ 创建带有参数的自定义函数123456789mysql&gt; CREATE FUNCTION f2(num1 SMALLINT UNSIGNED,num2 SMALLINT UNSIGNED) -&gt; RETURNS FLOAT(10,2) UNSIGNED -&gt; RETURN (num1+num2)/2;mysql&gt; SELECT f2(10,13);+-----------+| f2(10,13) |+-----------+| 11.50 |+-----------+","tags":[]},{"title":"《利用Python进行数据分析》范例数据集_babynames","date":"2017-05-04T10:34:31.000Z","path":"2017/05/04/《利用Python进行数据分析》范例数据集-babynames/","text":"《利用Python进行数据分析》范例数据集-babynames 1import pandas as pd 载入包含1880年美国新出生婴儿名字统计数据的文件 1path='/home/milhaven1733/Desktop/Data Analyst/pydata-book-master/ch02/names/yob1880.txt' 加载为DataFrame 1names1880=pd.read_csv(path,names=['names','sex','birth']) 1names1880.head() names sex birth 0 Mary F 7065 1 Anna F 2604 2 Emma F 2003 3 Elizabeth F 1939 4 Minnie F 1746 统计总出生数以‘sex’分组统计birth人数总数： 1names1880.groupby('sex').birth.sum() Outs: 1234sexF 90993M 110493Name: birth, dtype: int64 将1880-2010年所有数据文件加载至一个DataFrame 12345678910pieces=[]def get_df_pieces(year): path='/home/milhaven1733/Desktop/Data Analyst/pydata-book-master/ch02/names/yob%d.txt'%year df=pd.read_csv(path,names=['names','sex','birth']) df['year']=year pieces.append(df) return piecesyears=range(1880,2011)for year in years: get_df_pieces(year) 用pd.concat连接pieces中的每个元素： 1names=pd.concat(pieces,ignore_index=True) 1names.info() 123456789&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 1690784 entries, 0 to 1690783Data columns (total 4 columns):names 1690784 non-null objectsex 1690784 non-null objectbirth 1690784 non-null int64year 1690784 non-null int64dtypes: int64(2), object(2)memory usage: 51.6+ MB 数据聚合。利用pivot_table或groupby在year和sex级别上进行聚合 123total_birth=names.pivot_table('birth',index='year',columns='sex',aggfunc=sum)#total_birth=names.groupby(['year','sex']).birth.sum().unstack('sex').tail()total_birth.tail() sex F M year 2006 1896468 2050234 2007 1916888 2069242 2008 1883645 2032310 2009 1827643 1973359 2010 1759010 1898382 对分组统计结果绘图： 123%matplotlib inlineimport seaborn as snstotal_birth.plot(title='Total births by sex and year') 分析命名趋势添加一个prop列，表示当年某名字婴儿数对于总出生数的比例（可以先按年份和性别分组，对每个分组进行处理，再验证完整数据集的结果变化） 12345def add_prop(group): birth=group.birth.astype(float) group['prop']=birth/birth.sum() return groupnames=names.groupby(['year','sex']).apply(add_prop) 1names.head() names sex birth year prop 0 Mary F 7065 1880 0.077643 1 Anna F 2604 1880 0.028618 2 Emma F 2003 1880 0.022013 3 Elizabeth F 1939 1880 0.021309 4 Minnie F 1746 1880 0.019188 验证分组总和是否为1 123import numpy as npnp.allclose(names.groupby(['year','sex']).prop.sum(),1)#np.allclose(names.groupby(['year']).prop.sum(),2) Outs: 1True 由于数据集过大，我们可以取出每个[‘year’,’sex’]分组中总数目排在前1000的名字，作为之后分析工作的数据集： 123def get_top1000(group): return group.sort_values(by='birth',ascending=False)[:1000]top1000=names.groupby(['year','sex']).apply(get_top1000) 1top1000.info() 12345678910&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;MultiIndex: 261877 entries, (1880, F, 0) to (2010, M, 1677645)Data columns (total 5 columns):names 261877 non-null objectsex 261877 non-null objectbirth 261877 non-null int64year 261877 non-null int64prop 261877 non-null float64dtypes: float64(1), int64(2), object(2)memory usage: 13.7+ MB 按性别区分出2个子数据集： 12boys=top1000[top1000.sex=='M']girls=top1000[top1000.sex=='F'] 按照年份和名字分组统计，抽取几个名字的birth数据，绘图显示130年间该名字的使用趋势： 1total_births=top1000.pivot_table('birth',index='year',columns='names',aggfunc=sum) 1subset=total_births[['John','Harry','Mary','Marilyn']] 1subset.plot(subplots=True,figsize=(12,10),title='Number of births per year') Outs: 评估命名多样性的增长首先可以统计取最流行的1000个名字的婴儿占所有婴儿比例的变化趋势： 1table=top1000.pivot_table('prop',index='year',columns='sex',aggfunc=sum) 1table.plot(xticks=range(1880,2020,10),yticks=np.linspace(0,1.1,12)) Outs: 另外可以计算占总出生人数50%的名字的数量，先以2010年出生的男孩为例: 1df=boys[boys.year==2010] 排序后计算累积和： 1prop_cumsum=df.sort_values(by='prop',ascending=False).prop.cumsum() 1prop_cumsum.head() Outs: 1234567year sex 2010 M 1676644 0.011523 1676645 0.020934 1676646 0.029959 1676647 0.038930 1676648 0.047817Name: prop, dtype: float64 searchsorted方法求出0.5插入序列且不破坏顺序的位置： 1prop_cumsum.searchsorted(0.5) Outs: 1array([116]) 对所有组合执行计算，绘图表示趋势： 123456def get_quantile_count(group): group=group.sort_values(by='prop',ascending=False) return group.prop.cumsum().searchsorted(0.5)[0]+1diversity=top1000.groupby(['year','sex']).apply(get_quantile_count)diversity=diversity.unstack('sex')diversity.head() sex F M year 1880 38 14 1881 38 14 1882 38 15 1883 39 15 1884 39 16 1diversity.plot(title='Number of popular names in top 50%') Outs: 探究名字最后一个字母的变革首先为names添加表示名字末字母的一列，将全部出生数据进行聚合： 1234get_last_letter=lambda x:x[-1]last_letters=names.names.apply(get_last_letter)names['last_latter']=last_letterstable=names.pivot_table('birth',index='last_latter',columns=['sex','year'],aggfunc=sum) 取出有代表性的三列： 1subtable=table.reindex(columns=[1910,1960,2010],level='year') 1subtable.head() sex F M year 1910 1960 2010 1910 1960 2010 last_latter a 108376.0 691247.0 670605.0 977.0 5204.0 28438.0 b NaN 694.0 450.0 411.0 3912.0 38859.0 c 5.0 49.0 946.0 482.0 15476.0 23125.0 d 6750.0 3729.0 2607.0 22111.0 262112.0 44398.0 e 133569.0 435013.0 313833.0 28655.0 178823.0 129012.0 计算各年份末字母所占比例，绘图： 12letter_prop=subtable/subtable.sum(0)#letter_prop=subtable.div(subtable.sum(0)) 1234import matplotlib.pyplot as pltfig,axes=plt.subplots(2,1,figsize=(10,8))letter_prop['M'].plot(kind='bar',ax=axes[0],title='Male')letter_prop['F'].plot(kind='bar',ax=axes[1],title='Female') Outs: 探究一些女性化的“男性名字”的变化趋势：从全部的名字中取出包含’lesl’的集合： 1234all_names=pd.Series(top1000.names.unique())def lesley(str): return 'lesl' in str.lower()lesley_like=all_names[all_names.apply(lesley)] 从top1000中取出名字在lesley_like中的分组： 1filtered=top1000[top1000.names.isin(lesley_like)] 1filtered.groupby('names').sum().birth 1234567namesLeslee 1082Lesley 35022Lesli 929Leslie 370429Lesly 10067Name: birth, dtype: int64 按年份和性别进行聚合： 1table=filtered.pivot_table('birth',index='year',columns='sex',aggfunc='sum') 计算各年份各性别某名字的性别比例，并绘图展示变化趋势： 1table=table.div(table.sum(1),axis=0) 1table.plot(style=&#123;'M':'-','F':'--'&#125;)","tags":[]},{"title":"《利用Python进行数据分析》范例数据集_MovieLens","date":"2017-05-03T14:27:51.000Z","path":"2017/05/03/《利用Python进行数据分析》范例数据集-MovieLens/","text":"1import pandas as pd 加载.dat文件中的数据： 123unames=['user_id','gender','age','occupation','zip']path='/home/milhaven1733/Desktop/Data Analyst/pydata-book-master/ch02/movielens/users.dat'users=pd.read_table(path,sep='::',header=None,names=unames,engine = 'python') 123rnames=['user_id','movie_id','rating','timestamp']path='/home/milhaven1733/Desktop/Data Analyst/pydata-book-master/ch02/movielens/ratings.dat'ratings=pd.read_table(path,sep='::',header=None,names=rnames,engine = 'python') 123mnames=['movie_id','title','genres']path='/home/milhaven1733/Desktop/Data Analyst/pydata-book-master/ch02/movielens/movies.dat'movies=pd.read_table(path,sep='::',header=None,names=mnames,engine = 'python') 查看数据加载情况： 1users[:5] user_id gender age occupation zip 0 1 F 1 10 48067 1 2 M 56 16 70072 2 3 M 25 15 55117 3 4 M 45 7 02460 4 5 M 25 20 55455 1ratings[:5] user_id movie_id rating timestamp 0 1 1193 5 978300760 1 1 661 3 978302109 2 1 914 3 978301968 3 1 3408 4 978300275 4 1 2355 5 978824291 1movies[:5] movie_id title genres 0 1 Toy Story (1995) Animation\\ Children’s\\ Comedy 1 2 Jumanji (1995) Adventure\\ Children’s\\ Fantasy 2 3 Grumpier Old Men (1995) Comedy\\ Romance 3 4 Waiting to Exhale (1995) Comedy\\ Drama 4 5 Father of the Bride Part II (1995) Comedy 利用Pandas的merge函数合并三个表（Pandas会根据列名的重叠情况自动推断合并的键）： 1pd.merge(users,ratings).head() user_id gender age occupation zip movie_id rating timestamp 0 1 F 1 10 48067 1193 5 978300760 1 1 F 1 10 48067 661 3 978302109 2 1 F 1 10 48067 914 3 978301968 3 1 F 1 10 48067 3408 4 978300275 4 1 F 1 10 48067 2355 5 978824291 12data=pd.merge(pd.merge(users,ratings),movies)data.head() user_id gender age occupation zip movie_id rating timestamp title genres 0 1 F 1 10 48067 1193 5 978300760 One Flew Over the Cuckoo’s Nest (1975) Drama 1 2 M 56 16 70072 1193 5 978298413 One Flew Over the Cuckoo’s Nest (1975) Drama 2 12 M 25 12 32793 1193 4 978220179 One Flew Over the Cuckoo’s Nest (1975) Drama 3 15 M 25 7 22903 1193 4 978199279 One Flew Over the Cuckoo’s Nest (1975) Drama 4 17 M 50 1 95350 1193 5 978158471 One Flew Over the Cuckoo’s Nest (1975) Drama 对合并后的表进行聚合操作（数据透视）。根据电影title分类后按评分用户的性别计算每部电影的平均得分： 12mean_ratings=data.pivot_table('rating',index=['title'],columns=['gender'],aggfunc='mean')mean_ratings.head() gender F M title $1,000,000 Duck (1971) 3.375000 2.761905 ‘Night Mother (1986) 3.388889 3.352941 ‘Til There Was You (1997) 2.675676 2.733333 ‘burbs, The (1989) 2.793478 2.962085 …And Justice for All (1979) 3.828571 3.689024 过滤评分数据小于250分的电影： 根据title对评分数据分组，用size得到各组数据量的大小： 12ratings_by_title=data.groupby('title').size()ratings_by_title.head() 1234567title$1,000,000 Duck (1971) 37&apos;Night Mother (1986) 70&apos;Til There Was You (1997) 52&apos;burbs, The (1989) 303...And Justice for All (1979) 199dtype: int64 从上面的Series中分离出size&gt;250的数据： 1active_titles=ratings_by_title[ratings_by_title&gt;=250] 根据active_titles的条目的索引分离mean_ratings中评分数据多于250条的影片信息： 12active_mean_ratings=mean_ratings.ix[active_titles.index]active_mean_ratings.head() gender F M title ‘burbs, The (1989) 2.793478 2.962085 10 Things I Hate About You (1999) 3.646552 3.311966 101 Dalmatians (1961) 3.791444 3.500000 101 Dalmatians (1996) 3.240000 2.911215 12 Angry Men (1957) 4.184397 4.328421 根据’F‘列降序排序，得到女性观众评分最高电影的前十条： 12top_female_ratings=active_mean_ratings.sort_values(by='F',ascending=False)top_female_ratings.head() gender F M title Close Shave, A (1995) 4.644444 4.473795 Wrong Trousers, The (1993) 4.588235 4.478261 Sunset Blvd. (a.k.a. Sunset Boulevard) (1950) 4.572650 4.464589 Wallace &amp; Gromit: The Best of Aardman Animation (1996) 4.563107 4.385075 Schindler’s List (1993) 4.562602 4.491415 找出男女观众评分分歧最大的影片： 对active_mean_ratings添加一列’diff‘表示女性观众与男性观众对某部电影评分均值的差值： 1active_mean_ratings['diff']=active_mean_ratings['F']-active_mean_ratings['M'] 根据diff字段排序，得到差值最大的前5条： 12sort_by_diff=active_mean_ratings.sort_values(by='diff',ascending=False)sort_by_diff.head() gender F M diff title Dirty Dancing (1987) 3.790378 2.959596 0.830782 Jumpin’ Jack Flash (1986) 3.254717 2.578358 0.676359 Grease (1978) 3.975265 3.367041 0.608224 Little Women (1994) 3.870588 3.321739 0.548849 Steel Magnolias (1989) 3.901734 3.365957 0.535777 将排序结果逆序，得到男性观众评分更高的5部： 1sort_by_diff[::-1].head() gender F M diff title Good, The Bad and The Ugly, The (1966) 3.494949 4.221300 -0.726351 Kentucky Fried Movie, The (1977) 2.878788 3.555147 -0.676359 Dumb &amp; Dumber (1994) 2.697987 3.336595 -0.638608 Longest Day, The (1962) 3.411765 4.031447 -0.619682 Cable Guy, The (1996) 2.250000 2.863787 -0.613787 不考虑性别因素，仅根据评分标准差得出评分分歧最大的影片： 1rating_std_by_title=data.groupby('title')['rating'].std() 筛除评分数据小于250条的： 1rating_std_by_title=rating_std_by_title.ix[active_titles.index] 降序排序得到标准差最大的五部： 1rating_std_by_title.sort_values(ascending=False).head() 1234567titleDumb &amp; Dumber (1994) 1.321333Blair Witch Project, The (1999) 1.316368Natural Born Killers (1994) 1.307198Tank Girl (1995) 1.277695Rocky Horror Picture Show, The (1975) 1.260177Name: rating, dtype: float64","tags":[]},{"title":"《利用Python进行数据分析》范例数据集:usagov_bitly_data","date":"2017-05-03T08:56:38.000Z","path":"2017/05/03/《利用Python进行数据分析》范例数据集-usagov-bitly-data/","text":"读取生成.gov或.mil短链接的用户数据文本文件（文本中每行的为JSON） 1path='/home/milhaven1733/Desktop/Data Analyst/pydata-book-master/ch02/usagov_bitly_data2012-03-16-1331923249.txt' 1open(path).readline() Outs: 1&apos;&#123; &quot;a&quot;: &quot;Mozilla\\\\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\\\\/535.11 (KHTML, like Gecko) Chrome\\\\/17.0.963.78 Safari\\\\/535.11&quot;, &quot;c&quot;: &quot;US&quot;, &quot;nk&quot;: 1, &quot;tz&quot;: &quot;America\\\\/New_York&quot;, &quot;gr&quot;: &quot;MA&quot;, &quot;g&quot;: &quot;A6qOVH&quot;, &quot;h&quot;: &quot;wfLQtf&quot;, &quot;l&quot;: &quot;orofrog&quot;, &quot;al&quot;: &quot;en-US,en;q=0.8&quot;, &quot;hh&quot;: &quot;1.usa.gov&quot;, &quot;r&quot;: &quot;http:\\\\/\\\\/www.facebook.com\\\\/l\\\\/7AQEFzjSi\\\\/1.usa.gov\\\\/wfLQtf&quot;, &quot;u&quot;: &quot;http:\\\\/\\\\/www.ncbi.nlm.nih.gov\\\\/pubmed\\\\/22415991&quot;, &quot;t&quot;: 1331923247, &quot;hc&quot;: 1331822918, &quot;cy&quot;: &quot;Danvers&quot;, &quot;ll&quot;: [ 42.576698, -70.954903 ] &#125;\\n&apos; 用Python的JSON模块将JSON字符串转换为Python字典对象 1import json 1records=[json.loads(line) for line in open(path)] 查看list中的第一个字典元素： 1records[0] Outs: 12345678910111213141516&#123;u&apos;a&apos;: u&apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.78 Safari/535.11&apos;, u&apos;al&apos;: u&apos;en-US,en;q=0.8&apos;, u&apos;c&apos;: u&apos;US&apos;, u&apos;cy&apos;: u&apos;Danvers&apos;, u&apos;g&apos;: u&apos;A6qOVH&apos;, u&apos;gr&apos;: u&apos;MA&apos;, u&apos;h&apos;: u&apos;wfLQtf&apos;, u&apos;hc&apos;: 1331822918, u&apos;hh&apos;: u&apos;1.usa.gov&apos;, u&apos;l&apos;: u&apos;orofrog&apos;, u&apos;ll&apos;: [42.576698, -70.954903], u&apos;nk&apos;: 1, u&apos;r&apos;: u&apos;http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf&apos;, u&apos;t&apos;: 1331923247, u&apos;tz&apos;: u&apos;America/New_York&apos;, u&apos;u&apos;: u&apos;http://www.ncbi.nlm.nih.gov/pubmed/22415991&apos;&#125; 统计数据集中的时区（tz字段）信息： 用列表推导式取出tz字段： 12time_zones=[rec['tz'] for rec in records if 'tz' in rec]time_zones[:5] Outs: 12345[u&apos;America/New_York&apos;, u&apos;America/Denver&apos;, u&apos;America/New_York&apos;, u&apos;America/Sao_Paulo&apos;, u&apos;America/New_York&apos;] 利用Python标准库中的collections.Counter类，对time_zones列表进行分类计数，取出出现频次最多的十个。 123from collections import Countercounts=Counter(time_zones)counts.most_common(10) Outs: 12345678910[(u&apos;America/New_York&apos;, 1251), (u&apos;&apos;, 521), (u&apos;America/Chicago&apos;, 400), (u&apos;America/Los_Angeles&apos;, 382), (u&apos;America/Denver&apos;, 191), (u&apos;Europe/London&apos;, 74), (u&apos;Asia/Tokyo&apos;, 37), (u&apos;Pacific/Honolulu&apos;, 36), (u&apos;Europe/Madrid&apos;, 35), (u&apos;America/Sao_Paulo&apos;, 33)] 用Pandas对时区计数： 12import pandas as pdframe=pd.DataFrame(records) 1frame.head() 将tz字段取出为Series对象。利用value_counts方法进行统计： 12tz=frame['tz']tz.value_counts()[:5] Outs: 123456America/New_York 1251 521America/Chicago 400America/Los_Angeles 382America/Denver 191Name: tz, dtype: int64 替换NA值和空字符串： 1234clean_tz=frame['tz'].fillna('Missing')clean_tz[clean_tz=='']='Unknow'tz_counts=clean_tz.value_counts()[:10]tz_counts Outs: 1234567891011America/New_York 1251Unknow 521America/Chicago 400America/Los_Angeles 382America/Denver 191Missing 120Europe/London 74Asia/Tokyo 37Pacific/Honolulu 36Europe/Madrid 35Name: tz, dtype: int64 为以上整理好的数据创建一张水平条形图： 123%matplotlib inlineimport seaborn as snstz_counts.plot(kind='barh',rot=0) 通过a字段，利用agent信息对数据集进行更细致的加工： 舍弃NA值，对字符串分段，取出浏览器信息，创建一个Series： 1agent=pd.Series([x.split()[0] for x in frame.a.dropna()]) 1agent.value_counts()[:5] Outs: 123456Mozilla/5.0 2594Mozilla/4.0 601GoogleMaps/RochesterNY 121Opera/9.80 34TEST_INTERNET_AGENT 24dtype: int64 按用户是否使用Windows系统对时区信息进行分解： 首先分离出frame中a字段非空的数据： 1new_frame=frame[frame.a.notnull()] 判断a字段是否包含“Windows”： 1new_frame.a.str.contains('Windows').head() Outs: 1234560 True1 False2 True3 False4 TrueName: a, dtype: bool 添加一个os字段： 12import numpy as npnew_frame['os']=np.where(new_frame.a.str.contains('Windows'),'Windows','Not Windows') 按照‘tz’和‘os’字段分组： 1by_tz_os=new_frame.groupby(['tz','os']) 通过size对分组结果进行计数,利用unstack重塑结果，填充NA值： 1aggent=by_tz_os.size().unstack().fillna(0) 为了选取最常出现的时区，先对统计结果构造一个整理顺序（按升序排列）的索引： 1index=aggent.sum(1).argsort() 按照索引对统计结果排序，选取最后10行： 12aggent_count=aggent.take(index)[-10:]aggent_count os Not Windows Windows tz America/Sao_Paulo 13.0 20.0 Europe/Madrid 16.0 19.0 Pacific/Honolulu 0.0 36.0 Asia/Tokyo 2.0 35.0 Europe/London 43.0 31.0 America/Denver 132.0 59.0 America/Los_Angeles 130.0 252.0 America/Chicago 115.0 285.0 245.0 276.0 America/New_York 339.0 912.0 生成堆积条形图： 1aggent_count.plot(kind='bar',stacked=True) 1aggent_count.plot(kind='barh',stacked=True) 将使用widows和不使用的用户数量表示为相对比例： 1aggent_normal=aggent_count.div(aggent_count.sum(1),axis=0) 1aggent_normal os Not Windows Windows tz America/Sao_Paulo 0.393939 0.606061 Europe/Madrid 0.457143 0.542857 Pacific/Honolulu 0.000000 1.000000 Asia/Tokyo 0.054054 0.945946 Europe/London 0.581081 0.418919 America/Denver 0.691099 0.308901 America/Los_Angeles 0.340314 0.659686 America/Chicago 0.287500 0.712500 0.470250 0.529750 America/New_York 0.270983 0.729017 生成堆积条形图： 1aggent_normal.plot(kind='barh',stacked=True)","tags":[]},{"title":"A/B test概览","date":"2017-05-02T00:56:02.000Z","path":"2017/05/02/A-B-test概览/","text":"A/B tests作用通过尝试可能的改变，寻找用户更喜欢的方式，更科学的确定如何优化网站或移动应用——根据数据作出决定。 步骤： 设计任务 选择指标 分析结果 关于A/B test用于在线测试的常规方法，用来测试新产品或新功能。 何时使用A/B tests不适合使用的情境： 在线购物网站：网站是否全面、是否存在用户想购买但无法提供的商品 （测试无法确认是否提供了所有商品） 免费使用的网站考虑是否提供更高级的付费功能 （无法全面测试高级服务是否是好的商业决策） 汽车销售网站：考虑变更是否使顾客再次访问网站或向朋友推荐 （短时间内无法测试效果） 某家公司：测试改变品牌logo的效果 （改变logo对于客户冲击较大，可能需要时间适应，短时间内采集的数据意义不大） 适合使用的情境： 为用户提供电影推荐的网站：测试新的推荐排序算法 （有明确的对照组和实验组和明确的评估改变的指标） 是否改变网站基础架构的后台（可能影响页面加载速度、用户可以看到的显示结果等） （如果计算机能够同时运行两个版本，则可以对变化结果进行测试） 某移动应用：测试改变应用首页的效果 （明确的对照组和实验组、更容易选择评估改变的指标）","tags":[]},{"title":"利用matplotlib.finance 模块获取财经数据","date":"2017-04-28T16:00:59.000Z","path":"2017/04/29/利用matplotlib-finance-模块获取财经数据/","text":"从matplotlib.finance模块引入quotes_historical_yahoo_ochl（获取雅虎财经股票历史数据） datetime.today()获取当天信息，start设置为一年前的今天。 quotes_historical_yahoo_ochl()方法需要三个参数，上市公司股票代码，历史数据起始日期，终止日期。 来查询中国移动一年来的股票数据，并转化为DataFrame： 12345678from matplotlib.finance import quotes_historical_yahoo_ochlfrom datetime import *import pandas as pdtoday=datetime.today()start=(today.year-1,today.month,today.day)quotes=quotes_historical_yahoo_ochl(&apos;0941.HK&apos;,start,today)df=pd.DataFrame(quotes)print df 表格六列分别表示日期、开盘价、收盘价、最高价、最低价、成交价。","tags":[]},{"title":"Python数据统计和可视化","date":"2017-04-28T02:16:02.000Z","path":"2017/04/28/Python数据统计和可视化/","text":"数据整理日期转化 获取的某组信息中，日期的表示方法为格里高利历法(Gregorian calendar)，要转化为通常的日期表示方法，可以使用datetime.date.fromordinal()方法： 123date.fromordinal(736082)Out[19]:datetime.date(2016, 4, 28) 对于Series，可以使用apply整体转化： 1quotesdf[&apos;date&apos;]=quotesdf[&apos;date&apos;].apply(int).apply(date.fromordinal) pandas生成日期序列 生成从起始日期始的某个周期内所有日期的序列： 123456789101112import pandas as pddates=pd.date_range(&apos;20170427&apos;,periods=10)datesimport pandas as pddates=pd.date_range(&apos;20170427&apos;,periods=10)datesOut[16]:DatetimeIndex([&apos;2017-04-27&apos;, &apos;2017-04-28&apos;, &apos;2017-04-29&apos;, &apos;2017-04-30&apos;, &apos;2017-05-01&apos;, &apos;2017-05-02&apos;, &apos;2017-05-03&apos;, &apos;2017-05-04&apos;, &apos;2017-05-05&apos;, &apos;2017-05-06&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;) 数据选择选择行： 1234quotesdf.index=quotesdf[&apos;date&apos;].apply(str)quotesdf=quotesdf.drop([&apos;date&apos;],axis=1)quotesdf.loc[&apos;2017-01-06&apos;] #选择一行quotesdf[&apos;2017-01-06&apos;:&apos;2017-01-28&apos;] #选择多行 选择列： 12quotesdf[&apos;open&apos;]quotesdf.open #open为column_name 选择行和列：(loc参数，前面选择列，后面选择行） 12quotesdf.loc[&apos;2017-01-06&apos;:&apos;2017-01-28&apos;,[&apos;high&apos;,&apos;lower&apos;]]quotesdf.iloc[1:5,1:3] #通过行列位置来选择 选择单个值： 1quotesdf.at[&apos;2017-01-06&apos;,&apos;high&apos;] 数据统计与处理计算某列相邻两数值之间差值： 12np.diff(quotesdf[&apos;close&apos;])np.sign(np.diff(quotesdf[&apos;close&apos;])) #仅显示为正值或负值 排序：（根据某一列的值，ascending=1表示升序，ascending=0表示降序） 1quotesdf.sort(columns=&apos;open&apos;,ascending=1) MergeAppend1p=quotesdf[:2] open close high low volume month date 2017-01-03 62.411631 62.202897 62.461328 61.755608 20694100.0 01 2017-01-04 62.103497 61.924581 62.371870 61.745666 21340000.0 01 1q=quotesdf[&apos;2017-02-03&apos;:&apos;2017-02-07&apos;] open close high low volume month date 2017-02-03 63.117352 63.296267 63.316147 62.689943 30301800.0 02 2017-02-06 63.117352 63.256507 63.266450 62.759520 19796400.0 02 2017-02-07 63.355907 63.047773 63.395663 62.848978 20277200.0 02 1p.append(q) open close high low volume month date 2017-01-03 62.411631 62.202897 62.461328 61.755608 20694100.0 01 2017-01-04 62.103497 61.924581 62.371870 61.745666 21340000.0 01 2017-02-03 63.117352 63.296267 63.316147 62.689943 30301800.0 02 2017-02-06 63.117352 63.256507 63.266450 62.759520 19796400.0 02 2017-02-07 63.355907 63.047773 63.395663 62.848978 20277200.0 02 concat连接前三和后三个数据 12pieces=[quotesdf[:3],quotesdf[-3:]]pd.concat(pieces) open close high low volume month date 2017-01-03 62.411631 62.202897 62.461328 61.755608 20694100.0 01 2017-01-04 62.103497 61.924581 62.371870 61.745666 21340000.0 01 2017-01-05 61.815244 61.924581 62.282413 61.656208 24876000.0 01 2017-03-29 65.120003 65.470001 65.500000 64.949997 13512400.0 03 2017-03-30 65.419998 65.709999 65.980003 65.360001 15100400.0 03 2017-03-31 65.650002 65.860001 66.190002 65.449997 21001100.0 03 逻辑结构不同的也可以连接： 123quotes_without_month=quotesdf.drop(&apos;month&apos;,axis=1)pieces=[quotesdf[:3],quotes_without_month[-3:]]pd.concat(pieces) close high low month open volume date 2017-01-03 62.202897 62.461328 61.755608 01 62.411631 20694100.0 2017-01-04 61.924581 62.371870 61.745666 01 62.103497 21340000.0 2017-01-05 61.924581 62.282413 61.656208 01 61.815244 24876000.0 2017-03-29 65.470001 65.500000 64.949997 NaN 65.120003 13512400.0 2017-03-30 65.709999 65.980003 65.360001 NaN 65.419998 15100400.0 2017-03-31 65.860001 66.190002 65.449997 NaN 65.650002 21001100.0 聚类分析k-均值算法——简洁、快速、使用广泛 流程： 任意选择k个对象作为初始聚类中心 对每个点计算聚类中心点（一般采用均方差作为测度函数来计算距离） 计算每个新聚类的聚类中心，直到收敛（即确定的中心点不再变化） 要求：保证各聚类本身尽可能紧凑、各聚类之间尽可能分开 scikit-learn机器学习包、scipy.clustre等聚类算法包中都会有k-均值算法 案例： ​ 已知有一学霸的四科成绩为：100,95,100,92，同时知道另外五名同学的成绩，通过聚类分析，找出另外五名同学中的学霸和非学霸。 12345678910111213from pylab import *from scipy.cluster.vq import * #聚类分析包list1=[88.0,74,96,85] #将现有六组成绩放入6个list，注意kmeans()函数只接受浮点数list2=[92,99,95,94]list3=[91,87,99,95]list4=[78,99,97,81]list5=[88,78,98,84]list6=[100,95,100,92]data=vstack((list1,list2,list3,list4,list5,list6)) #堆积数据centroids,_=kmeans(data,2) #利用kmeans计算，第一个参数为需聚类的数据，第二个数据为聚类数目。返回含两个元素的元组，第一个为聚类中心。result,_=vq(data,centroids) #用已有数据和聚类中心进行聚类print result Out： 1[1 0 0 0 1 0] 用0,1表示两个聚类。可以看出第1,5名同学为一类，第2,3,4,6名同学为一类。 Matplotlab绘图基础 主要用于二维绘图 绘图API——pyplot模块 集成库——pylab模块（包含NumPy和pyplot中的常用函数） 折线图 1234import matplotlib.pyplot as pltt=np.arange(0.,4.,0.1)plt.plot(t,t,t,t+2,t,t**2) #3组横纵坐标值plt.show() 图像属性控制： 线条色彩和样式 通过第三个参数来指定： 12import pylab as plpl.plot(x, y,&apos;g--&apos;) #表示线条为绿色虚线 1pl.plot(x, y,&apos;rD&apos;) #红色钻石形状散点 其他样式： 其他属性： 123456789import matplotlib.pylab as plpl.figure(figsize=(8,6),dpi=100) #指定图的大小和精度t=np.arange(0.,4.,0.1)pl.plot(t,t,color=&apos;red&apos;,linestyle=&apos;-&apos;,linewidth=3,label=&apos;Line 1&apos;)#指定颜色，样式，线宽，图例pl.plot(t,t+2,color=&apos;green&apos;,linestyle=&apos;&apos;,marker=&apos;*&apos;,linewidth=3,label=&apos;Line 2&apos;)pl.plot(t,t**2,color=&apos;blue&apos;,linestyle=&apos;&apos;,marker=&apos;+&apos;,linewidth=3,label=&apos;Line 3&apos;)pl.legend(loc=&apos;upper left&apos;) #图例位置pl.show() 控制子图区域： subplot子图：三个参数表示子图分几行几列，当前在第几区域绘图（小于10逗号可省略） 123456subplot(3,1,1)pl.plot(t,t,color=&apos;red&apos;,linestyle=&apos;-&apos;,linewidth=3,label=&apos;Line 1&apos;)subplot(312)pl.plot(t,t+2,color=&apos;green&apos;,linestyle=&apos;&apos;,marker=&apos;*&apos;,linewidth=3,label=&apos;Line 2&apos;)subplot(313)pl.plot(t,t**2,color=&apos;blue&apos;,linestyle=&apos;&apos;,marker=&apos;+&apos;,linewidth=3,label=&apos;Line 3&apos;) axes子图： axes([left,bottom,width,height]) 参数分别表示离左边界，底边界距离，图像高和宽（范围0-1） 1234pl.axes([0.1,0.1,0.8,0.8])pl.plot(t,t,color=&apos;red&apos;,linestyle=&apos;-&apos;,linewidth=3,label=&apos;Line 1&apos;)pl.axes([0.5,0.15,0.3,0.3])pl.plot(t,t+2,color=&apos;green&apos;,linestyle=&apos;&apos;,marker=&apos;*&apos;,linewidth=3,label=&apos;Line 2&apos;) pandas作图Pandas可以直接对Series和DataFrame绘图 用Pandas做图之后，还可以使用plt中的函数来对参数做修改。 12345678import numpy as npimport matplotlib.pyplot as pltimport pandas as pdx = np.linspace(0, 1)y = np.sin(4 * np.pi * x) * np.exp(-5 * x)t = pd.DataFrame(y,index =x)t.plot()plt.show() 指定参数： 123456df.plot(kind=&apos;bar&apos;) #绘制柱形图df.plot(kind=&apos;barh&apos;) #绘制横向柱形图df.plot(kind=&apos;bar&apos;,stacked=True) #绘制累积柱形图df.plot(kind=&apos;scatter&apos;) #绘制散点图df.plot(x=&apos;&apos;,y=&apos;&apos;,color=&apos;&apos;) #指定横纵坐标label和颜色df.plot(kind=&apos;kde&apos;) #绘制概率分布图","tags":[]},{"title":"Series&DateFrame","date":"2017-04-27T01:12:55.000Z","path":"2017/04/27/Series-DateFrame/","text":"Series 定长有序字典 包含数据和索引 查看index和value： 123bSer=pd.Series([&apos;apple&apos;,&apos;banana&apos;,&apos;peach&apos;],index=[1,2,3])bSer.indexbSer.values 1234561 apple2 banana3 peachdtype: objectInt64Index([1, 2, 3], dtype=&apos;int64&apos;)array([&apos;apple&apos;, &apos;banana&apos;, &apos;peach&apos;], dtype=object) 对元素应用一些基本运算和函数： 1234567aSer=pd.Series([1,2.0,&apos;a&apos;])aSer*2Out:0 21 42 aadtype: object 1234567aSer=pd.Series([3,5,7])np.exp(aSer) #计算自然对数的n次方Out:0 20.0855371 148.4131592 1096.633158dtype: float64 数据对齐： 提供一些待查找的索引，在另一张存在索引和数值的表中，将要查找的索引和表中存在的索引对齐： 1234data=&#123;&apos;AXP&apos;:&apos;86.40&apos;,&apos;CSCO&apos;:&apos;122.64&apos;,&apos;BA&apos;:&apos;99.44&apos;&#125;sindex=[&apos;AXP&apos;,&apos;CSCO&apos;,&apos;BA&apos;,&apos;AAPL&apos;]aSer=pd.Series(data,index=sindex)aSer 12345AXP 86.40CSCO 122.64BA 99.44AAPL NaNdtype: object NaN (Not a Number)表示未定义或不可表示的值，这里可以理解为缺失值或控制。 检测空值： 1234567pd.isnull(aSer)Out:AXP FalseCSCO FalseBA FalseAAPL Truedtype: bool name属性： Series对象本身及其索引均有一个name属性，可以进行指定： 12345678910aSer.name=&apos;cname&apos;aSer.index.name=&apos;volumn&apos;aSerOut:volumnAXP 86.40CSCO 122.64BA 99.44AAPL NaNName: cname, dtype: object DataFrame 可以看做共享一个index的Series的集合 可以指定index 可以进行数据对齐 取Dataframe对象的行和列： 12345678910111213data=&#123;&apos;name&apos;:[&apos;XueXue&apos;,&apos;TuTu&apos;,&apos;JingJing&apos;,&apos;GaiGai&apos;],&apos;Age&apos;:[22,21,20,19]&#125;frame=pd.DataFrame(data,index=[1,2,3,4])frame.nameOut:0 XueXue1 TuTu2 JingJing3 GaiGaiName: name, dtype: objectframe.ix[2] #根据index名称取值Age 20name JingJingName: 2, dtype: object 删除一列： 12345678del frame[&apos;Age&apos;]frameOut:name1 XueXue2 TuTu3 JingJing4 GaiGai 指定name属性： 12frame.index.name=&apos;No&apos;frame Age name No 1 22 XueXue 2 21 TuTu 3 20 JingJing 4 19 GaiGai","tags":[]},{"title":"《Python数据分析实战》笔记-1","date":"2017-04-27T00:52:28.000Z","path":"2017/04/27/《Python数据分析实战》笔记-1/","text":"关于NumPy库ndarray（N维数组） 同质元素（几乎所有元素类型、大小都相同）组成 数据类型由dtype指定 大小固定（创建后大小不发生改变）","tags":[]},{"title":"MySQL-学习笔记-6","date":"2017-04-25T08:40:12.000Z","path":"2017/04/25/MySQL-学习笔记-6/","text":"运算符与函数概述按照功能划分： 字符函数 数值运算符和函数 比较运算符和函数 日期时间函数 信息函数 聚合函数 加密函数 字符函数 函数名称 描述 CONCAT() 字符连接 CONCAT_WS() 使用指定的分隔符进行字符连接 FORMAT() 数字格式化 LOWER() 转换成小写字母 UPPER() 转换成大写字母 LEFT() 获取左侧字符 RIGHT() 获取右侧字符 LENGTH() 获取字符串长度 LIRIM() 删除前导空格 RTTIM() 删除后导空格 TRIM() 删除前导和后导空格 SUBSTRING() 字符串截取 [NOT] LIKE 模式匹配 REPLACE() 字符串替换 CONCAT()连接多个字符串： 123456mysql&gt; SELECT CONCAT(&apos;test&apos;,&apos;CONCAT&apos;);+-------------------------+| CONCAT(&apos;test&apos;,&apos;CONCAT&apos;) |+-------------------------+| testCONCAT |+-------------------------+ 123456mysql&gt; SELECT CONCAT(&apos;test&apos;,&apos;-&apos;,&apos;CONCAT&apos;);+-----------------------------+| CONCAT(&apos;test&apos;,&apos;-&apos;,&apos;CONCAT&apos;) |+-----------------------------+| test-CONCAT |+-----------------------------+ 连接数据表中的字符串： 123456789101112131415161718mysql&gt; SELECT * FROM test2;+------------+-----------+| first_name | last_name |+------------+-----------+| A | B || C | D || tom% | 123 || NULL | 11 |+------------+-----------+mysql&gt; SELECT CONCAT(first_name,last_name) AS fullname FROM test2;+----------+| fullname |+----------+| AB || CD || tom%123 || NULL |+----------+ CONCAT_WS()连接字符串时使用指定字符来分隔。 123456mysql&gt; SELECT CONCAT_WS(&apos;|&apos;,&apos;A&apos;,&apos;B&apos;,&apos;C&apos;);+----------------------------+| CONCAT_WS(&apos;|&apos;,&apos;A&apos;,&apos;B&apos;,&apos;C&apos;) |+----------------------------+| A|B|C |+----------------------------+ 123456789mysql&gt; SELECT CONCAT_WS(&apos;_&apos;,first_name,last_name) AS fullname FROM test2;+----------+| fullname |+----------+| A_B || C_D || tom%_123 || 11 |+----------+ FORMAT()将数值格式化为字符型。 123456mysql&gt; SELECT FORMAT(12345.678,2);+---------------------+| FORMAT(12345.678,2) |+---------------------+| 12,345.68 |+---------------------+ 保留两位小数，千位划分。 123456mysql&gt; SELECT FORMAT(12345.678,0);+---------------------+| FORMAT(12345.678,0) |+---------------------+| 12,346 |+---------------------+ 保留整数。 LOWER()将字符串全部转化为小写 123456789101112131415mysql&gt; SELECT LOWER(&apos;MySQL&apos;);+----------------+| LOWER(&apos;MySQL&apos;) |+----------------+| mysql |+----------------+mysql&gt; SELECT LOWER(first_name) FROM test2;+-------------------+| LOWER(first_name) |+-------------------+| a || c || tom% || NULL |+-------------------+ UPPER()将字符串全部转化为大写 123456mysql&gt; SELECT UPPER(&apos;MySQL&apos;);+----------------+| UPPER(&apos;MySQL&apos;) |+----------------+| MYSQL |+----------------+ LEFT()选取字符串左侧n位字符 123456mysql&gt; SELECT LEFT(&apos;MySQL&apos;,2);+-----------------+| LEFT(&apos;MySQL&apos;,2) |+-----------------+| My |+-----------------+ RIGHT()选取字符串右侧n位字符 123456mysql&gt; SELECT RIGHT(&apos;MySQL&apos;,3);+------------------+| RIGHT(&apos;MySQL&apos;,3) |+------------------+| SQL |+------------------+ 函数嵌套使用： 123456 mysql&gt; SELECT UPPER(CONCAT(LEFT(&apos;MySQL&apos;,2),RIGHT(&apos;MySQL&apos;,3))) AS FULL;+-------+| FULL |+-------+| MYSQL |+-------+ LENGTH()返回字符串长度 123456mysql&gt; SELECT LENGTH(&apos;GHHJK%^ &amp;G &apos;);+------------------------+| LENGTH(&apos;GHHJK%^ &amp;G &apos;) |+------------------------+| 12 |+------------------------+ LTRIM()删除前导空格 123456789101112mysql&gt; SELECT LTRIM(&apos; MySQL &apos;);+----------------------+| LTRIM(&apos; MySQL &apos;) |+----------------------+| MySQL |+----------------------+mysql&gt; SELECT LENGTH(LTRIM(&apos; MySQL &apos;));+------------------------------+| LENGTH(LTRIM(&apos; MySQL &apos;)) |+------------------------------+| 8 |+------------------------------+ RTRIM()删除后导空格 123456789101112mysql&gt; SELECT RTRIM(&apos; MySQL &apos;);+----------------------+| RTRIM(&apos; MySQL &apos;) |+----------------------+| MySQL |+----------------------+mysql&gt; SELECT LENGTH(RTRIM(&apos; MySQL &apos;));+------------------------------+| LENGTH(RTRIM(&apos; MySQL &apos;)) |+------------------------------+| 8 |+------------------------------+ TRIM()删除前后导字符或特定字符： 123456789101112mysql&gt; SELECT TRIM(&apos; MySQL &apos;);+---------------------+| TRIM(&apos; MySQL &apos;) |+---------------------+| MySQL |+---------------------+mysql&gt; SELECT LENGTH(TRIM(&apos; MySQL &apos;));+-----------------------------+| LENGTH(TRIM(&apos; MySQL &apos;)) |+-----------------------------+| 5 |+-----------------------------+ 从字符串中删除前导的某字符： 123456mysql&gt; SELECT TRIM(LEADING &apos;?&apos; FROM &apos;??MySQL???&apos;);+-------------------------------------+| TRIM(LEADING &apos;?&apos; FROM &apos;??MySQL???&apos;) |+-------------------------------------+| MySQL??? |+-------------------------------------+ 从字符串中删除后导的某字符： 123456mysql&gt; SELECT TRIM(TRAILING &apos;?&apos; FROM &apos;??MySQL???&apos;);+--------------------------------------+| TRIM(TRAILING &apos;?&apos; FROM &apos;??MySQL???&apos;) |+--------------------------------------+| ??MySQL |+--------------------------------------+ 从字符串中删除前导及后导的某字符： 123456mysql&gt; SELECT TRIM(BOTH &apos;?&apos; FROM &apos;??MySQL???&apos;);+----------------------------------+| TRIM(BOTH &apos;?&apos; FROM &apos;??MySQL???&apos;) |+----------------------------------+| MySQL |+----------------------------------+ REPLACE()替换字符串中的某字符： 123456mysql&gt; SELECT REPLACE(&apos;??My???SQL???&apos;,&apos;?&apos;,&apos;&apos;);+---------------------------------+| REPLACE(&apos;??My???SQL???&apos;,&apos;?&apos;,&apos;&apos;) |+---------------------------------+| MySQL |+---------------------------------+ SUBTRING()包含三个参数：字符串、从第几位开始截取、共截取几位。 MySQL中，字符下标从1开始计数。 第二个参数可以为负，表示从倒数第几位起截取。 省略第三个参数，则截取至尾部。 123456789101112131415161718mysql&gt; SELECT SUBSTRING(&apos;MySQL&apos;,3,2);+------------------------+| SUBSTRING(&apos;MySQL&apos;,3,2) |+------------------------+| SQ |+------------------------+mysql&gt; SELECT SUBSTRING(&apos;MySQL&apos;,3);+----------------------+| SUBSTRING(&apos;MySQL&apos;,3) |+----------------------+| SQL |+----------------------+mysql&gt; SELECT SUBSTRING(&apos;MySQL&apos;,-2);+-----------------------+| SUBSTRING(&apos;MySQL&apos;,-2) |+-----------------------+| QL |+-----------------------+ [NOT]LIKE模式匹配： %表示匹配任意个字符（0,1或多个） _表示匹配一个字符 123456mysql&gt; SELECT &apos;MySQL&apos; LIKE &apos;M%&apos;;+-------------------+| &apos;MySQL&apos; LIKE &apos;M%&apos; |+-------------------+| 1 |+-------------------+ 123456mysql&gt; SELECT * FROM test2 WHERE first_name LIKE &apos;%o%&apos;;+------------+-----------+| first_name | last_name |+------------+-----------+| tom% | 123 |+------------+-----------+ 123456mysql&gt; SELECT &apos;MySQL&apos; LIKE &apos;M_S_L&apos;;+----------------------+| &apos;MySQL&apos; LIKE &apos;M_S_L&apos; |+----------------------+| 1 |+----------------------+ 123456mysql&gt; SELECT * FROM test2 WHERE first_name LIKE &apos;%\\%&apos;;+------------+-----------+| first_name | last_name |+------------+-----------+| tom% | 123 |+------------+-----------+ 也可以用任意字符来取代’\\’,使通配符表示字符本义： 123456mysql&gt; SELECT * FROM test2 WHERE first_name LIKE &apos;%1%%&apos; ESCAPE &apos;1&apos;;+------------+-----------+| first_name | last_name |+------------+-----------+| tom% | 123 |+------------+-----------+ 数值运算符和函数CEIL()向上取整 FLOOR()向下取整 ROUND()四舍五入 123456mysql&gt; SELECT ROUND(3.784,1);+----------------+| ROUND(3.784,1) |+----------------+| 3.8 |+----------------+ DIV整除 MOD取模（也可以用 ’%‘） POWER()幂运算 123456mysql&gt; SELECT POWER(3,3);+------------+| POWER(3,3) |+------------+| 27 |+------------+ TRUNCATE()数字截断 123456789101112mysql&gt; SELECT TRUNCATE(123.56,1);+--------------------+| TRUNCATE(123.56,1) |+--------------------+| 123.5 |+--------------------+mysql&gt; SELECT TRUNCATE(123.56,-1);+---------------------+| TRUNCATE(123.56,-1) |+---------------------+| 120 |+---------------------+ 比较运算符与函数[NOT] BETWEEN…AND…123456mysql&gt; SELECT 15 BETWEEN 1 AND 15;+---------------------+| 15 BETWEEN 1 AND 15 |+---------------------+| 1 |+---------------------+ [NOT] IN()123456mysql&gt; SELECT 15 IN (1,5,10);+----------------+| 15 IN (1,5,10) |+----------------+| 0 |+----------------+ IS [NOT] NULL123456mysql&gt; SELECT NULL IS NULL;+--------------+| NULL IS NULL |+--------------+| 1 |+--------------+ 123456mysql&gt; SELECT &apos;&apos; IS NULL;+------------+| &apos;&apos; IS NULL |+------------+| 0 |+------------+ 123456mysql&gt; SELECT * FROM test2 WHERE first_name IS NULL;+------------+-----------+| first_name | last_name |+------------+-----------+| NULL | 11 |+------------+-----------+ 12345678mysql&gt; SELECT * FROM test2 WHERE first_name IS NOT NULL;+------------+-----------+| first_name | last_name |+------------+-----------+| A | B || C | D || tom% | 123 |+------------+-----------+ 日期时间函数（实际开发中不常用）NOW()当前日期时间 123456mysql&gt; SELECT NOW();+---------------------+| NOW() |+---------------------+| 2017-04-26 15:41:10 |+---------------------+ CURDATE()当前日期 12345mysql&gt; SELECT CURDATE();+------------+| CURDATE() |+------------+| 2017-04-26 | CURTIME()当前时间 123456mysql&gt; SELECT CURTIME();+-----------+| CURTIME() |+-----------+| 15:41:38 |+-----------+ DATE_ADD()当前日期变化（添加或减少） 123456789101112131415161718192021222324mysql&gt; SELECT DATE_ADD(CURDATE(),INTERVAL 365 DAY);+--------------------------------------+| DATE_ADD(CURDATE(),INTERVAL 365 DAY) |+--------------------------------------+| 2018-04-26 |+--------------------------------------+mysql&gt; SELECT DATE_ADD(CURDATE(),INTERVAL -365 DAY);+---------------------------------------+| DATE_ADD(CURDATE(),INTERVAL -365 DAY) |+---------------------------------------+| 2016-04-26 |+---------------------------------------+mysql&gt; SELECT DATE_ADD(CURDATE(),INTERVAL 3 WEEK);+-------------------------------------+| DATE_ADD(CURDATE(),INTERVAL 3 WEEK) |+-------------------------------------+| 2017-05-17 |+-------------------------------------+mysql&gt; SELECT DATE_ADD(CURDATE(),INTERVAL 4 MONTH);+--------------------------------------+| DATE_ADD(CURDATE(),INTERVAL 4 MONTH) |+--------------------------------------+| 2017-08-26 |+--------------------------------------+ DATEDIFF()计算日期差值 123456mysql&gt; SELECT DATEDIFF(&apos;2017-5-12&apos;,&apos;2017-2-15&apos;);+-----------------------------------+| DATEDIFF(&apos;2017-5-12&apos;,&apos;2017-2-15&apos;) |+-----------------------------------+| 86 |+-----------------------------------+ DATE_FORMAT()日期格式化 123456mysql&gt; SELECT DATE_FORMAT(CURDATE(),&apos;%m/%d/%Y&apos;);+-----------------------------------+| DATE_FORMAT(CURDATE(),&apos;%m/%d/%Y&apos;) |+-----------------------------------+| 04/26/2017 |+-----------------------------------+ 信息函数CONNECTION_ID()当前连接MySQL服务的线程ID 12345mysql&gt; SELECT CONNECTION_ID();+-----------------+| CONNECTION_ID() |+-----------------+| 3 | DATABASE()当前数据库 123456mysql&gt; SELECT DATABASE();+------------+| DATABASE() |+------------+| lcyDB |+------------+ USER()当前用户 123456mysql&gt; SELECT USER();+----------------+| USER() |+----------------+| root@localhost |+----------------+ VERSION()版本信息 123456mysql&gt; SELECT VERSION();+-----------+| VERSION() |+-----------+| 5.7.15-1 |+-----------+ LAST_INSERT_ID() 最后插入记录的ID号 先在表中加入ID字段： 12mysql&gt; ALTER TABLE test2 ADD -&gt; id SMALLINT UNSIGNED PRIMARY KEY AUTO_INCREMENT FIRST; 添加一条新纪录： 1mysql&gt; INSERT test2 VALUES(DEFAULT,&apos;Lee&apos;,&apos;CY&apos;); 查看最新插入记录的ID： 123456mysql&gt; SELECT LAST_INSERT_ID();+------------------+| LAST_INSERT_ID() |+------------------+| 5 |+------------------ 同时插入多条，返回新插入的第一条ID 1234567mysql&gt; INSERT test2 VALUES(DEFAULT,&apos;Lee&apos;,&apos;CY&apos;),(DEFAULT,&apos;YYY&apos;,&apos;FFFF&apos;);mysql&gt; SELECT LAST_INSERT_ID();+------------------+| LAST_INSERT_ID() |+------------------+| 6 |+------------------+ 聚合函数特点：用于数据表中的数据，仅有一个返回值。 AVG()均值 123456mysql&gt; SELECT ROUND(AVG(goods_price),2) FROM tdb_goods;+---------------------------+| ROUND(AVG(goods_price),2) |+---------------------------+| 5845.10 |+---------------------------+ COUNT()计数 123456mysql&gt; SELECT COUNT(goods_id) FROM tdb_goods;+-----------------+| COUNT(goods_id) |+-----------------+| 20 |+-----------------+ MAX()最大值 123456mysql&gt; SELECT MAX(goods_price) FROM tdb_goods;+------------------+| MAX(goods_price) |+------------------+| 28888.000 |+------------------+ MIN()最小值 SUM()求和 加密函数MD5()信息摘要算法 123456mysql&gt; SELECT MD5(&apos;MySQL&apos;);+----------------------------------+| MD5(&apos;MySQL&apos;) |+----------------------------------+| 62a004b95946bb97541afa471dcca73a |+----------------------------------+ 多用于Web页面。 PASSWORD()密码算法 （可以用于修改当前用户密码） 1mysql&gt; SET PASSWORD=PASSWORD(&apos;root&apos;);","tags":[]},{"title":"针对泰坦尼克号乘客数据集的数据分析","date":"2017-04-25T05:24:55.000Z","path":"2017/04/25/针对泰坦尼克号乘客数据集的数据分析/","text":"针对泰坦尼克号乘客数据集的数据分析在这个项目中，我们将针对泰坦尼克号乘客数据集展开分析，对于乘客的某些信息进行一些描述性的统计分析，并简要分析乘客的某些因素与生还率之间是否具有相关性或具有怎样程度的相关性。 导入和熟悉数据首先，导入分析所需的Python库并载入数据文件。 1234567import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inlineTitanic_data=pd.read_csv('/home/milhaven1733/titanic-data.csv') 载入数据后，先来查看几行数据，熟悉一下数据集中包含哪些字段，分别是什么数据类型以及表示哪方面的信息。 查看DataFrame的前5行，大致了解包含哪些字段: 1Titanic_data.head() PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th… female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 查看字段数据类型，以及哪些字段存在缺失: 1Titanic_data.info() 1234567891011121314151617&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId 891 non-null int64Survived 891 non-null int64Pclass 891 non-null int64Name 891 non-null objectSex 891 non-null objectAge 714 non-null float64SibSp 891 non-null int64Parch 891 non-null int64Ticket 891 non-null objectFare 891 non-null float64Cabin 204 non-null objectEmbarked 889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.6+ KB 有信息登记的乘客总数：891数据集共包含12个字段： 字段 含义 数据类型 缺失数量 PassengerId 乘客ID int64 无 Survived 是否幸存（1表示幸存） int64 无 Pclass 船票等级（在1/2/3等舱） int64 无 Name 乘客姓名 object 无 Sex 乘客性别 object 无 Age 乘客年龄 float64 277 SibSp 船上兄弟姐妹/配偶的数量 int64 无 Parch 船上父母/子女的数量 int64 无 Ticket 船票信息 object 无 Fare 票价 float64 无 Cabin 舱位信息 object 687 Embarked 登船港口 object 2 清理数据“Age”字段存在较多缺失值，为了方便之后的统计与计算，我们可以使用年龄的平均值来填充缺失值的字段 1234passenger_age=Titanic_data[\"Age\"]print passenger_age.describe()Titanic_data['Age'].fillna(passenger_age.mean(),inplace=True)print passenger_age.describe() #用均值来填充，Age的部分描述统计量不发生变化 123456789101112131415161718count 714.000000mean 29.699118std 14.526497min 0.42000025% 20.12500050% 28.00000075% 38.000000max 80.000000Name: Age, dtype: float64count 891.000000mean 29.699118std 13.002015min 0.42000025% 22.00000050% 29.69911875% 35.000000max 80.000000Name: Age, dtype: float64 提出问题对于数据集的内容、结构有了一定的了解，我们可以尝试提出一些感兴趣的问题，进行探索和分析： 有多少乘客获救幸存，占比多少？ 乘客的性别、年龄、船票等级、票价，船上家人数量呈现怎样的分布？ 乘客的性别、年龄、船票等级、票价、船上家人数量与是否幸存存在相关性吗？存在怎样的相关性、相关程度如何？ 数据探索问题一：有多少乘客获救幸存，占比多少？1Titanic_data.groupby(['Survived'])['PassengerId'].count() 1234Survived0 5491 342Name: PassengerId, dtype: int64 可以看出，有342名乘客获救幸存，549名乘客未能生还。 用图形化表示以上数据。先创建条形图，可以看出未获救的乘客数量明显多于幸存的乘客数量： 123456def paint_bar(data,title,x_ticklabel): ax=plt.gca() bar=data.plot(kind='bar') ax.set_title(title) ax.set_xticklabels(x_ticklabel,rotation=1)paint_bar(Titanic_data.groupby(['Survived'])['PassengerId'].count(),\"Bar of survived or not\",[\"Not Survived\",\"Survived\"]) 用饼状图可以更清晰地显示（未）幸存乘客占比： 1234567def plt_pie(title,data,label): plt.axis('equal') plt.title(title) plt.pie(data,labels=label,autopct='%1.1f%%') plt.show()var=Titanic_data.groupby(['Survived']).count()plt_pie('survived or not',var['PassengerId'],[\"Not Survived\",\"Survived\"]) 问题二：乘客的性别、年龄、船票等级、票价、船上家人数量呈现怎样的分布？首先对乘客性别比例进行分析： 1234passenger_sex=Titanic_data['Sex']data=passenger_sex.value_counts()print dataplt_pie('passenger_sex',data,['male','female']) 123male 577female 314Name: Sex, dtype: int64 可以看出，大概三分之二的乘客为男性。 接下来查看乘客船票等级的分布： 1234passenger_Pclass=Titanic_data['Pclass']data=passenger_Pclass.value_counts()print dataplt_pie('passenger_Pclass',data,['3 Class','1 Class','2 Class']) 12343 4911 2162 184Name: Pclass, dtype: int64 可以看出，过半的乘客购买了三等船票，接近四分之一的乘客是一等票，其余五分之一的乘客为二等票。 接下来查看票价的分布： 123456def paint_hist(data,title,bin): fig,ax = plt.subplots() data.hist(bins=bin) ax.set_title(title)passenger_fare=Titanic_data[\"Fare\"]paint_hist(passenger_fare,\"Hist of Fare\",20) 1Titanic_data[\"Fare\"][Titanic_data[\"Fare\"]&gt;200].count() 120 绝大部分乘客的票价在0-50之间(总体半数以上票价在25以内)，也有极少量的乘客购买200以上的高价船票（20人左右） 对乘客年龄进行分析： 1paint_hist(passenger_age,\"Hist of Age\",40) 由于用均值填充了缺失值的字段，导致均值所在范围高度比较异常。暂考虑在外。除了0-4岁乘客数量稍多以外，乘客的年龄分布大体呈现正态分布，均值在30岁左右 对家人在船上的数量进行分析： 12passenger_family=Titanic_data['SibSp']+Titanic_data['Parch']paint_hist(passenger_family,\"Hist of Family\",21) 可以看出，大部分(60%左右)乘客没有家人在船上。 12passenger_has_family=passenger_family[passenger_family&gt;0]paint_hist(passenger_has_family,\"Hist of Family\",19) 对有家人在船上的乘客进行分析，大部分（大约80%）乘客有1-3名家人在船上。 问题三：乘客的性别、年龄、船票等级、票价、船上家人数量与是否幸存存在相关性吗？存在怎样的相关性、相关程度如何？1）分析乘客性别与幸存率的关系： 首先来看幸存乘客中的性别占比： 1Survived_Passenger=Titanic_data[Titanic_data.Survived==1] #幸存乘客信息DataFrame 12Survived_sex=Survived_Passenger.groupby(['Sex'])['PassengerId'].count()plt_pie('sex_of_survival',Survived_sex,['Female','Male']) 幸存者中，女性占2/3、男性占1/3左右。 再来看男性乘客和女性乘客群体 各自的幸存比例： 1234#print Titanic_data.groupby(['Sex']).count()Survived_by_sex=Titanic_data.groupby(['Sex'])['Survived']print Survived_by_sex.count()print Survived_by_sex.sum() 12345678Sexfemale 314male 577Name: Survived, dtype: int64Sexfemale 233male 109Name: Survived, dtype: int64 用柱状图体现： 12sex_scale=Survived_by_sex.sum()/Survived_by_sex.count()paint_bar(sex_scale,\"Bar of survived by sex\",[\"Female\",\"Male\"]) 可以看出，船上的女乘客有70%以上获救，但幸存的男乘客不足20%.因此幸存率与性别有相关性。 2） 分析乘客船票等级与幸存率的关系： 首先来看幸存乘客中的各个等级的占比： 12Survived_Pclass=Survived_Passenger.groupby(['Pclass'])['PassengerId'].count()plt_pie('Pclass_of_survival',Survived_Pclass,['1 Class','2 Class','3 Class']) 可以看出，幸存的乘客中，买了一等票的人占比最多。 再来看，购买不同等级船票人群各自的幸存率： 1234#Titanic_data.groupby(['Pclass']).count()Survived_by_Pclass=Titanic_data.groupby(['Pclass'])['Survived']Pclass_scale=Survived_by_Pclass.sum()/Survived_by_Pclass.count()paint_bar(Pclass_scale,\"Bar of survived by Pclass\",['1 Class','2 Class','3 Class']) 由图表可以发现，购买一等票的乘客幸存率最高，二等次之，三等最低。说明幸存率和船票等级有一定的相关性。 3)分析幸存率与船上家人数量之间的相关性： 1Titanic_data['Family']=Titanic_data['SibSp']+Titanic_data['Parch'] #在DataFrame中添加一列表示家人数量 123#Titanic_data.groupby(['Family']).count()Family_scale=Titanic_data.groupby(['Family'])['Survived'].sum()/Titanic_data.groupby(['Family'])['Survived'].count()paint_bar(Family_scale,\"Bar of survived by Family\",[0,1,2,3,4,5,6,7,10]) 可以看出，当有1-3位家人同时在船上时，幸存的几率会更高些。（但也可能是有更多家人在船上的乘客基数比较小，比较幸存比例没有太大的意义。） 4）分析幸存率与票价之间的相关性： 由于票价种类数目较大，用柱状图表达幸存率效果会较差，所以我们改用箱线图来展示幸存与未幸存群体的票价分布： 1Titanic_data.boxplot(column='Fare',by='Survived') 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3430852050&gt; 从上面箱线图中可以看出，可能有一些非常大的异常值对绘图效果造成了影响，可以通过设置忽略outlier来使图形获得更好效果: 1Titanic_data.boxplot(column='Fare',by='Survived',showfliers=False) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3430959cd0&gt; 从箱线图可以看出：未幸存的乘客中，50%分布于票价非常低的群体，75%分布于票价低于30的群体，只有约25%票价处于(30,60)区间。而幸存的乘客，接近50%处于(30,120)票价区间。可以得出，买低价票的乘客获救幸存的几率更低。换言之，购买高价船票的乘客可能更容易幸存。 由于存在较大异常值，所以不适合对票价分组划分进行统计分析。 5)分析幸存率与年龄之间的相关性： Age是连续变量，如果能转化为分类变量，统计和分析的难度就会降低很多。所以我们可以为乘客信息添加一个新的字段，记录乘客所在的“年龄段”： 1Titanic_data['Age'].describe() 123456789count 891.000000mean 29.699118std 13.002015min 0.42000025% 22.00000050% 29.69911875% 35.000000max 80.000000Name: Age, dtype: float64 最小年龄为0.42,最大年龄为80,我们可以以8岁为一个区间划分十个年龄段： 123Titanic_data['Age_group']=pd.cut(Titanic_data['Age'],np.arange(0,81,8))print Titanic_data['Age'].head()Titanic_data['Age_group'].head() 12345678910111213140 22.01 38.02 26.03 35.04 35.0Name: Age, dtype: float64Out[32]:0 (16, 24]1 (32, 40]2 (24, 32]3 (32, 40]4 (32, 40]Name: Age_group, dtype: categoryCategories (10, object): [(0, 8] &lt; (8, 16] &lt; (16, 24] &lt; (24, 32] ... (48, 56] &lt; (56, 64] &lt; (64, 72] &lt; (72, 80]] 查看头几行，证实划分正确。 再来分析各个年龄段的幸存率： 123Survived_by_Age=Titanic_data.groupby(['Age_group'])['Survived']Age_scale=Survived_by_Age.sum()/Survived_by_Age.count()Age_scale.plot(kind=\"bar\") 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3430bc15d0&gt; 从图表可以看出，0-8岁年龄层幸存率最高，72-80年龄次之。其余年龄层差异不大。 得出结论通过上面对于数据的一些探索，针对我们之前提出的问题，我们大体可以做出如下的结论： 有３８％左右乘客幸存 乘客中男性占三分之二、三等舱乘客占半数以上、大部分乘客在船上没有同行的家人，年龄呈现正态分布，均值在３０岁左右 性别、票价（舱位）等级、年龄，家人等因素都与幸存率有相关性。其中性别对幸存率影响最大（女性幸存率远高于男性），船票的等级对于是否幸存也有较大影响（级别、票价越高的舱位，幸存率也会越高）。同时当在船上有１－３个家人时，乘客的幸存率也会有所提高。最后，年龄也是幸存率的一个影响因素，小孩，老人获救的几率相对更高。 分析的局限性由于： 没有全部乘客的信息，现有信息的一些字段缺失 没有从统计学意义上对分析的结果加以验证结果是否具有偶然性 可能还有一些隐含因素对幸存率有影响但信息中未给出 分析结论几乎全部由对整理数据所得图表推断得出，因而分析具有很大的局限性，结论只是暂时的。 参考资料 Python数据可视化：Matplotlib 直方图、箱线图、条形图、热图、折线图、散点图 python__matplotlib画直方图 python中plt.hist参数详解 Python–matplotlib 绘图可视化练手–折线图/条形图 Titanic数据分析报告（python）","tags":[]},{"title":"使用NumPy和Pandas分析二维数据","date":"2017-04-23T00:34:45.000Z","path":"2017/04/23/使用NumPy和Pandas分析二维数据/","text":"课程内容概述 了解NumPy和Pandas更多特征 用两种库分析二维数据 数据准备及提出问题从nyc-subway-weather.csv下载纽约地铁客流量与天气数据。针对数据集提出一些问题，如： 哪些变量与地铁客流量相关？ 哪个车站客流量最多？各车间之间有什么差异？ 客流量模式（如客流高峰期时段，工作日和周末的差别等 天气对客流量有什么影响？ 单独研究天气数据，如一段时间内气温走势，不同城市间天气差异等。 NumPy和Pandas中的二维数据二维数据的表示： Python: list of lists NumPy: 2-Dimensional array （更简要，容易理解） Pandas: Data Frame (功能更多) 创建一个 2D arrays和创建一个 array of arrays的区别： 2D arrays更节省内存 获取元素语法不同，2D arrays使用l类似a[1,3]来获取元素而非a[1][3],且可以将行、列或两者都表示为slice切片 mean、std等函数，可以用于整个2D arrays 2D arrays slice: 12345678910111213141516import numpy as npridership = np.array([ [ 0, 0, 2, 5, 0], [1478, 3877, 3674, 2328, 2539], [1613, 4088, 3991, 6461, 2691], [1560, 3392, 3826, 4787, 2613], [1608, 4802, 3932, 4477, 2705], [1576, 3933, 3909, 4979, 2685], [ 95, 229, 255, 496, 201], [ 2, 0, 1, 27, 0], [1438, 3785, 3589, 4174, 2215], [1342, 4043, 4009, 4665, 3033]])print ridership[1, 3]print ridership[1:3, 3:5]print ridership[1, :] Out: 12342328[[2328 2539] [6461 2691]][1478 3877 3674 2328 2539] 行列向量运算： 12345print ridership[0, :] + ridership[1, :]print ridership[:, 0] + ridership[:, 1]Out:[1478 3877 3676 2333 2539][ 0 5355 5701 4952 6410 5509 324 2 5223 5385] array的向量运算： 1234567a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])b = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])print a + bOut:[[ 2 3 4] [ 6 7 8] [10 11 12]] 练习：用2D arrays表示不同日期、车站的地铁客流量。编写函数，使其能够找出第一天客流量最多的车站，并找出这个车站每天的平均乘客数。同时，返回各个车站每天的平均乘客数，进行比较。 12345678def mean_riders_for_max_station(ridership): fist_day_data=ridership[0:1,:] sta=fist_day_data.argmax() all_data=ridership[:,sta] overall_mean = ridership.mean() mean_for_max = all_data.mean() return (overall_mean, mean_for_max)mean_riders_for_max_station(ridership) 分析得出，第一天客流量多的车站，平均客流量也高于所有车站客流量的平均值。 NumPy Axis（轴）应用Axis，可以以行或列为单位进行运算。 2D array的axis数值通常为0或1 axis=0，在每行中进行运算；axis=1，在每列中进行运算； 如： 123456789101112a = np.array([ [1, 2, 3], [4, 5, 6], [7, 8, 9]])print a.sum()print a.sum(axis=0)print a.sum(axis=1)Out:45[12 15 18][ 6 15 24] 练习：找出每一个地铁站所有日期内的客流均值，以及每个地铁站客流量均值中的最高和最低值。 123station_riders=ridership.mean(axis=0)print station_riders.max() print station_riders.min() 关于NumPy和Pandas的数据类型NumPy 2D array具有一个dtype ，数组中每一个元素都应该属于同一类型。 故2D array不便于表现csv文件的内容，如果创建一个包含字符型数据的array 则array中所有的元素将被转换为字符串，而无法进行计算。 Pandas DataFrame——Pandas中的二维数据结构 优势： 每一列可以为不同的数值类型 拥有index （类似Pandas理论，每行都有一个索引值，每列都有一个名称） 在创建DataFrame时，可以直接输入一个字典，使列名称映射该列的数值列表。 如： 123456789import pandas as pdenrollments_df=pd.DataFrame(&#123; &apos;account_key&apos;:[448,448,448,448,448], &apos;status&apos;:[&apos;canceled&apos;,&apos;canceled&apos;,&apos;canceled&apos;,&apos;canceled&apos;,&apos;current&apos;], &apos;join_data&apos;:[&apos;2014-11-10&apos;,&apos;2014-11-5&apos;,&apos;2015-1-27 &apos;,&apos;2014-11-10&apos;,&apos;2015-3-10&apos;], &apos;days_to_cancel&apos;:[65,5,0,0,np.nan], &apos;is_udacity&apos;:[True,True,True,True,True]&#125;)enrollments_df account_key days_to_cancel is_udacity join_data status 0 448 65.0 True 2014-11-10 canceled 1 448 5.0 True 2014-11-5 canceled 2 448 0.0 True 2015-1-27 canceled 3 448 0.0 True 2014-11-10 canceled 4 448 NaN True 2015-3-10 current DataFrame.mean() 计算每一列的平均值（仅计算数值、布尔值而忽略非数值），如： 123456enrollments_df.mean()Out:account_key 448.0days_to_cancel 17.5is_udacity 1.0dtype: float64 以指定index和column名称的方式创建一个DataFrame： 123456789101112131415ridership_df = pd.DataFrame( data=[[ 0, 0, 2, 5, 0], [1478, 3877, 3674, 2328, 2539], [1613, 4088, 3991, 6461, 2691], [1560, 3392, 3826, 4787, 2613], [1608, 4802, 3932, 4477, 2705], [1576, 3933, 3909, 4979, 2685], [ 95, 229, 255, 496, 201], [ 2, 0, 1, 27, 0], [1438, 3785, 3589, 4174, 2215], [1342, 4043, 4009, 4665, 3033]], index=[&apos;05-01-11&apos;, &apos;05-02-11&apos;, &apos;05-03-11&apos;, &apos;05-04-11&apos;, &apos;05-05-11&apos;, &apos;05-06-11&apos;, &apos;05-07-11&apos;, &apos;05-08-11&apos;, &apos;05-09-11&apos;, &apos;05-10-11&apos;], columns=[&apos;R003&apos;, &apos;R004&apos;, &apos;R005&apos;, &apos;R006&apos;, &apos;R007&apos;]) R003 R004 R005 R006 R007 05-01-11 0 0 2 5 0 05-02-11 1478 3877 3674 2328 2539 05-03-11 1613 4088 3991 6461 2691 05-04-11 1560 3392 3826 4787 2613 05-05-11 1608 4802 3932 4477 2705 05-06-11 1576 3933 3909 4979 2685 05-07-11 95 229 255 496 201 05-08-11 2 0 1 27 0 05-09-11 1438 3785 3589 4174 2215 05-10-11 1342 4043 4009 4665 3033 从DataFrame中选取一行数据： 12ridership_df.iloc[0]ridership_df.loc[&apos;05-05-11&apos;] 从DataFrame中选取一列数据： 1ridership_df[&apos;R003&apos;] 从DataFrame中选取特定位置的某个数据： 1ridership_df.iloc[1, 3] 数据切片展示： 12ridership_df.iloc[1:4]ridership_df[[&apos;R003&apos;, &apos;R005&apos;]] DataFrame.sum() 计算每一行的和（默认axis=0） DataFrame.sum(axis=1) 计算每一列的和 如果要计算所有值的和（均值、标准差等），需要通过DataFrame.values 转化为array后计算，如： 12345678910111213df = pd.DataFrame(&#123;&apos;A&apos;: [0, 1, 2], &apos;B&apos;: [3, 4, 5]&#125;)print df.sum()print df.sum(axis=1)print df.values.sum()Out:A 3B 12dtype: int640 31 52 7dtype: int6415 练习：使用DataFrame重写第一个练习中的函数找出第一天客流量最多的车站，和这个车站每天的平均乘客数。以及所有车站的平均乘客数。 123456def mean_riders_for_max_station(ridership): sta=ridership.iloc[0].argmax() overall_mean = ridership.values.mean() mean_for_max = ridership[sta].mean() return (overall_mean, mean_for_max)mean_riders_for_max_station(ridership_df) 将数据加载到DataFrameDataFrame可以有效表示csv文件内容。 所以可以使用Pandas的read_csv 函数将csv文件加载为DataFrame。 如： 1subway_df=pd.read_csv(&apos;/home/milhaven1733/nyc-subway-weather.csv&apos;) 用DataFrame.head() 函数打印前几行： 1subway_df.head(3) 使用DataFrame.describe() 查看每一列的统计数据 1subway_df.describe() 计算x相关系数皮尔逊积矩相关系数 (Pearson Correlation Coefficient，又称Pearson’s r）用来衡量两组变量之间的相关程度。 计算步骤： 将每组变量标准化（归一化，每个数值减去均值再除以标准偏差） 计算每组标准化后的变量的乘积 计算乘积的均值r 即 r=average of （x in std units）*（y in std units） 作用： 标准化之后，二维空间被分为四个象限。 一三象限表示两个值均高于或低于平均值，说明一个变量随另一个变量增加而增加，乘积为正——正相关关系 二四象限表示一个值高于、一个值低于平均值，说明一个变量随另一个变量增加而减少，乘积为负——负相关关系 所以，r为正，表示大部分点位于一三象限，两组变量为正相关，反之为负相关。 r的范围：[-1,1],越接近零说明相关程度越小，绝对值越接近1，说明相关程度越大。 注意： 在标准化两变量时，必须使用未更正的标准偏差，即必须加入参数ddof=0 默认情况下，Pandas 的 std() 函数使用贝塞耳校正系数来计算标准偏差。调用 std(ddof=0) 可以禁止使用贝塞耳校正系数。 练习：编写函数，计算两变量的相关系数 12345def correlation(x, y): x_sd=(x-x.mean())/x.std(ddof=0) y_sd=(y-y.mean())/y.std(ddof=0) mul_sd=x_sd*y_sd return mul_sd.mean() 实际上，使用NumPy的corrcoef() 函数可以直接计算两变量的相关系数。 这个网站 可以通过移动滑块查看不同相关系数的数据模型。 Pandas Axis Name在Pandas中，可以用axis=&#39;index&#39; axis=&#39;columns&#39; 来表示axis 但两个参数值容易混淆。 当使用axis=&#39;index&#39; 时，以索引列为轴，计算的是每一行的某统计量。 当使用axis=&#39;columns&#39; 时，以列名称这一行为轴，计算的是每一列的某统计量。 DataFrame向量运算和偏移量DataFrame支持向量运算。 以column_name和index作为向量配对条件，如： 12345678df1 = pd.DataFrame(&#123;&apos;a&apos;: [1, 2, 3], &apos;b&apos;: [4, 5, 6], &apos;c&apos;: [7, 8, 9]&#125;)df2 = pd.DataFrame(&#123;&apos;d&apos;: [10, 20, 30], &apos;c&apos;: [40, 50, 60], &apos;b&apos;: [70, 80, 90]&#125;)print df1 + df2Outs: a b c d0 NaN 74 47 NaN1 NaN 85 58 NaN2 NaN 96 69 NaN 1234567891011df1 = pd.DataFrame(&#123;&apos;a&apos;: [1, 2, 3], &apos;b&apos;: [4, 5, 6], &apos;c&apos;: [7, 8, 9]&#125;, index=[&apos;row1&apos;, &apos;row2&apos;, &apos;row3&apos;])df2 = pd.DataFrame(&#123;&apos;a&apos;: [10, 20, 30], &apos;b&apos;: [40, 50, 60], &apos;c&apos;: [70, 80, 90]&#125;, index=[&apos;row4&apos;, &apos;row3&apos;, &apos;row2&apos;])print df1 + df2Outs: a b crow1 NaN NaN NaNrow2 32.0 65.0 98.0row3 23.0 56.0 89.0row4 NaN NaN NaN 练习：现在有某地铁站运营截止到每个时段的总进站和出站人数的数据集，由此计算每个时段内的进站和出站人数。 tips：DataFrame.shift() 函数——按照偏移量移动索引值 DataFrame.shift(periods=1, freq=None, axis=0) periods : int Number of periods to move, can be positive or negative 示例： 12345678910111213df1 = pd.DataFrame(&#123;&apos;a&apos;: [1, 2, 3], &apos;b&apos;: [4, 5, 6], &apos;c&apos;: [7, 8, 9]&#125;, index=[&apos;row1&apos;, &apos;row2&apos;, &apos;row3&apos;])print df1print df1.shift(1)Out: a b crow1 1 4 7row2 2 5 8row3 3 6 9 a b crow1 NaN NaN NaNrow2 1.0 4.0 7.0row3 2.0 5.0 8.0 以index为轴偏移一个单位。 所以，以原始DataFrame减去偏移一个单位的DataFrame，可以得到每小时内进出站的人数： 12def get_hourly_entries_and_exits(entries_and_exits): return (entries_and_exits-entries_and_exits.shift(1)).dropna() #可以舍弃NaN值 DataFrame.applymap()与series类似，DataFrame支持apply函数，对DataFrame中的每一个数值调用apply中的函数，得到一个新的DataFrame。如将数字成绩转化为字母等级： 1234567891011121314151617181920grades_df = pd.DataFrame( data=&#123;&apos;exam1&apos;: [43, 81, 78, 75, 89, 70, 91, 65, 98, 87], &apos;exam2&apos;: [24, 63, 56, 56, 67, 51, 79, 46, 72, 60]&#125;, index=[&apos;Andre&apos;, &apos;Barry&apos;, &apos;Chris&apos;, &apos;Dan&apos;, &apos;Emilio&apos;, &apos;Fred&apos;, &apos;Greta&apos;, &apos;Humbert&apos;, &apos;Ivan&apos;, &apos;James&apos;])def convert_grade(grades): if grades&gt;=90: return &apos;A&apos; elif grades&gt;=80: return &apos;B&apos; elif grades&gt;=70: return &apos;C&apos; elif grades&gt;=60: return &apos;D&apos; else: return &apos;F&apos;def convert_grades(grades): return grades.applymap(convert_grade)convert_grades(grades_df) DataFrame.apply()在DataFrame中，apply() 和applymap() 调用原理不同。 apply()在调用时，相当于将DataFrame的每一列单独作为一个series，调用针对这个series的函数进行运算。 即调用的函数针对于DataFrame的每一列运算并返回一个新的DataFrame列。 如：若字母等级成绩取决于某个成绩在这一科所有成绩中的占比，那么直接应用applymap(),对DataFrame中每个元素运算就无法完成转化，必须以每一列为对象进行运算： 123456def convert_grades_curve(exam_grades): return pd.qcut(exam_grades, [0, 0.1, 0.2, 0.5, 0.8, 1], labels=[&apos;F&apos;, &apos;D&apos;, &apos;C&apos;, &apos;B&apos;, &apos;A&apos;]) convert_grades_curve(grades_df[&apos;exam1&apos;]) #对某一列调用convert_grades_curve函数grades_df.apply(convert_grades_curve) #对DataFrame的每一列调用convert_grades_curve函数 pandas.qcut() 函数： pandas.qcut(x, q, labels=None, retbins=False, precision=3) x : ndarray or Series q : integer or array of quantiles Number of quantiles. 10 for deciles, 4 for quartiles, etc. Alternately array of quantiles, e.g. [0, .25, .5, .75, 1.] for quartiles labels : array or boolean, default None Used as labels for the resulting bins. Must be of the same length as the resulting bins. If False, return only integer indicators of the bins. 即利用分位数划分Series，返回数值位于哪个区间上，并可以使用labels对区间进行逐个命名。 练习：对DataFrame的每一列进行标准化。 123def standardize(df): return (df-df.mean())/df.std(ddof = 0) #不使用校正系数print grades_df.apply(standardize) DataFrame.apply()还可以对每一列调用函数 ，最后将每一列的结果生成一个series返回，如： 1234567df = pd.DataFrame(&#123; &apos;a&apos;: [4, 5, 3, 1, 2], &apos;b&apos;: [20, 10, 40, 50, 30], &apos;c&apos;: [25, 20, 5, 15, 10]&#125;)print df.apply(np.mean)print df.apply(np.max) Out: 12345678a 3.0b 30.0c 15.0dtype: float64a 5b 50c 25dtype: int64 练习：找出一个DataFrame中，每列第二大的数值。 123456def second_largest_in_column(column): sort_column=column.sort_values(ascending=False) #对column降序排列 return sort_column.iloc[1] #返回第二大的值def second_largest(df): return df.apply(second_largest_in_column) #对DataFrame的每一列调用函数second_largest(df) DataFrame与series 相加Pandas中，DataFrame可以和Series相加。 列数相同： 12345s = pd.Series([1, 2, 3, 4])df = pd.DataFrame(&#123;0: [10], 1: [20], 2: [30], 3: [40]&#125;)print dfprint &apos;&apos; print df + s Out: 12345 0 1 2 30 10 20 30 40 0 1 2 30 11 22 33 44 列数不同： 12345s = pd.Series([1, 2, 3, 4])df = pd.DataFrame(&#123;0: [10, 20, 30, 40]&#125;)print dfprint &apos;&apos; print df + s Out: 1234567891011 00 101 202 303 40 0 1 2 30 11 NaN NaN NaN1 21 NaN NaN NaN2 31 NaN NaN NaN3 41 NaN NaN NaN 索引与默认索引相同： 12345678s = pd.Series([1, 2, 3, 4])df = pd.DataFrame(&#123; 0: [10, 20, 30, 40], 1: [50, 60, 70, 80], 2: [90, 100, 110, 120], 3: [130, 140, 150, 160]&#125;)print df + s Out: 12345 0 1 2 30 11 52 93 1341 21 62 103 1442 31 72 113 1543 41 82 123 164 指定索引相同： 12345678s = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])df = pd.DataFrame(&#123; &apos;a&apos;: [10, 20, 30, 40], &apos;b&apos;: [50, 60, 70, 80], &apos;c&apos;: [90, 100, 110, 120], &apos;d&apos;: [130, 140, 150, 160]&#125;)print df + s Out: 12345 a b c d0 11 52 93 1341 21 62 103 1442 31 72 113 1543 41 82 123 164 索引不同： 12345678s = pd.Series([1, 2, 3, 4])df = pd.DataFrame(&#123; &apos;a&apos;: [10, 20, 30, 40], &apos;b&apos;: [50, 60, 70, 80], &apos;c&apos;: [90, 100, 110, 120], &apos;d&apos;: [130, 140, 150, 160]&#125;)print df + s Out： 12345 0 1 2 3 a b c d0 NaN NaN NaN NaN NaN NaN NaN NaN1 NaN NaN NaN NaN NaN NaN NaN NaN2 NaN NaN NaN NaN NaN NaN NaN NaN3 NaN NaN NaN NaN NaN NaN NaN NaN 标准化行运用向量运算对DataFrame的每一行进行标准化： 创建DataFrame: 123456grades_df = pd.DataFrame( data=&#123;&apos;exam1&apos;: [43, 81, 78, 75, 89, 70, 91, 65, 98, 87], &apos;exam2&apos;: [24, 63, 56, 56, 67, 51, 79, 46, 72, 60]&#125;, index=[&apos;Andre&apos;, &apos;Barry&apos;, &apos;Chris&apos;, &apos;Dan&apos;, &apos;Emilio&apos;, &apos;Fred&apos;, &apos;Greta&apos;, &apos;Humbert&apos;, &apos;Ivan&apos;, &apos;James&apos;]) 设置axis=&#39;columns&#39; ,计算每一行的均值： 1df_mean=grades_df.mean(axis=&apos;columns&apos;) 12345678910Andre 33.5Barry 72.0Chris 67.0Dan 65.5Emilio 78.0Fred 60.5Greta 85.0Humbert 55.5Ivan 85.0James 73.5 同理，计算每一行的标准偏差： 1df_std=grades_df.std(ddof=0,axis=&apos;columns&apos;) 12345678910Andre 9.5Barry 9.0Chris 11.0Dan 9.5Emilio 11.0Fred 9.5Greta 6.0Humbert 9.5Ivan 13.0James 13.5 但由于grades_df的column与df_mean、df_std 的不同，不能直接相减或相除，必须在运算时指定axis=&#39;index&#39;,即在每一行上进行运算： 12df_diffs=grades_df.sub(df_mean,axis=&apos;index&apos;)print df_diffs.div(df_std,axis=&apos;index&apos;) 1234567891011 exam1 exam2Andre 1.0 -1.0Barry 1.0 -1.0Chris 1.0 -1.0Dan 1.0 -1.0Emilio 1.0 -1.0Fred 1.0 -1.0Greta 1.0 -1.0Humbert 1.0 -1.0Ivan 1.0 -1.0James 1.0 -1.0 得到每一行的标准化量。 Pandas groupby()DataFrame具有分组功能。 如可以将地铁客流量数据按照”hour”字段分组，计算分组后每组数据的均值，在对各个均值进行分析。 通过一个实例来查看分组统计的步骤： 首先创建一个DataFrame： 12345678enrollments_df=pd.DataFrame(&#123; &apos;account_key&apos;:[&apos;1200&apos;,&apos;1200&apos;,&apos;1200&apos;,&apos;1200&apos;,&apos;1200&apos;,&apos;1200&apos;,&apos;1200&apos;, &apos;1175&apos;,&apos;1175&apos;,&apos;1175&apos;,&apos;1175&apos;,&apos;1175&apos;,&apos;1175&apos;,&apos;1175&apos;], &apos;utc_date&apos;:[&apos;2015-03-04&apos;,&apos;2015-03-05&apos;,&apos;2015-03-06&apos;,&apos;2015-03-07&apos;,&apos;2015-03-08&apos;, &apos;2015-03-09&apos;,&apos;2015-03-10&apos;,&apos;2015-04-02&apos;,&apos;2015-04-03&apos;,&apos;2015-04-04&apos;,&apos;2015-04-05&apos;, &apos;2015-04-06&apos;,&apos;2015-04-07&apos;,&apos;2015-04-08&apos;], &apos;total_minutes_visited&apos;:[114.9,43.4,187.8,150.1,19.6,0,8.8,2.7,0,0,0,0,0,0]&#125;) 按照某个字段来分组： 创建了一个特殊的自定义对象： 123enrollments_df.groupby(&apos;account_key&apos;)Out:&lt;pandas.core.groupby.DataFrameGroupBy object at 0x7f924f702510&gt; 接下来，对创建的groupby对象使用.sum()函数 注意这里的sum是groupby对象自身的函数，groupby具有很多内置函数，也支持apply 1enrollments_df.groupby(&apos;account_key&apos;).sum() 返回一个列表： total_minutes_visited account_key 1175 2.7 1200 524.6 由于’utc_date’字段无法计算总和，故结果不含该列。 最后，选取结果中的一列： 1enrollments_df.groupby(&apos;account_key&apos;).sum()[&apos;total_minutes_visited&apos;] 返回一个series： 1234account_key1175 2.71200 524.6Name: total_minutes_visited, dtype: float64 groupby示例二： 1234567891011121314values = np.array([1, 3, 2, 4, 1, 6, 4])example_df = pd.DataFrame(&#123; &apos;value&apos;: values, &apos;even&apos;: values % 2 == 0, &apos;above_three&apos;: values &gt; 3 &#125;, index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;, &apos;g&apos;])print example_dfprint &apos;&apos;grouped_data = example_df.groupby(&apos;even&apos;)print grouped_data.sum()print &apos;&apos;print grouped_data.sum()[&apos;value&apos;]print &apos;&apos;print grouped_data[&apos;value&apos;].sum() Out: 1234567891011121314151617181920212223 above_three even valuea False False 1b False False 3c False True 2d True True 4e False False 1f True True 6g True True 4 above_three valueeven False 0.0 5True 3.0 16evenFalse 5True 16Name: value, dtype: int64evenFalse 5True 16Name: value, dtype: int64 练习：以某个统计量对地铁数据分组，求分组后各组，各数值统计量的均值，选取其中一个有具体意义的进行绘图 如：以日期为单位（字段：”day_week”） 分组，计算其与统计量均值： 12subway_df.groupby(&apos;day_week&apos;)subway_df.groupby(&apos;day_week&apos;).mean() 选取’ENTRIESn_hourly’字段查看每小时进入地铁站的人数均值： 1ridership_weekday=subway_df.groupby(&apos;hour&apos;).mean()[&apos;ENTRIESn_hourly&apos;] 得到以下结果： 123456789day_week0 1825.2649071 2164.8364332 2297.0979573 2317.0723794 2277.3722945 1383.9014796 1066.436106Name: ENTRIESn_hourly, dtype: float64 绘图： 123%pylab inlineimport seaborn as snsridership_weekday.plot() 周末客流量小于平日，图像比较合理。 groupby apply()对每个分组内的某一列值调用指定函数。 如： 12345def second_largest(xs): sorted_xs = xs.sort_values(inplace=False, ascending=False) return sorted_xs.iloc[1]grouped_data = example_df.groupby(&apos;even&apos;)print grouped_data[&apos;value&apos;].apply(second_largest) 分组后，False组的value包含[1,3,1],True组包含[2,4,6,4],分别对这两组自定义对象调用 second_largest函数： 1234evenFalse 1True 4Name: value, dtype: int64 又如： 1234def standardize(xs): return (xs - xs.mean()) / xs.std()grouped_data = example_df.groupby(&apos;even&apos;)print grouped_data[&apos;value&apos;].apply(standardize) 通过’even’分组，在每组的’value’字段上调用standardize()进行组内数据的标准化 练习：对地铁数据按照站台分组，使用apply()调用之前所写的get_hourly_entries_and_exits函数，计算每个时段内的出站和进站人数。 1ridership_df.groupby(&apos;UNIT&apos;)[[&apos;ENTRIESn&apos;,&apos;EXITSn&apos;]].apply(get_hourly_entries_and_exits) 用’UNIT’分组，指定仅对于[‘ENTRIESn’,’EXITSn’]两个字段的分组值调用get_hourly_entries_and_exits()函数 DataFrame merge()两个不同的DataFrame可以进行合并操作，类似于SQL中的JOIN操作。 语法：DF1.merge(DF2,on=””,how=””) on=””,指定进行匹配的字段，相当于SQL的连接条件 how=””指定连接类型，可选值有”inner”,”left”,”right”,相当于SQL的内，左外，右外连接。 使用外连接时，没有相应数据的字段显示为NaN 练习，合并地铁和天气数据。（要求仅保存两个DataFrame中有相同记录字段的行） 123456789101112131415161718192021222324252627282930subway_df = pd.DataFrame(&#123; &apos;UNIT&apos;: [&apos;R003&apos;, &apos;R003&apos;, &apos;R003&apos;, &apos;R003&apos;, &apos;R003&apos;, &apos;R004&apos;, &apos;R004&apos;, &apos;R004&apos;, &apos;R004&apos;, &apos;R004&apos;], &apos;DATEn&apos;: [&apos;05-01-11&apos;, &apos;05-02-11&apos;, &apos;05-03-11&apos;, &apos;05-04-11&apos;, &apos;05-05-11&apos;, &apos;05-01-11&apos;, &apos;05-02-11&apos;, &apos;05-03-11&apos;, &apos;05-04-11&apos;, &apos;05-05-11&apos;], &apos;hour&apos;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &apos;ENTRIESn&apos;: [ 4388333, 4388348, 4389885, 4391507, 4393043, 14656120, 14656174, 14660126, 14664247, 14668301], &apos;EXITSn&apos;: [ 2911002, 2911036, 2912127, 2913223, 2914284, 14451774, 14451851, 14454734, 14457780, 14460818], &apos;latitude&apos;: [ 40.689945, 40.689945, 40.689945, 40.689945, 40.689945, 40.69132 , 40.69132 , 40.69132 , 40.69132 , 40.69132 ], &apos;longitude&apos;: [-73.872564, -73.872564, -73.872564, -73.872564, -73.872564, -73.867135, -73.867135, -73.867135, -73.867135, -73.867135]&#125;)weather_df = pd.DataFrame(&#123; &apos;DATEn&apos;: [&apos;05-01-11&apos;, &apos;05-01-11&apos;, &apos;05-02-11&apos;, &apos;05-02-11&apos;, &apos;05-03-11&apos;, &apos;05-03-11&apos;, &apos;05-04-11&apos;, &apos;05-04-11&apos;, &apos;05-05-11&apos;, &apos;05-05-11&apos;], &apos;hour&apos;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &apos;latitude&apos;: [ 40.689945, 40.69132 , 40.689945, 40.69132 , 40.689945, 40.69132 , 40.689945, 40.69132 , 40.689945, 40.69132 ], &apos;longitude&apos;: [-73.872564, -73.867135, -73.872564, -73.867135, -73.872564, -73.867135, -73.872564, -73.867135, -73.872564, -73.867135], &apos;pressurei&apos;: [ 30.24, 30.24, 30.32, 30.32, 30.14, 30.14, 29.98, 29.98, 30.01, 30.01], &apos;fog&apos;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &apos;rain&apos;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &apos;tempi&apos;: [ 52. , 52. , 48.9, 48.9, 54. , 54. , 57.2, 57.2, 48.9, 48.9], &apos;wspdi&apos;: [ 8.1, 8.1, 6.9, 6.9, 3.5, 3.5, 15. , 15. , 15. , 15. ]&#125;) 首先，查看两个DataFrame的结构： 1subway_df.head() DATEn ENTRIESn EXITSn UNIT hour latitude longitude 0 05-01-11 4388333 2911002 R003 0 40.689945 -73.872564 1 05-02-11 4388348 2911036 R003 0 40.689945 -73.872564 2 05-03-11 4389885 2912127 R003 0 40.689945 -73.872564 3 05-04-11 4391507 2913223 R003 0 40.689945 -73.872564 4 05-05-11 4393043 2914284 R003 0 40.689945 -73.872564 1weather_df.head() DATEn fog hour latitude longitude pressurei rain tempi wspdi 0 05-01-11 0 0 40.689945 -73.872564 30.24 0 52.0 8.1 1 05-01-11 0 0 40.691320 -73.867135 30.24 0 52.0 8.1 2 05-02-11 0 0 40.689945 -73.872564 30.32 0 48.9 6.9 3 05-02-11 0 0 40.691320 -73.867135 30.32 0 48.9 6.9 4 05-03-11 0 0 40.689945 -73.872564 30.14 0 54.0 3.5 可以进行连接的字段有’DATEn’,’hour’,’latitude’,’longitude’，所以进行连接： 123subway_df.merge(weather_df, on=[&apos;DATEn&apos;,&apos;hour&apos;,&apos;latitude&apos;,&apos;longitude&apos;], how=&quot;inner&quot;) 若两DataFrame中，内容相同的字段column名称不同，可以分别注明left_on=””;right_on=””. 如：（需匹配的字段相对应） 1234subway_df.merge(weather_df, left_on=[&apos;DATEn&apos;,&apos;hour&apos;,&apos;latitude&apos;,&apos;longitude&apos;], right_on=[&apos;DATEn&apos;,&apos;hour&apos;,&apos;latitude&apos;,&apos;longitude&apos;], how=&quot;inner&quot;) 使用DataFrame绘制图形DataFrame 也像 Pandas Series 一样拥有 plot() 方法。如果 df 是 DataFrame，那么 df.plot() 将生成线条图，其中不同颜色的每条线代表 DataFrame 中的一个变量。 但是对于更复杂的图形，通常需要直接使用 matplotlib。 每日平均客流量折线图： 12data_by_date=subway_df.groupby([&apos;DATEn&apos;],as_index=False).mean()data_by_date[&apos;EXITSn_hourly&apos;].plot() 以经纬度作为 x 和 y 轴、客流量作为气泡大小的地铁站散点图: 以经纬度分组：（as_index=False表示使用来分组的值仍作为column） 1data_by_location=subway_df.groupby([&apos;latitude&apos;,&apos;longitude&apos;],as_index=False).mean() 绘图： 12345%pylab inlineimport matplotlib.pyplot as pltimport seaborn as snsscaled=(data_by_location[&apos;ENTRIESn_hourly&apos;]/data_by_location[&apos;ENTRIESn_hourly&apos;].std()*10)plt.scatter(data_by_location[&apos;latitude&apos;],data_by_location[&apos;longitude&apos;],s=scaled) 参数s用来设置每个散点的气泡大小，可以设置为该点对应平均客流量。（除以标准偏差deng 操作用来缩放气泡大小） 大体可以根据气泡大小，经纬度来判断客流量大的地区。","tags":[]},{"title":"使用NumPy和Pandas分析一维数据","date":"2017-04-22T02:25:46.000Z","path":"2017/04/22/使用NumPy和Pandas分析一维数据/","text":"课程内容概述 熟悉NumPy的使用 熟悉Pandas库的使用 两种库的优势：降低编写数据分析代码的难度，提高代码的运算速度。 数据准备从 gapminder.org 中下载一些课程分析所需的数据集，包含： 不同国家就业水平（15 岁以上人口就业率 (%)） 预期寿命（年） 人均 GDP（美元，已经过通胀调整) 教育普及率（% 男性） 教育普及率（% 女性） 通过观察某数据集，如不同国家就业率变化表，我们可以提出诸如以下的问题： 某国家的就业率随时间发生了怎样的变化？ 最高和最低的就业水平出现在哪个国家？我国处于哪个水平？ 各国就业率有没有相同的持续性趋势？ 等。 NumPy和Pandas中的一维数据利用Pandas处理csv文档具有很多优势。 首先是： 编写代码便捷 没有引入Pandas之前，我们读取一个csv文件，并统计数据集中某个字段的不重复量需要经过如下处理： 12345678910111213import unicodecsv def read_csv(filename): with open(filename, &apos;rb&apos;) as f: reader = unicodecsv.DictReader(f) return list(reader)def get_unique_students(data): unique_students=set() for data_point in data: unique_students.add(data_point[&apos;acct&apos;]) return unique_studentsdaily_engagement = read_csv(&apos;/home/milhaven1733/daily_engagement.csv&apos;)unique_engagement_students=get_unique_students(daily_engagement)print len(unique_engagement_students) 引入Pandas之后，大大缩短了代码量： 123import pandas as pddaily_engagement=pd.read_csv(&apos;/home/milhaven1733/daily_engagement.csv&apos;)len(daily_engagement[&apos;acct&apos;].unique()) 其次是： 运算、处理速度高 现在有一个包含100万条左右记录的csv，在引入Pandas之前用read_csv方法读入，统计用时如下： 12345678910import unicodecsv import timedef read_csv(filename): with open(filename, &apos;rb&apos;) as f: reader = unicodecsv.DictReader(f) return list(reader)start = time.time()daily_engagement = read_csv(&apos;/home/milhaven1733/daily_engagement_full.csv&apos;)end=time.time()print end-start 12Out：48.103812933 现在引入Pandas，同样计算读取用时，速度在未引入前的十倍以上： 12345import pandas as pdstart = time.time()daily_engagement=pd.read_csv(&apos;/home/milhaven1733/daily_engagement_full.csv&apos;)end=time.time()print end-start 12Out：3.31470179558 统计不重复的字段数，两者对比，可以发现使用Pandas速度同样有了很大的提升，神器！ 12345start = time.time()unique_engagement_students=get_unique_students(daily_engagement)print len(unique_engagement_students)end=time.time()print end-start 11.04902696609 1234start = time.time()len(daily_engagement[&apos;acct&apos;].unique())end=time.time()print end-start 10.027951002121 NumPy数组Pandas和NumPy都有表示一维、二维数组的特殊数据结构。其中一维的数组： Pandas——series NumPy——array series建立在array基础上 对比NumPy array和Python list： 相同点： 按顺序排列，可以通过位置获取元素 可以利用数据切片 可以使用循环 不同点： NumPy数组中各个元素必须为同一类别（int，boolean，string等） NumPy数组有很多方便使用的函数且运行速度很快。 NumPy 数组可以是多维的。 NumPy数组的建立： 123456789101112131415import numpy as npcountries = np.array([ &apos;Afghanistan&apos;, &apos;Albania&apos;, &apos;Algeria&apos;, &apos;Angola&apos;, &apos;Argentina&apos;, &apos;Armenia&apos;, &apos;Australia&apos;, &apos;Austria&apos;, &apos;Azerbaijan&apos;, &apos;Bahamas&apos;, &apos;Bahrain&apos;, &apos;Bangladesh&apos;, &apos;Barbados&apos;, &apos;Belarus&apos;, &apos;Belgium&apos;, &apos;Belize&apos;, &apos;Benin&apos;, &apos;Bhutan&apos;, &apos;Bolivia&apos;, &apos;Bosnia and Herzegovina&apos;])employment = np.array([ 55.70000076, 51.40000153, 50.5 , 75.69999695, 58.40000153, 40.09999847, 61.5 , 57.09999847, 60.90000153, 66.59999847, 60.40000153, 68.09999847, 66.90000153, 53.40000153, 48.59999847, 56.79999924, 71.59999847, 58.40000153, 70.40000153, 41.20000076]) 首先import NumPy 创建一个list 调用np.array,将其转换为NumPy array。 查看array的元素类型： 123print countries.dtypeprint employment.dtypeprint np.array([True, False, True]).dtype 123|S22 #s表示字符串，22表示最大串长度float64|S2 计算一个array中数据的均值、标准差、最大值、总和： 1234print employment.mean()print employment.std()print employment.max()print employment.sum() 练习：编写函数，返回employment数组的最大值和它对应的countries数组中的国家。 12345def max_employment(countries, employment): x=employment.argmax() max_country = countries[x] max_value = employment[x] return (max_country, max_value) array.argmax() 方法，返回最大值下标. 向量化运算在Python list中，将两个列表相加，返回的是列表连接的结果，如： 12345a=[1,2,3]b=[4,5,6]print a+bOut:[1, 2, 3, 4, 5, 6] 但NumPy array支持向量运算，支持两个长度相同的向量相加： 12345a=np.array([1,2,3])b=np.array([4,5,6])print a+bOut:[5, 7, 9] 向量（vector）乘以标量（ scalar）1234567a=[1,2,3]b=np.array([1,2,3])print a*3print b*3Out：[1, 2, 3, 1, 2, 3, 1, 2, 3][3 6 9] 在Python list中，list*num表示将list中所有值重复num次 在NumPy array中，表示向量乘法计算。 其他向量运算 Math Operations Logical Operations Comparison Operations + &amp;（and） > - \\ （or） >= * ～（not） &lt; / &lt;= ** == != Math Operation 用于两个array间或一个array和一个数字之间 Logical Operations 用于两个布尔型array之间，如果数组类型为整数，运算将执行按位与、按位或、按位取反 向量的算数运算： 123456789a = np.array([1, 2, 3, 4])b = np.array([1, 2, 1, 2])print a * bprint a / bprint a ** bOut：[1 4 3 8][1 1 3 2][ 1 4 3 16] 12345678910111213a = np.array([1, 2, 3, 4])b = 2print a + bprint a - bprint a * bprint a / bprint a ** bOut:[3 4 5 6][-1 0 1 2][2 4 6 8][0 1 1 2][ 1 4 9 16] 注意一点，在Python2中，浮点数或整数除以整数，得到的是整除的结果。如果想保留结果小数点后的值，需要在除数后加小数点。 向量的逻辑运算： 123456789a = np.array([True, True, False, False])b = np.array([True, False, True, False])print a &amp; bprint a | bprint ~aOut:[ True False False False][ True True True False][False False True True] 向量的比较运算： 123456789101112131415a = np.array([1, 2, 3, 4, 5])b = np.array([5, 4, 3, 2, 1])print a &gt; bprint a &gt;= bprint a &lt; bprint a &lt;= bprint a == bprint a != bOut:[False False False True True][False False True True True][ True True False False False][ True True True False False][False False True False False][ True True False True True] 标准化数据点标准化——将数据值转化为与总体均值相差多少个标准偏差 练习：编写标准化数据函数，返回某个NumPy array的标准化结果。 1234567891011employment = np.array([ 55.70000076, 51.40000153, 50.5 , 75.69999695, 58.40000153, 40.09999847, 61.5 , 57.09999847, 60.90000153, 66.59999847, 60.40000153, 68.09999847, 66.90000153, 53.40000153, 48.59999847, 56.79999924, 71.59999847, 58.40000153, 70.40000153, 41.20000076])def standardize_data(values): standardize_values=(values-values.mean())/values.std() return standardize_valuesstandardize_data(employment) NumPy索引数组现在存在两个长度相同的NumPy array，其中第一个array类型任意，第二个为布尔型。如： 12a=[1,2,3,4,5]b=[False,False,True,True,True] 此时，执行a[b]将返回a array在b中对应索引为T的数值构成的array 12345a=np.array([1,2,3,4,5])b=np.array([False,False,True,True,True])print a[b]Out:[3,4,5] 相当于： 123a=np.array([1,2,3,4,5])b=a&gt;2print a[b] 或： 12a=np.array([1,2,3,4,5])print a[a&gt;2] 练习：现有time_spent、days_to_cancel两个相对应的数据集表示学生学习所花时间和注册到注销的时长，计算注册七天内未注销的学生的平均学习时间： 123def mean_time_for_paid_students(time_spent, days_to_cancel): total_time=time_spent[days_to_cancel&gt;=7] return total_time.mean() NumPy + VS +=NumPy中 运算符+ 和+=是有区别的 1234a=np.array([1,2,3,4])b=aa+=np.array([1,1,1,1])print b Out： 1[2 3 4 5] 使用+=，更改了现有的数组。a，b array均被更改为[2 3 4 5] 1234a=np.array([1,2,3,4])b=aa=a+np.array([1,1,1,1])print b Out: 1[1,2,3,4] 使用= 相当于创建了一个新的数组命名为a，a内容更新而b不变。 # in-place VS NOT-in-place+=通常被称为原位运算 （Operation in-place），将新值储存在原值所在位置 而 + 称为非原位运算（Operation NOT-in-place），创建新的内容（较常用） NumPy array 针对切片的运算与Python list不同，如： 1234a=np.array([1,2,3,4,5])slice=a[:3]slice[0]=10print a Out: 1[10 2 3 4 5] 即：对array生成切片时，没有创建新的数组，只是反映了array的情况，当更改切片时，原array也跟着改变。 因为不需创建新的数组，复制任何数据，这也使得NumPy 数据切片速度加快。 Pandas seriesseries与NumPy array类似，但在array基础上具有其他功能。（array适用的方法、函数series也适用） 如 series.describe()可以返回 series的相关统计数据。 1234567891011gdp.describe() #gdp在下方练习中定义Out:count 20.000000mean 9147.879916std 9763.958973min 366.04496725% 1362.12451850% 2967.19027075% 15495.296870max 27036.487332dtype: float64 练习：目前有部分国家平均寿命和GDP的数据，想检查各变量是否具有相关性。即预期寿命高（低）于平均值时，GDP是否也高（低）于平均值？编写函数统计两个值均高于或低于平均值的国家数量，以及一者高于均值而一者低的国家数量。 1234567891011121314151617181920life_expectancy_values = [74.7, 75. , 83.4, 57.6, 74.6, 75.4, 72.3, 81.5, 80.2, 70.3, 72.1, 76.4, 68.1, 75.2, 69.8, 79.4, 70.8, 62.7, 67.3, 70.6]gdp_values = [ 1681.61390973, 2155.48523109, 21495.80508273, 562.98768478, 13495.1274663 , 9388.68852258, 1424.19056199, 24765.54890176, 27036.48733192, 1945.63754911, 21721.61840978, 13373.21993972, 483.97086804, 9783.98417323, 2253.46411147, 25034.66692293, 3680.91642923, 366.04496652, 1175.92638695, 1132.21387981]life_expectancy = pd.Series(life_expectancy_values)gdp = pd.Series(gdp_values)def variable_correlation(variable1, variable2): above=(variable1&gt;variable1.mean())&amp;(variable2&gt;variable2.mean()) below=(variable1&lt;variable1.mean())&amp;(variable2&lt;variable2.mean()) both_above_or_below=above|below num_same_direction = both_above_or_below.sum() num_different_direction = len(both_above_or_below)- num_same_direction return (num_same_direction, num_different_direction)variable_correlation(life_expectancy,gdp)Out:(17,3) above统计是否两者均高于平均值，below统计是否两者均低于平均值，both_above_or_below统计是否符合前两项中一项。三者均返回布尔型的series 布尔型的series可以通过 .sum()方法返回其真值的数量，即符合两者均高于或低于均值的国家数量，再用总数量减去这一数量，得到不满足均高或低于均值的国家数量。 从结果看，这两者之间有较强的正相关关系。 series 索引series和NumPy array的主要区别是：series有索引 如： 12345678910countries = [&apos;Afghanistan&apos;, &apos;Albania&apos;, &apos;Algeria&apos;, &apos;Angola&apos;]employment_values = [55.70000076, 51.40000153, 50.5, 75.69999695,]employment = pd.Series(employment_values, index=countries)print employmentOut：Afghanistan 55.700001Albania 51.400002Algeria 50.500000Angola 75.699997dtype: float64 设置employment的索引为对应的国家名称。因此series更像list和dictionary的合体 series中的值可以通过位置和索引来查找，如： 12employment[0]employment.loc[&apos;Albania&apos;] 当不设置索引时，索引值默认从0排列。 练习：重新编写函数，返回employment series中的最大值和它对应的的国家。 12345employment = pd.Series(employment_values, index=countries)def max_employment(employment): max_country = employment.argmax() max_value = employment.max() return (max_country, max_value) employment.argmax() 返回最大值的索引，即对应的国家。 向量运算与series索引series索引对向量运算有一定影响。 将索引不同的series相加： 123456789s1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])s2 = pd.Series([10, 20, 30, 40], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])print s1 + s2Out:a 11b 22c 33d 44dtype: int64 123456789s1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])s2 = pd.Series([10, 20, 30, 40], index=[&apos;b&apos;, &apos;d&apos;, &apos;a&apos;, &apos;c&apos;])print s1 + s2Out:a 31b 12c 43d 24dtype: int64 1234567891011s1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])s2 = pd.Series([10, 20, 30, 40], index=[&apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;])print s1 + s2Out:a NaNb NaNc 13.0d 24.0e NaNf NaNdtype: float64 12345678910111213s1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])s2 = pd.Series([10, 20, 30, 40], index=[&apos;e&apos;, &apos;f&apos;, &apos;g&apos;, &apos;h&apos;])print s1 + s2Out:a NaNb NaNc NaNd NaNe NaNf NaNg NaNh NaNdtype: float64 可以看出，索引相同顺序不同时，逐个进行相加；索引相同，顺序不同时，值的匹配根据索引进行。 索引不同时，相同索引的值相加，其余索引得到NaN，表示非数字。 填充缺失值如果不想让相加结果中出现NaN，可以怎样处理？ 删除缺失值 12345678s1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])s2 = pd.Series([10, 20, 30, 40], index=[&apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;])result=s1+s2print result.dropna()Out:c 13.0d 24.0dtype: float64 填充缺失值 可以在计算前将缺失索引的值视为0： 123s1=s1.reindex([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;,&apos;e&apos;,&apos;f&apos;],fill_value=0)s2=s2.reindex([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;,&apos;e&apos;,&apos;f&apos;],fill_value=0)print s1 + s2 比较繁琐，需要填写所有索引。更便捷的有： 1234567891011s1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])s2 = pd.Series([10, 20, 30, 40], index=[&apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;])s1.add(s2,fill_value=0)Out:a 1.0b 2.0c 13.0d 24.0e 30.0f 40.0dtype: float64 用add方法取代 ‘+’，同时设置填充值 Series apply()如何对 Series进行没有内置函数或无法通过向量运算的计算？ Pandas为Series提供了apply()函数, apply()载入 Series和函数，然后对 Series中的每个元素调用这个函数，并创建一个新的 Series （类似map的apply函数） 如： 1234567891011s = pd.Series([1, 2, 3, 4, 5])def add_one(x): return x + 1print s.apply(add_one)Out:0 21 32 43 54 6dtype: int64 练习：目前有一个names Series，存储格式为“名 姓”，编写函数，使其转换为”姓, 名”的格式。 12345def reverse_name(names): name_split=names.split(&apos; &apos;) name_new=name_split[1]+&quot; &quot;+name_split[0] return name_newnames.apply(reverse_name) 在 Pandas 中绘图如果变量 data是一个 NumPy array或 Pandas Series，使用 12import matplotlib.pyplot as pltplt.hist(data) 将创建数据的直方图。 Pandas 还在后台使用 matplotlib 的内置绘图函数，因此如果 data 是一个 Series，你可以使用 data.hist() 创建直方图。 有时候 使用Pandas 封装器更加方便，如可以使用 data.plot() 创建 Series 的折线图。Series 索引被用于 x 轴，值被用于 y 轴。 在Jupyter notebook内绘图，需要添加 %pylab inline 首先，载入csv文件为“数据框”，并抽取index对应的记录： 1234567891011121314151617181920212223242526272829303132333435import pandas as pdimport seaborn as snspath=&apos;/home/milhaven1733/Desktop/Data Analyst/data/&apos;employment = pd.read_csv(path + &apos;employment_above_15.csv&apos;, index_col=&apos;Country&apos;)life_expectancy = pd.read_csv(path + &apos;life_expectancy.csv&apos;, index_col=&apos;Country&apos;)employment_ch = employment.loc[&apos;China&apos;]life_expectancy_ch = life_expectancy.loc[&apos;China&apos;]​```​```employment_ch = employment.loc[&apos;China&apos;]life_expectancy_ch = life_expectancy.loc[&apos;China&apos;]gdp_ch=gdp.loc[&apos;China&apos;]​```绘制图形：​```%pylab inlineemployment_ch.plot()​```![](使用NumPy和Pandas分析一维数据\\1.png)​```life_expectancy_us.plot()​```![](使用NumPy和Pandas分析一维数据\\2.png)​```gdp_ch.plot()​```![](使用NumPy和Pandas分析一维数据\\3.png)","tags":[]},{"title":"MySQL-学习笔记-5","date":"2017-04-21T02:47:43.000Z","path":"2017/04/21/MySQL-学习笔记-5/","text":"子查询与连接数据的准备创建商城信息数据库： 123456789mysql&gt; CREATE TABLE IF NOT EXISTS tdb_goods( -&gt; goods_id SMALLINT UNSIGNED PRIMARY KEY AUTO_INCREMENT, -&gt; goods_name VARCHAR(150) NOT NULL, -&gt; goods_cate VARCHAR(40) NOT NULL, -&gt; brand_name VARCHAR(40) NOT NULL, -&gt; goods_price DECIMAL(15,3) UNSIGNED NOT NULL DEFAULT 0, -&gt; is_show BOOLEAN NOT NULL DEFAULT 1, -&gt; is_saleoff BOOLEAN NOT NULL DEFAULT 0 -&gt; ); 字段说明： goods_id 商品id 主键 goods_cate 商品分类 is_show 是否上架 布尔型 is_saleoff 是否售空 布尔型 录入数据： 由于名称、品牌等含有中文字符，需要更改表的编码格式： 1mysql&gt; ALTER TABLE tdb_goods CONVERT TO CHARACTER SET utf8; 录入20余条商品记录，如： 1INSERT tdb_goods (goods_name,goods_cate,brand_name,goods_price,is_show,is_saleoff) VALUES(&apos;商务双肩背包&apos;,&apos;笔记本配件&apos;,&apos;索尼&apos;,&apos;99&apos;,DEFAULT,DEFAULT); 以网格形式显示表内容： 1234567891011121314151617mysql&gt; SELECT * FROM tdb_goods\\G*************************** 1. row *************************** goods_id: 1 goods_name: R510VC 15.6英寸笔记本 goods_cate: 笔记本 brand_name: 华硕goods_price: 3399.000 is_show: 1 is_saleoff: 0*************************** 2. row *************************** goods_id: 2 goods_name: Y400N 14.0英寸笔记本电脑 goods_cate: 笔记本 brand_name: 联想goods_price: 4899.000 is_show: 1 is_saleoff: 0 子查询子查询是指出现在其他SQL语句内的SELECE子句 如： 1SELECT * FROM t1 WHERE col1=(SELECT col2 FROM t2); 其中SELECE * FROM t1称为Outer Quert(外层查询) SELECT col2 FROM t2，称为SubQuery(子查询) 注意几点： 子查询嵌套在查询内部,且必须始终出现在圆括号内。 子查询可以包含多个关键字或条件，如DISTINCT、GROUP BY、LIMIT、函数等。 子查询的外层查询可以是：SELECT,INSERT,UPDATE,SET,DO等 强调：这里的外层查询不是指狭义的“查找”，而是指所有SQL 命令的统称，SQL——结构化查询语言。 子查询返回值： 子查询可以返回标量、一行、一列或子查询。 由比较运算符引发的子查询（第一类）使用比较运算符的子查询： =、&gt;、&lt;、&gt;=、&lt;=、&lt;&gt;、!=、&lt;=&gt; 语法结构 operand comparison_operator subquery 示例一： 查询商品平均价格：（使用聚合函数AVG（）） 123456mysql&gt; SELECT AVG(goods_price) FROM tdb_goods;+------------------+| AVG(goods_price) |+------------------+| 5636.3636364 |+------------------+ 四舍五入到百分位：（ROUND函数） 123456mysql&gt; SELECT ROUND(AVG(goods_price),2) FROM tdb_goods;+---------------------------+| ROUND(AVG(goods_price),2) |+---------------------------+| 5636.36 |+---------------------------+ 查找表中价格大于平均价格的商品信息： 123456789101112mysql&gt; SELECT goods_id,goods_name,goods_price FROM tdb_goods WHERE goods_price&gt;5636.36;+----------+-----------------------------------------+-------------+| goods_id | goods_name | goods_price |+----------+-----------------------------------------+-------------+| 3 | G150TH 15.6英寸游戏本 | 8499.000 || 7 | SVP13226SCB 13.3英寸触控超极本 | 7999.000 || 13 | iMac ME086CH/A 21.5英寸一体电脑 | 9188.000 || 17 | Mac Pro MD878CH/A 专业级台式电脑 | 28888.000 || 18 | HMZ-T3W 头戴显示设备 | 6999.000 || 20 | X3250 M4机架式服务器 2583i14 | 6888.000 || 21 | HMZ-T3W 头戴显示设备 | 6999.000 |+----------+-----------------------------------------+-------------+ 使用子查询得到相同结果： 123456789101112mysql&gt; SELECT goods_id,goods_name,goods_price FROM tdb_goods WHERE goods_price&gt;(SELECT ROUND(AVG(goods_price),2) FROM tdb_goods);+----------+-----------------------------------------+-------------+| goods_id | goods_name | goods_price |+----------+-----------------------------------------+-------------+| 3 | G150TH 15.6英寸游戏本 | 8499.000 || 7 | SVP13226SCB 13.3英寸触控超极本 | 7999.000 || 13 | iMac ME086CH/A 21.5英寸一体电脑 | 9188.000 || 17 | Mac Pro MD878CH/A 专业级台式电脑 | 28888.000 || 18 | HMZ-T3W 头戴显示设备 | 6999.000 || 20 | X3250 M4机架式服务器 2583i14 | 6888.000 || 21 | HMZ-T3W 头戴显示设备 | 6999.000 |+----------+-----------------------------------------+-------------+ 示例二： 查找商品中类型为“超级本”的商品价格： 12345678mysql&gt; SELECT goods_price FROM tdb_goods WHERE goods_cate=&quot;超级本&quot;;+-------------+| goods_price |+-------------+| 4999.000 || 4299.000 || 7999.000 |+-------------+ 在表中查找商品价格大于等于超级本价格的商品： 12mysql&gt; SELECT goods_id,goods_name,goods_price FROM tdb_goods WHERE goods_price&gt;=(SELECT goods_price FROM tdb_goods WHERE goods_cate=&quot;超级本&quot;);ERROR 1242 (21000): Subquery returns more than 1 row 报错:子查询返回结果多于一行，即没有指定对于一个还是多个结果进行比较。 用ANY、SOME、ALL、修饰比较运算符： ANY和SOME等价，表示符合一个即可，ALL表示需要符合所有。返回值原则如下： ANY SOME ALL >、&gt;= 最小值 最小值 最大值 &lt;、&lt;= 最大值 最大值 最小值 = 任意值 任意值 &lt;&gt;、!= 任意值 如查询大于等于最大值的： 123456789mysql&gt; SELECT goods_id,goods_name,goods_price FROM tdb_goods WHERE goods_price&gt;=ALL(SELECT goods_price FROM tdb_goods WHERE goods_cate=&quot;超级本&quot;);+----------+-----------------------------------------+-------------+| goods_id | goods_name | goods_price |+----------+-----------------------------------------+-------------+| 3 | G150TH 15.6英寸游戏本 | 8499.000 || 7 | SVP13226SCB 13.3英寸触控超极本 | 7999.000 || 13 | iMac ME086CH/A 21.5英寸一体电脑 | 9188.000 || 17 | Mac Pro MD878CH/A 专业级台式电脑 | 28888.000 |+----------+-----------------------------------------+-------------+ 由[NOT] IN/EXISTS 引发的子查询（第二、三类）语法结构 operand comparison_operator [NOT] IN subquery =ANY 运算符与IN等效。 !=ALL或&lt;&gt;ALL运算符与NOT IN等效。 如： 12345678mysql&gt; SELECT goods_id,goods_name,goods_price FROM tdb_goods WHERE goods_price IN (SELECT goods_price FROM tdb_goods WHERE goods_cate=&quot;超级本&quot;);+----------+---------------------------------------+-------------+| goods_id | goods_name | goods_price |+----------+---------------------------------------+-------------+| 5 | X240(20ALA0EYCD) 12.5英寸超极本 | 4999.000 || 6 | U330P 13.3英寸超极本 | 4299.000 || 7 | SVP13226SCB 13.3英寸触控超极本 | 7999.000 |+----------+---------------------------------------+-------------+ 使用[NOT] EXISTS 的子查询 如果子查询返回任何行，EXISTS将返回TRUE；否则为FALSE。（不常用） 使用INSERT…SELECT插入记录将查询结果写入数据表： 1INSERT [INTO] table_name [(col_name,...)] SELECT... 之前使用的表中goods_cate和brand_name字段重复信息很多，如果商品信息增多，会造成数据表体积过于庞大，故该字段适合用外键存储，需要另建商品类型表及品牌表，以创建商品类型表为例： 1234mysql&gt; CREATE TABLE tdb_goods_cates( -&gt; cate_id SMALLINT UNSIGNED PRIMARY KEY AUTO_INCREMENT, -&gt; cate_name VARCHAR(40) NOT NULL -&gt; ); 把分类信息插入新建表： 对商品信息进行分组： 123456789101112mysql&gt; SELECT goods_cate FROM tdb_goods GROUP BY goods_cate;+---------------------+| goods_cate |+---------------------+| 台式机 || 平板电脑 || 服务器/工作站 || 游戏本 || 笔记本 || 笔记本配件 || 超级本 |+---------------------+ 将查询结果写入数据表：（含有中文字符需修改字符编码） 1mysql&gt; INSERT tdb_goods_cates(cate_name) SELECT goods_cate FROM tdb_goods GROUP BY goods_cate; 查看结果： 123456789101112mysql&gt; SELECT * FROM tdb_goods_cates;+---------+---------------------+| cate_id | cate_name |+---------+---------------------+| 1 | 台式机 || 2 | 平板电脑 || 3 | 服务器/工作站 || 4 | 游戏本 || 5 | 笔记本 || 6 | 笔记本配件 || 7 | 超级本 |+---------+---------------------+ 接下来，如果想使用外键表示商品分类，需要参照分类表去更新商品表，涉及下一节——多表更新。 多表更新1UPDATE table_references SET col_name1=&#123;expr1|DEFAULT&#125;[,col_name2=&#123;expr2|DEFAULT&#125;]... [WHERE where_condition] 多表更新重点：表的参照关系，即：table_references 语法结构： 1234table_references&#123;[INNER|CROSS]JOIN|&#123;LEFT|RIGHT&#125;[OUTER]JOIN&#125;table_referencesON conditional_expr 连接类型： 1234567inner join,内连接在mysql，join,cross join和inner join 是等价的left [outer] join,左外连接right [outer] join,右外连接 使用内连接更新商品表： 1mysql&gt; UPDATE tdb_goods INNER JOIN tdb_goods_cates ON goods_cate=cate_name SET goods_cate=cate_id; 查看结果： 12345678910111213141516171819202122232425mysql&gt; SELECT * FROM tdb_goods\\G*************************** 1. row *************************** goods_id: 1 goods_name: R510VC 15.6英寸笔记本 goods_cate: 5 brand_name: 华硕goods_price: 3399.000 is_show: 1 is_saleoff: 0*************************** 2. row *************************** goods_id: 2 goods_name: Y400N 14.0英寸笔记本电脑 goods_cate: 5 brand_name: 联想goods_price: 4899.000 is_show: 1 is_saleoff: 0*************************** 3. row *************************** goods_id: 3 goods_name: G150TH 15.6英寸游戏本 goods_cate: 4 brand_name: 雷神goods_price: 8499.000 is_show: 1 is_saleoff: 0 一步到位的多表更新首先，我们可以在创建表的同时将查询结果写入到数据表（使用CREATE…SELECT语句） 如： 12345mysql&gt; ALTER DATABASE lcyDB CHARACTER SET = utf8; //先改变数据库编码方式，否则不能在创建同时写入mysql&gt; CREATE TABLE tdb_goods_brands( -&gt; brand_id SMALLINT UNSIGNED PRIMARY KEY AUTO_INCREMENT, -&gt; brand_name VARCHAR(40) NOT NULL) -&gt; SELECT brand_name FROM tdb_goods GROUP BY brand_name; 查看结果： 1234567891011121314mysql&gt; SELECT * FROM tdb_goods_brands;+----------+------------+| brand_id | brand_name |+----------+------------+| 1 | IBM || 2 | 华硕 || 3 | 宏碁 || 4 | 惠普 || 5 | 戴尔 || 6 | 索尼 || 7 | 联想 || 8 | 苹果 || 9 | 雷神 |+----------+------------+ 更新商品表品牌信息： 先按照上面的方法更新，报错： 12mysql&gt; UPDATE tdb_goods INNER JOIN tdb_goods_brands ON brand_name=brand_name SET brand_name=brand_id;ERROR 1052 (23000): Column &apos;brand_name&apos; in field list is ambiguous 原因是，两表中都含有字段brand_name，无法区分字段是属于哪个表（含义不明），这时候需要在字段前加表名或者为表起别名： 1mysql&gt; UPDATE tdb_goods AS g INNER JOIN tdb_goods_brands AS b ON g.brand_name=b.brand_name SET g.brand_name=b.brand_id; 更新完成，查看： 123456789mysql&gt; SELECT * FROM tdb_goods\\G*************************** 1. row *************************** goods_id: 1 goods_name: R510VC 15.6英寸笔记本 goods_cate: 5 brand_name: 2goods_price: 3399.000 is_show: 1 is_saleoff: 0 但查看表的结构发现虽然更新了字段内容，字段的数据类型依然为“varchar(40)”，这时候，最好能修改字段的类型。 同时修改列名称和列定义需使用ALTER…CHANGE语句： 123mysql&gt; ALTER TABLE tdb_goods -&gt; CHANGE goods_cate cate_id SMALLINT UNSIGNED NOT NULL, -&gt; CHANGE brand_name brand_id SMALLINT UNSIGNED NOT NULL; 再来查看字段类型： 123456789101112mysql&gt; SHOW COLUMNS FROM tdb_goods;+-------------+------------------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-------------+------------------------+------+-----+---------+----------------+| goods_id | smallint(5) unsigned | NO | PRI | NULL | auto_increment || goods_name | varchar(150) | NO | | NULL | || cate_id | smallint(5) unsigned | NO | | NULL | || brand_id | smallint(5) unsigned | NO | | NULL | || goods_price | decimal(15,3) unsigned | NO | | 0.000 | || is_show | tinyint(1) | NO | | 1 | || is_saleoff | tinyint(1) | NO | | 0 | |+-------------+------------------------+------+-----+---------+----------------+ 此时，上面修改的两个字段已经成为了一种“事实上的外键” 内连接使用ON关键字来设定连接条件，也可以使用WHERE来代替 但通常使用ON关键字来设定连接条件，而使用WHERE关键字进行结果集记录的过滤 内连接： 只显示坐标和右表符合连接条件的记录 我们先在商品表中插入一条类型表中不含有其cate_id的记录： 1mysql&gt; INSERT tdb_goods(goods_name,cate_id,brand_id,goods_price) VALUES(&apos; LaserJet Pro P1606dn 黑白激光打印机&apos;,&apos;12&apos;,&apos;4&apos;,&apos;1849&apos;); 使用内连接查找两张表的字段： 1mysql&gt; SELECT goods_id,goods_name cate_name FROM tdb_goods INNER JOIN tdb_goods_cates ON tdb_goods.cate_id=tdb_goods_cates.cate_id\\G 找到22条商品记录 1234*************************** 22. row *************************** goods_id: 22cate_name: 商务双肩背包22 rows in set (0.00 sec) 因为新增加的一条记录不符合连接条件（商品分类表不存在其cate_id） 外连接左外连接 LEFT JOIN： 显示左表全部和右表符合连接条件的记录 右外连接 RIGHT JOIN： 显示右表全部和左表符合连接条件的记录 左外连接示例： 1mysql&gt; SELECT goods_id,goods_name,cate_name FROM tdb_goods LEFT JOIN tdb_goods_cates ON tdb_goods.cate_id=tdb_goods_cates.cate_id\\G 查找到23条记录： 12345*************************** 23. row *************************** goods_id: 23goods_name: LaserJet Pro P1606dn 黑白激光打印机 cate_name: NULL23 rows in set (0.00 sec) 可以看到，当左表中的记录，在右表中不存在含有符合连接条件的字段，右表中字段显示为NULL 右外连接示例： 先在类型表中插入几条商品表中不含有其cate_id的记录： 1mysql&gt; INSERT tdb_goods_cates(cate_name) VALUES(&apos;路由器&apos;),(&apos;交换机&apos;),(&apos;网卡&apos;); 进行右外连接查询： 1mysql&gt; SELECT goods_id,goods_name,cate_name FROM tdb_goods RIGHT JOIN tdb_goods_cates ON tdb_goods.cate_id=tdb_goods_cates.cate_id\\G 这次查询到了25条记录： 12345678910111213*************************** 23. row *************************** goods_id: NULLgoods_name: NULL cate_name: 路由器*************************** 24. row *************************** goods_id: NULLgoods_name: NULL cate_name: 交换机*************************** 25. row *************************** goods_id: NULLgoods_name: NULL cate_name: 网卡25 rows in set (0.00 sec) 同样，当右表中的记录在左表中没有相应记录能满足连接条件，查询结果中，来自左表的字段被设置为NULL 多表连接连接三张表查询商品的完整信息：（要为表起别名方便操作） 123mysql&gt; SELECT goods_id,goods_name,cate_name,brand_name,goods_price FROM tdb_goods AS g -&gt; INNER JOIN tdb_goods_cates AS c ON g.cate_id=c.cate_id -&gt; INNER JOIN tdb_goods_brands AS b ON g.brand_id=b.brand_id\\G 查询结果示例： 123456*************************** 1. row *************************** goods_id: 1 goods_name: R510VC 15.6英寸笔记本 cate_name: 笔记本 brand_name: 华硕goods_price: 3399.000 多表连接操作相当于外键的逆向操作。 多表删除在当前数据表中，id为18,19的记录和id为21,22的记录是完全相同的，怎样删除数据表中的重复记录？ 可以用一张表模拟多表删除的方式来实现。 首先——查找重复记录 1mysql&gt; SELECT MIN(goods_id),goods_name FROM tdb_goods GROUP BY goods_name\\G 以goods_name对记录进行分组（mysql 5.7以后必须用MIN(goods_id)指定显示分组数据时，每组若有多个goods_id，应具体显示哪个） 发现查找到21条记录，即有21个不同goods_name的商品. 但是这样操作只显示了不重复的数据有多少，我们还需要查找出是那些记录出现了重复，可以在查询结果中用HAVING语句加以筛选： 1mysql&gt; SELECT MIN(goods_id),goods_name FROM tdb_goods GROUP BY goods_name HAVING count(goods_name)&gt;=2; 结果显示： 123456+---------------+-----------------------------+| MIN(goods_id) | goods_name |+---------------+-----------------------------+| 18 | HMZ-T3W 头戴显示设备 || 19 | 商务双肩背包 |+---------------+-----------------------------+ 查找到了重复的记录。 接下来，我们可以将查询结果看成一张新表，参照这张表来删除原有表中的记录： 1mysql&gt; DELETE t1 FROM tdb_goods AS t1 LEFT JOIN (SELECT MIN(goods_id) AS goods_id,goods_name FROM tdb_goods GROUP BY goods_name HAVING count(goods_name)&gt;=2) AS t2 ON t1.goods_name=t2.goods_name WHERE t1.goods_id&gt;t2.goods_id; 语句比较复杂。语句中执行了如下操作： 将表tdb_goods设置别名t1 以goods_name分类，查找出重复商品记录的goods_id和goods_name，将查询结果生成一张新表，起别名t2 左外连接两张表，连接条件为t1.goods_name=t2.goods_name 删除t1表中的重复记录，删除的条件是：t1.goods_id&gt;t2.goods_id; 个人理解：删除是在连接两张表得到的结果中查找满足WHERE后语句的记录，并将这些记录从t1中删除。 试验证明，连接时使用INNER JOIN也可以，连接结果是重复的四条记录。 无限极分类数据表（重点内容）如果商品分类下还有小分类，小分类下继续包含分类，这样无限极分类的数据表如何设计？ 通过表自身的连接来实现。 首先创建一张无限分类数据表 12345mysql&gt; CREATE TABLE tdb_goods_types( -&gt;type_id SMALLINT UNSIGNED PRIMARY KEY AUTO_INCREMENT, -&gt;type_name VARCHAR(20) NOT NULL, -&gt;parent_id SMALLINT UNSIGNED NOT NULL DEFAULT 0 -&gt;); parent_id字段 表示其父类的id 插入若干记录： 123456789101112131415INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;家用电器&apos;,DEFAULT); INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;电脑、办公&apos;,DEFAULT); INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;大家电&apos;,1); INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;生活电器&apos;,1); INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;平板电视&apos;,3); INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;空调&apos;,3); INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;电风扇&apos;,4); INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;饮水机&apos;,4); INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;电脑整机&apos;,2); INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;电脑配件&apos;,2); INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;笔记本&apos;,9); INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;超级本&apos;,9); INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;游戏本&apos;,9); INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;CPU&apos;,10); INSERT tdb_goods_types(type_name,parent_id) VALUES(&apos;主机&apos;,10); 查看记录: 1234567891011121314151617181920mysql&gt; SELECT * FROM tdb_goods_types;+---------+-----------------+-----------+| type_id | type_name | parent_id |+---------+-----------------+-----------+| 1 | 家用电器 | 0 || 2 | 电脑、办公 | 0 || 3 | 大家电 | 1 || 4 | 生活电器 | 1 || 5 | 平板电视 | 3 || 6 | 空调 | 3 || 7 | 电风扇 | 4 || 8 | 饮水机 | 4 || 9 | 电脑整机 | 2 || 10 | 电脑配件 | 2 || 11 | 笔记本 | 9 || 12 | 超级本 | 9 || 13 | 游戏本 | 9 || 14 | CPU | 10 || 15 | 主机 | 10 |+---------+-----------------+-----------+ “大家电”父类——“ 家用电器 ” “空调”父类——“ 大家电 ” 以此类推。 自身连接查找： 可以想象在当前表右侧出现一张相同的表。我们将左边的表当做子表，右边的表当做父表进行连接： 1mysql&gt; SELECT s.type_id,s.type_name,p.type_name FROM tdb_goods_types AS s LEFT JOIN tdb_goods_types AS p ON s.parent_id=p.type_id; 连接结果： 12345678910111213141516171819+---------+-----------------+-----------------+| type_id | type_name | type_name |+---------+-----------------+-----------------+| 1 | 家用电器 | NULL || 2 | 电脑、办公 | NULL || 3 | 大家电 | 家用电器 || 4 | 生活电器 | 家用电器 || 5 | 平板电视 | 大家电 || 6 | 空调 | 大家电 || 7 | 电风扇 | 生活电器 || 8 | 饮水机 | 生活电器 || 9 | 电脑整机 | 电脑、办公 || 10 | 电脑配件 | 电脑、办公 || 11 | 笔记本 | 电脑整机 || 12 | 超级本 | 电脑整机 || 13 | 游戏本 | 电脑整机 || 14 | CPU | 电脑配件 || 15 | 主机 | 电脑配件 |+---------+-----------------+-----------------+ 查找出了子类对应的父类名称。 接下来查找父类中包含的子类： 1mysql&gt; SELECT p.type_id,p.type_name,s.type_name FROM tdb_goods_types AS p LEFT JOIN tdb_goods_types AS s ON s.parent_id=p.type_id; 结果： 1234567891011121314151617181920212223242526+---------+-----------------+--------------+| type_id | type_name | type_name |+---------+-----------------+--------------+| 1 | 家用电器 | 大家电 || 1 | 家用电器 | 生活电器 || 3 | 大家电 | 平板电视 || 3 | 大家电 | 空调 || 4 | 生活电器 | 电风扇 || 4 | 生活电器 | 饮水机 || 2 | 电脑、办公 | 电脑整机 || 2 | 电脑、办公 | 电脑配件 || 9 | 电脑整机 | 笔记本 || 9 | 电脑整机 | 超级本 || 9 | 电脑整机 | 游戏本 || 10 | 电脑配件 | CPU || 10 | 电脑配件 | 主机 || 5 | 平板电视 | NULL || 6 | 空调 | NULL || 7 | 电风扇 | NULL || 8 | 饮水机 | NULL || 11 | 笔记本 | NULL || 12 | 超级本 | NULL || 13 | 游戏本 | NULL || 14 | CPU | NULL || 15 | 主机 | NULL |+---------+-----------------+--------------+ 对结果以父类名称分类呈现，并显示子类的数目(别名为child_count)： 1mysql&gt; SELECT MIN(p.type_id),p.type_name,count(s.type_name) AS child_count FROM tdb_goods_types AS p LEFT JOIN tdb_goods_types AS s ON s.parent_id=p.type_id GROUP BY p.type_name; 结果为： 12345678910111213141516171819+----------------+-----------------+-------------+| MIN(p.type_id) | type_name | child_count |+----------------+-----------------+-------------+| 14 | CPU | 0 || 15 | 主机 | 0 || 3 | 大家电 | 2 || 1 | 家用电器 | 2 || 5 | 平板电视 | 0 || 13 | 游戏本 | 0 || 4 | 生活电器 | 2 || 2 | 电脑、办公 | 2 || 9 | 电脑整机 | 3 || 10 | 电脑配件 | 2 || 7 | 电风扇 | 0 || 6 | 空调 | 0 || 11 | 笔记本 | 0 || 12 | 超级本 | 0 || 8 | 饮水机 | 0 |+----------------+-----------------+-------------+ 为最左一列字段起别名并以此为标准排序： 1mysql&gt; SELECT MIN(p.type_id) AS type_id,p.type_name,count(s.type_name) AS child_count FROM tdb_goods_types AS p LEFT JOIN tdb_goods_types AS s ON s.parent_id=p.type_id GROUP BY p.type_name ORDER BY child_count DESC,type_id; 结果： 12345678910111213141516171819+---------+-----------------+-------------+| type_id | type_name | child_count |+---------+-----------------+-------------+| 9 | 电脑整机 | 3 || 1 | 家用电器 | 2 || 2 | 电脑、办公 | 2 || 3 | 大家电 | 2 || 4 | 生活电器 | 2 || 10 | 电脑配件 | 2 || 5 | 平板电视 | 0 || 6 | 空调 | 0 || 7 | 电风扇 | 0 || 8 | 饮水机 | 0 || 11 | 笔记本 | 0 || 12 | 超级本 | 0 || 13 | 游戏本 | 0 || 14 | CPU | 0 || 15 | 主机 | 0 |+---------+-----------------+-------------+","tags":[]},{"title":"MySQL-学习笔记-4","date":"2017-04-20T02:34:13.000Z","path":"2017/04/20/MySQL-学习笔记-4/","text":"操作数据表中的记录本节重点：对于数据表中记录的增、删、改、查。 插入记录INSERT插入记录 1INSERT [INTO] table_name [(col_name,...)] &#123;VALUES|VALUE&#125; (&#123;expr|DEFAULT&#125;,...),(...),... 列名称省略，代表所有字段依次赋值 值可以为表达式或默认值，字段之间逗号分隔 可以一次插入多条记录 对于自动编号的字段，可以用NULL或DEFAULT赋值，实现自动编号，如： 12mysql&gt; INSERT user_new VALUES(NULL,&apos;John&apos;,&apos;1111&apos;,25,1);mysql&gt; INSERT user_new VALUES(DEFAULT,&apos;John&apos;,&apos;1111&apos;,25,1); 赋值时可以采用表达式，对于包含默认值的字段，可以用DEFAULT使之保持默认值： 12mysql&gt; INSERT user_new VALUES(DEFAULT,&apos;Tom&apos;,&apos;1111&apos;,3*7-2,1);mysql&gt; INSERT user_new VALUES(DEFAULT,&apos;Tom&apos;,&apos;2222&apos;,DEFAULT,1); 以上指令操作结果： 123456789mysql&gt; SELECT * FROM user_new;+----+----------+----------+-----+------+| id | username | password | age | sex |+----+----------+----------+-----+------+| 1 | John | 1111 | 25 | 1 || 2 | John | 1111 | 25 | 1 || 3 | Tom | 1111 | 19 | 1 || 4 | Tom | 2222 | 10 | 1 |+----+----------+----------+-----+------+ 可以一次插入多条记录，且值可以返回自函数： 123456789101112mysql&gt; INSERT user_new VALUES(DEFAULT,&apos;Tom&apos;,&apos;2222&apos;,DEFAULT,1),(NULL,&apos;Rose&apos;,md5(&apos;123&apos;),DEFAULT,0);mysql&gt; SELECT * FROM user_new; +----+----------+----------------------------------+-----+------+| id | username | password | age | sex |+----+----------+----------------------------------+-----+------+| 1 | John | 1111 | 25 | 1 || 2 | John | 1111 | 25 | 1 || 3 | Tom | 1111 | 19 | 1 || 4 | Tom | 2222 | 10 | 1 || 5 | Tom | 2222 | 10 | 1 || 6 | Rose | 202cb962ac59075b964b07152d234b70 | 10 | 0 |+----+----------+----------------------------------+-----+------+ 插入记录 INSERT SET/SELECT插入记录： 1INSERT [INTO] table_name SET col_name=&#123;expr|DEFAULT&#125;,... 区别： 此方法可以引发子查询（SubQuery） 【由比较运算符引发的子查询，是引发子查询的三种方式之一】 只能一次性插入一条记录 例如： 1mysql&gt; INSERT user_new SET username=&apos;Bob&apos;,password=&apos;456&apos;; id字段自动编号，age字段有默认值，sex字段可以为空，可以均不赋值。 插入记录： 1INSERT [INTO] table_name [(col_name,...)] SELECT... 此方法可以将查询结果插入到指定数据表。（详见最后笔记底部） 单表更新记录UPDATE更新记录（单表更新） 1UPDATE [LOW_PRIORITY][IGNORE] table_reference SET col_name1=&#123;expr1|DEFAULT&#125;[,col_name2=&#123;expr2|DEFAULT&#125;]...[WHERE where_condition] 可选项：降低优先权、处理更新过程中的错误中断等。 参照关系：在单表操作中只能为某一张表 省略WHERE条件，则所有的记录都被更新 如： 1mysql&gt; UPDATE user_new set age=age+5; 将所有记录的age值加5. 可以同时更新多个字段： 12345678910111213mysql&gt; UPDATE user_new set age=age-1,sex=0;mysql&gt; SELECT * FROM user_new;+----+----------+----------------------------------+-----+------+| id | username | password | age | sex |+----+----------+----------------------------------+-----+------+| 1 | John | 1111 | 29 | 0 || 2 | John | 1111 | 29 | 0 || 3 | Tom | 1111 | 23 | 0 || 4 | Tom | 2222 | 14 | 0 || 5 | Tom | 2222 | 14 | 0 || 6 | Rose | 202cb962ac59075b964b07152d234b70 | 14 | 0 || 7 | Bob | 456 | 14 | 0 |+----+----------+----------------------------------+-----+------+ 可以在WHERE之后加入条件，如： 1mysql&gt; UPDATE user_new set age=age+5 WHERE id%2=0; 注意：判断取余结果要用“=” 不是“==” 可以看到，id为偶数的记录被更新了。 123456789101112mysql&gt; SELECT * FROM user_new;+----+----------+----------------------------------+-----+------+| id | username | password | age | sex |+----+----------+----------------------------------+-----+------+| 1 | John | 1111 | 29 | 0 || 2 | John | 1111 | 34 | 0 || 3 | Tom | 1111 | 23 | 0 || 4 | Tom | 2222 | 19 | 0 || 5 | Tom | 2222 | 14 | 0 || 6 | Rose | 202cb962ac59075b964b07152d234b70 | 19 | 0 || 7 | Bob | 456 | 14 | 0 |+----+----------+----------------------------------+-----+------+ 单表删除记录DELETE删除记录（单表删除） 1DELETE FROM table_name [WHERE where_condition] 如： 1mysql&gt; DELETE FROM user_new WHERE id=6; id为６的记录被删除。但需要注意，当此时再插入一条新纪录，id为当前最大id值加１，而非补充被删去的６： 12345678910111213mysql&gt; INSERT user_new VALUES(NULL,&apos;Tom&apos;,&apos;2222&apos;,33,NULL);mysql&gt; SELECT * FROM user_new;+----+----------+----------+-----+------+| id | username | password | age | sex |+----+----------+----------+-----+------+| 1 | John | 1111 | 29 | 0 || 2 | John | 1111 | 34 | 0 || 3 | Tom | 1111 | 23 | 0 || 4 | Tom | 2222 | 19 | 0 || 5 | Tom | 2222 | 14 | 0 || 7 | Bob | 456 | 14 | 0 || 8 | Tom | 2222 | 33 | NULL |+----+----------+----------+-----+------+ 查询表达式解析SELECT语句在针对表操作的语句中使用率非常高。 SELECT语句语法： SELECT语句可以只书写表达式，如： 1SELECT VERSION(); SELECT NOW(); 不依附于任何一张表 SELECE语句查询表达式： SELECT 查询表达式顺序影响结果顺序： 123456789101112131415161718192021222324mysql&gt; SELECT id,username FROM user_new;+----+----------+| id | username |+----+----------+| 1 | John || 2 | John || 3 | Tom || 4 | Tom || 5 | Tom || 7 | Bob || 8 | Tom |+----+----------+mysql&gt; SELECT username,id FROM user_new;+----------+----+| username | id |+----------+----+| John | 1 || John | 2 || Tom | 3 || Tom | 4 || Tom | 5 || Bob | 7 || Tom | 8 |+----------+----+ table_name.colume_name表示某表的某列，table_name.*表示该表所有列（多表连接操作中需要明确某列隶属于某表）。 可以使用 as alias_name 使用别名，如：（as可以书写，也可以不写） 123456789101112mysql&gt; SELECT id AS userid,username AS uname FROM user_new;+--------+-------+| userid | uname |+--------+-------+| 1 | John || 2 | John || 3 | Tom || 4 | Tom || 5 | Tom || 7 | Bob || 8 | Tom |+--------+-------+ 字段的别名将影响结果集。 where语句进行条件查询条件表达式： 对记录进行过滤，如果没有指定where语句，则显示所有记录（同理，在UPDATE语句和DELETE语句中将更新和删除所有记录） 在WHERE表达式中，可以使用MySQL支持的函数（如数学、字符函数）和运算符。 group by语句对查询结果分组查询结果分组： 1[GROUP BY &#123;cool_name|position&#125; [ASC|DESC],...] ASC表示分组结果升序排列（默认），DESC表示分组结果降序排列。 如： 1234567891011121314mysql&gt; SELECT sex FROM user_new GROUP BY sex;+------+| sex |+------+| NULL || 0 |+------+mysql&gt; SELECT sex FROM user_new GROUP BY sex DESC;+------+| sex |+------+| 0 || NULL |+------+ having语句设置分组筛选条件分组条件 1[HAVING where_condition] 用HAVING字句做分组筛选的条件时，HAVING后的分组条件需要为聚合函数或条件中字段出现在SELECT之后 聚合函数：永远只有一个返回结果的函数，如常规的min、max、avg、sum、count等 如： 123456mysql&gt; SELECT sex FROM user_new GROUP BY sex HAVING count(id)&gt;2;+------+| sex |+------+| 0 |+------+ 分组筛选的结果剔除了SEX是NULL的结果，其原因是SEX=NULL的结果只有一个，不满足count(id)&gt;2的条件。 order by语句对查询结果排序对查询结果进行排序： 1[ORDER BY &#123;col_name|expr|position&#125;[ASC|DESC],...] 可以添加多个排序条件，当按照第一个字段排列后有重复值，再按照第二个字段排列，以此类推。如： 123456789101112131415161718192021222324mysql&gt; SELECT * FROM user_new ORDER BY age;+----+----------+----------+-----+------+| id | username | password | age | sex |+----+----------+----------+-----+------+| 5 | Tom | 2222 | 14 | 0 || 7 | Bob | 456 | 14 | 0 || 4 | Tom | 2222 | 19 | 0 || 3 | Tom | 1111 | 23 | 0 || 1 | John | 1111 | 29 | 0 || 8 | Tom | 2222 | 33 | NULL || 2 | John | 1111 | 34 | 0 |+----+----------+----------+-----+------+mysql&gt; SELECT * FROM user_new ORDER BY age,id DESC;+----+----------+----------+-----+------+| id | username | password | age | sex |+----+----------+----------+-----+------+| 7 | Bob | 456 | 14 | 0 || 5 | Tom | 2222 | 14 | 0 || 4 | Tom | 2222 | 19 | 0 || 3 | Tom | 1111 | 23 | 0 || 1 | John | 1111 | 29 | 0 || 8 | Tom | 2222 | 33 | NULL || 2 | John | 1111 | 34 | 0 |+----+----------+----------+-----+------+ limit语句限制查询数量限制查询结果返回的数量 1[LIMIT &#123;[offset,]row_count|row_count OFFSET offset&#125;] 返回查询结果中的前两条： 1234567mysql&gt; SELECT * FROM user_new LIMIT 2;+----+----------+----------+-----+------+| id | username | password | age | sex |+----+----------+----------+-----+------+| 1 | John | 1111 | 29 | 0 || 2 | John | 1111 | 34 | 0 |+----+----------+----------+-----+------+ 返回从第2条起的2条记录：（记录从0开始编号） 1234567mysql&gt; SELECT * FROM user_new LIMIT 2,2;+----+----------+----------+-----+------+| id | username | password | age | sex |+----+----------+----------+-----+------+| 3 | Tom | 1111 | 23 | 0 || 4 | Tom | 2222 | 19 | 0 |+----+----------+----------+-----+------+ 还要注意，id号和结果中的排号无联系，如： 1234567mysql&gt; SELECT * FROM user_new ORDER BY id DESC LIMIT 2;+----+----------+----------+-----+------+| id | username | password | age | sex |+----+----------+----------+-----+------+| 8 | Tom | 2222 | 33 | NULL || 7 | Bob | 456 | 14 | 0 |+----+----------+----------+-----+------+ 将查询结果插入到表中可以讲查询返回的结果插入到指定表中，如： 新建一张表： 123mysql&gt; CREATE TABLE test( -&gt; id TINYINT UNSIGNED PRIMARY KEY AUTO_INCREMENT, -&gt; username VARCHAR(20) -&gt; ); 将查询到的字段插入到新建表中： 12mysql&gt; INSERT test(username) SELECT username FROM user_new WHERE age&gt;=25;Query OK, 3 rows affected (0.04 sec) 查看: 12345678mysql&gt; SELECT * FROM test;+----+----------+| id | username |+----+----------+| 1 | John || 2 | John || 3 | Tom |+----+----------+","tags":[]},{"title":"关于udacity学生数据集的t检验分析","date":"2017-04-19T14:23:28.000Z","path":"2017/04/19/关于udacity学生数据集的t检验分析/","text":"前言上一次，应用udacity提供的学生数据集，我们熟悉数据分析的基本流程，完成了数据的获取、导入、整理、取样、分类等步骤，也得到了相应的推测结论。但由于没有经过统计学检查，得到的只是未经证实的试验性结论。 因此，这一次我希望选取上次分析过程中得到的几个采样方法有代表的数据样本，对其进行统计学上的检验，看看分析结果是否能验证上次做出的一些预测或推论。 检验什么问题根据上次分析中得到的一个结论：“通过第一个项目的学生上课分钟数要多于未通过的学生”，若进行验证，我们可以将要检验的论题定为：“通过第一个项目与未通过的学生，上课时长是否有显著的差异”。 检验方式通过项目的学生与未通过项目的学生为两个不同的、无相互影响的群体，针对他们的一些特征值进行对比分析，应该采用独立样本的t检验 数据导出上一次，为了对数据进行简单的描述统计，我们对注册一周内的参与记录进行分组，并根据字段提取了我们感兴趣的数据。那么上一次用于描述统计的数据集同样也是我们这一次进行推论统计分析的数据来源。 为了对数据有直观的感觉，也为了便于计算、对比，虽然也可以直接在程序内部继续加工，整理，分析数据，但这一次我还是希望能将数据重新导出到csv文件，利用熟悉的表格应用来展示分析过程。 Python csv库的写入功能比想象中略繁琐一些，csv.writer.writerows方法接受一个包含tuple的list，其中每个list元素（即一个tuple）写入一行，tuple的每个元素占一列，元素需要有多个且必须为string类型。（经试验，tuple只含一个元素时，该字符串会被逐字符分入单元格。。加一个’,’可以解决问题） 但我们之前得到的数据集，如:non_passing_minutes.values()返回的是一个元素为float类型的list，必须对数据加以加工转化才能顺利写入csv文件。 设计写入函数： 12345678910import csvdef writecsv(filename,listname): with open (filename, &apos;wb+&apos;) as csvfile: writer = csv.writer(csvfile) data_tuple=() for value in listname.values(): data=[] #list data_tuple=(value,) #list的tuple元素，一定要加&apos;,&apos; data.append(data_tuple) writer.writerows(data) #写入 得到csv文件： 1234items_list=[non_passing_minutes,passing_minutes]items_name=[&apos;non_passing_minutes&apos;,&apos;passing_minutes&apos;]for name,item in zip(items_name,items_list): writecsv(name+&apos;.csv&apos;,item) 打开本地文件检查是否成功写入： 12print len(non_passing_minutes.values())print len(passing_minutes.values()) out: 12348647 与list长度一致，写入完成。 进入分析——假设检验数据导出到表格，可以正式进入分析阶段。 针对我们想要研究的问题：“通过第一个项目与未通过的学生，上课时长是否有显著的差异” 我们可以先设定假设检验： $$T_n表示未通过项目的学生的听课时长，T_p表示通过项目的学生的听课时长$$ $$H_0:T_n=T_p(两者没有显著差异)$$ $$H_A:T_n &gt;T_p (两者存在显著差异,且前者大于后者)$$ $$(\\alpha=0.05)$$ 首先计算两组样本用时的均值（单位：min）： Pass: $$\\bar{X}_p=394.59$$ Non_Pass: $$\\bar{X}_n=143.33$$ 再来计算两组样本的标准偏差：(利用excel的stdev函数计算，之前分析结果中的标准差是不包含校正系数的) $$S_p=448.85$$ $$S_n=269.93$$ 接下来计算总体的标准误差： 根据两独立样本的标准误差的计算公式： $$sd=\\sqrt{\\frac{S_1^2}{n_1}+\\frac{S_2^2}{n_2}}$$ $$SEM=S_{\\bar X_p-\\bar X_n}=\\sqrt{\\frac{S_p^2}{n_p}+\\frac{S_n^2}{n_n}}=22.82$$ 计算t统计量： $$t=\\frac{\\bar X_p-\\bar X_n}{SEM}=11.01$$ 计算t临界值： $$DF=n_p+n_n-2=993$$ 对照t表格可知$$df=1000,单尾\\alpha=0.05 时，t_c=1.646$$ 结论： $$t&gt; t_c$$ 所以：拒绝零假设 证明：通过第一个项目与未通过的学生，上课时长有显著的差异，且通过项目学生听课时长显著偏高。 总结应用统计学的检验方法，我们可以证明我们的推论：通过第一个项目的学生上课分钟数要多于未通过的学生。 用类似的方式，我们也可以验证对与其他数据集的推论，此篇不赘述。","tags":[]},{"title":"利用udacity学生数据集实践数据分析过程","date":"2017-04-19T03:22:06.000Z","path":"2017/04/19/利用udacity学生数据集实践数据分析过程/","text":"利用udacity学生数据集实践数据分析过程数据采集与清理数据分析中的数据来源 下载数据文件 从API中获取 从网页收集 合并多种不同格式数据 在这次实践中，我们采用下载数据文件的方式。（将课程提供的三个csv数据集下载至本地) 接下来，需要对数据文件进行数据整理使数据能够用于分类及分析。 就要先了解我们下载的csv文件。 关于CSV Comma Separated Values——逗号分隔值 易于通过代码处理（相比.xlsx） csv文件的表格与文本格式: Python中的CSVcsv文件内容通常呈现为一系列行 每行为一个列表，整体数据结构为包含一系列列表的列表 1csv=[[&apos;A1&apos;,&apos;A2&apos;,&apos;A3&apos;],[&apos;B1&apos;,&apos;B2&apos;,&apos;B3&apos;]] 每行为一个字典，整体数据结构为包含一系列字典的列表（标题行的每个关键词为键，每个字段作为值。） 1csv=[&#123;&apos;name1&apos;:&apos;A1&apos;,&apos;name2&apos;:&apos;A2&apos;,&apos;name3&apos;:&apos;A3&apos;&#125;,&#123;&apos;name1&apos;:&apos;B1&apos;,&apos;name2&apos;:&apos;B2&apos;,&apos;name3&apos;:&apos;B3&apos;&#125;] Python unicodecsv库 123456789import unicodecsvenrollments=[]f=open(&apos;/home/milhaven1733/enrollments.csv&apos;,&apos;rb&apos;)reader=unicodecsv.DictReader(f)for row in reader: enrollments.append(row)f.close()enrollments[0] 输出： 1234567&#123;u&apos;account_key&apos;: u&apos;448&apos;, u&apos;cancel_date&apos;: u&apos;2015-01-14&apos;, u&apos;days_to_cancel&apos;: u&apos;65&apos;, u&apos;is_canceled&apos;: u&apos;True&apos;, u&apos;is_udacity&apos;: u&apos;True&apos;, u&apos;join_date&apos;: u&apos;2014-11-10&apos;, u&apos;status&apos;: u&apos;canceled&apos;&#125; 运用DictReader方法将csv文件每行转化为字典并生成一个迭代器（注意迭代器只能一次用于循环一次。） 优化代码： 12345import unicodecsvwith open(&apos;/home/milhaven1733/enrollments.csv&apos;,&apos;rb&apos;) as f: reader=unicodecsv.DictReader(f) enrollments=list(reader)enrollments[0] 利用list方法不使用循环生成列表 分别读取所需的三个csv文件： 12345678import unicodecsv def read_csv(filename): with open(filename, &apos;rb&apos;) as f: reader = unicodecsv.DictReader(f) return list(reader)enrollments = read_csv(&apos;/home/milhaven1733/enrollments.csv&apos;)daily_engagement = read_csv(&apos;/home/milhaven1733/daily_engagement.csv&apos;)project_submissions = read_csv(&apos;/home/milhaven1733/project_submissions.csv&apos;) 修正数据类型从生成的字典列表中可以看出，unicodecsv处理时并不区分数据类型，需要根据实际使用来转换数据类型。（最好在获取数据之初就进行转换以免遗忘） 修正enrollments： 1234567&#123;u&apos;account_key&apos;: u&apos;448&apos;, u&apos;cancel_date&apos;: u&apos;2015-01-14&apos;, u&apos;days_to_cancel&apos;: u&apos;65&apos;, u&apos;is_canceled&apos;: u&apos;True&apos;, u&apos;is_udacity&apos;: u&apos;True&apos;, u&apos;join_date&apos;: u&apos;2014-11-10&apos;, u&apos;status&apos;: u&apos;canceled&apos;&#125; 12345678910111213141516171819from datetime import datetime as dtdef parse_date(date): if date==&apos;&apos;: return None else: return dt.strptime(date,&apos;%Y-%m-%d&apos;) def parse_int(i): if i==&apos;&apos;: return None else: return int(i)for enrollment in enrollments: enrollment[&apos;join_date&apos;]=parse_date(enrollment[&apos;join_date&apos;]) enrollment[&apos;cancel_date&apos;]=parse_date(enrollment[&apos;cancel_date&apos;]) enrollment[&apos;days_to_cancel&apos;]=parse_int(enrollment[&apos;days_to_cancel&apos;]) enrollment[&apos;is_udacity&apos;]=enrollment[&apos;is_udacity&apos;]==&apos;True&apos; enrollment[&apos;is_canceled&apos;]=enrollment[&apos;is_canceled&apos;]==&apos;True&apos; 运用datetime.strptime转换时间格式 修正Project_submissions： 123456&#123;u&apos;account_key&apos;: u&apos;256&apos;, u&apos;assigned_rating&apos;: u&apos;UNGRADED&apos;, u&apos;completion_date&apos;: u&apos;2015-01-16&apos;, u&apos;creation_date&apos;: u&apos;2015-01-14&apos;, u&apos;lesson_key&apos;: u&apos;3176718735&apos;, u&apos;processing_state&apos;: u&apos;EVALUATED&apos;&#125; 123for submission in project_submissions: submission[&apos;creation_date&apos;]=parse_date(submission[&apos;creation_date&apos;]) submission[&apos;completion_date&apos;]=parse_date(submission[&apos;completion_date&apos;]) 修正daily_engagement： 123456&#123;u&apos;acct&apos;: u&apos;0&apos;, u&apos;lessons_completed&apos;: u&apos;0.0&apos;, u&apos;num_courses_visited&apos;: u&apos;1.0&apos;, u&apos;projects_completed&apos;: u&apos;0.0&apos;, u&apos;total_minutes_visited&apos;: u&apos;11.6793745&apos;, u&apos;utc_date&apos;: u&apos;2015-01-09&apos;&#125; 123456for engagement in daily_engagement: engagement[&apos;lessons_completed&apos;]=int(float(engagement[&apos;lessons_completed&apos;])) engagement[&apos;num_courses_visited&apos;]=int(float(engagement[&apos;num_courses_visited&apos;])) engagement[&apos;projects_completed&apos;]=int(float(engagement[&apos;projects_completed&apos;])) engagement[&apos;total_minutes_visited&apos;]=float(engagement[&apos;total_minutes_visited&apos;]) engagement[&apos;utc_date&apos;]=parse_date(engagement[&apos;utc_date&apos;]) 先将包含小数点的字符串转化为float再转化为int 对处理好的数据集提出疑问数据集处理完成后需要针对数据提出疑问，如： 通常需要多久提交项目？ 通过和未通过项目的学生有什么区别？ 学员上课的平均时间 上课时间、数量与完成项目之间的关系 参与度随时间的改变 通过项目前提交的次数 等。 本次探究针对问题：通过与未通过首个项目的学生参与课程的差异。 整理数据成功加载数据并确保数据格式良好，说明你已经开始数据整理过程了。下一步就是调查，看看数据中是否存在不一致处或问题，如果有，则需要清理它们。 可以在每个数据表中查找不重复学员数，并创建一组帐号列表：（以处理enrollments列表为例） 1234unique_enrollment=[]for enrollment in enrollments: unique_enrollment.append(enrollment[&apos;account_key&apos;])unique_enrollment=set(unique_enrollment) 用set(list)达到去重目的。 修正数据表中的问题enrollments表与daily_engagement表中，表示学员账户的键名称不同，可能会对之后的数据处理及分析产生影响，怎么修改能使之保持一致？ 123for engagement_record in daily_engagement: engagement_record[&apos;account_key&apos;] = engagement_record[&apos;acct&apos;] del[engagement_record[&apos;acct&apos;]] 修改后，亦可创建函数方便实现上个话题中三个数据表分别清理重复学员账户的问题。 缺失的参与记录从之前的学员账户统计结果可以看出，enrollments表与daily_engagement表中，’account_key‘的数量不同，为什么部分注册用户没有参与记录呢？ 我们可以找出缺失参与记录的注册用户，对其特点展开探究： 123for enrollment in enrollments: if enrollment[&apos;account_key&apos;] not in unique_engagement: print enrollment 通过遍历enrollments表，查找缺失参与记录的用户。大体可以发现： 缺失记录的用户，大多在同一天注册与注销。 （即可能只有账户存在时长超过一天，才会存在参与记录） 核查更多问题记录我们上面找到的原因是造成部分用户缺失参与记录的全部原因吗？是否存在其他原因？可以对缺失参与记录的注册用户展开进一步探究： 123for enrollment in enrollments: if enrollment[&apos;account_key&apos;] not in unique_engagement and enrollment[&apos;join_date&apos;] != enrollment[&apos;cancel_date&apos;]: print enrollment 剩余问题对上个问题中存在的另外3个异常用户展开探究，发现其’is_udacity‘的值为True，即为系统的测试用户——找到了问题。 接下来，我们需要排除数据集中的测试用户，以避免对之后分析的干扰。 首先，创建测试用户集合： 12345test_account=[]for enrollment in enrollments: if enrollment[&apos;is_udacity&apos;]: test_account.append(enrollment[&apos;account_key&apos;])test_account=set(test_account) 创建从数据集中清理测试用户的函数： 123456def remove_uda_acc(data): non_uda_data=[] for data_point in data: if data_point[&apos;account_key&apos;] not in test_account: non_uda_data.append(data_point) return non_uda_data 执行函数，创建不包含测试用户的新数据集： 123non_uda_enrollments=remove_uda_acc(enrollments)non_uda_engagement=remove_uda_acc(daily_engagement)non_uda_submissions=remove_uda_acc(project_submissions) 提炼问题目前，想探究的问题： 对于通过和未通过第一个项目的学员，他们在daily_engagement表中的数据有何不同？ 目前整理出的数据集对于探究这个问题存在的缺陷： 提交后的参与数据与项目无关 可能会对比不同时间段的参与数据（参与度受时间影响） 参与数据包含不属于第一个项目的课程 对策：（针对前两个问题） 只查看学生注册前一周的数据，并排除一周内注销的用户。 针对以上方案，可以首先：创建未注销或注销前注册时长超过七天的学生字典paid_students 对于字典：Key：’account_key‘, Value:enrollment_date 创建过程如下： 12345678paid_student=&#123;&#125;for enrollment in non_uda_enrollments: if (not enrollment[&apos;is_canceled&apos;]) or (enrollment[&apos;days_to_cancel&apos;]&gt;7): account_key=enrollment[&apos;account_key&apos;] enrollment_date=enrollment[&apos;join_date&apos;] if(account_key not in paid_student or enrollment_date&gt;paid_student[account_key]): paid_student[account_key]=enrollment_date 由于一个学生可能多次注册，仅在account_key不存在，或enrollment_date比原注册日期更晚时才添加数据，可以保证保存的的是最近的注册日期。 获取第一周数据创建新列表，存储paid_students中学生注册一周内的参与数据： 首先，移除各数据集中注册时长小于一周的学生数据（即account_key未在paid_students中的数据） 12345678910def remove_free_trial_cancels(data): new_data = [] for data_point in data: if data_point[&apos;account_key&apos;] in paid_students: new_data.append(data_point) return new_datapaid_enrollments = remove_free_trial_cancels(non_udacity_enrollments)paid_engagement = remove_free_trial_cancels(non_udacity_engagement)paid_submissions = remove_free_trial_cancels(non_udacity_submissions) 创建检测参与时间是否在注册一周内的函数： 123def within_one_week(join_date, engagement_date): time_delta = engagement_date - join_date return time_delta.days &lt; 7 最后，创建新列表并存储符合要求的数据：（利用上述函数进行检验） 12345678paid_engagement_in_first_week = []for engagement_record in paid_engagement: account_key = engagement_record[&apos;account_key&apos;] join_date = paid_students[account_key] engagement_record_date = engagement_record[&apos;utc_date&apos;] if within_one_week(join_date, engagement_record_date): paid_engagement_in_first_week.append(engagement_record) 对参与度的探索怎样探索某学员第一周上课的平均时间？ 可以将搜娱的参与记录按学员账户分组存入字典，每组包含某学生的所有参与记录 key:account_key Value:engagement_table 123456from collections import defaultdict #允许设置默认值的字典engagement_by_account=defaultdict(list) #当在字典中寻找商不存在的关键字，会得到一个空列表。for engagement_record in paid_engagement_in_first_week: account_key=engagement_record[&apos;account_key&apos;] engagement_by_account[account_key].append(engagement_record) 运行代码后，一周内参与数据已分组存入字典。 接下来，可以另设一字典，存入某学生一周内学习的总时长 Key:account_key Value:total_minutes 123456total_minutes_by_account=&#123;&#125;for account_key,engagement_for_student in engagement_by_account.items(): total_minutes=0 for engagement_record in engagement_for_student: total_minutes+=engagement_record[&apos;total_minutes_visited&apos;] total_minutes_by_account[account_key]=total_minutes 得到了包含所有学员一周内学习时长的字典，可以通过dict.values()方法，将字典中所有value导入一列表，引入numpy库，进行简单的分析： 123456total_minutes=total_minutes_by_account.values()import numpy as npprint &apos;Mean:&apos;,np.mean(total_minutes)print &apos;Standard deviation:&apos;,np.std(total_minutes)print &apos;Min:&apos;,np.min(total_minutes)print &apos;Max:&apos;,np.max(total_minutes) 从输出结果： 1234Mean: 647.590173826Standard deviation: 1129.27121042Min: 0.0Max: 10568.1008673 #（异常值） 可以看出，数据存在一定的问题，需要进一步地探究。 寻找异常所在我们可以从学习时长最大的account入手，寻找数据采集是否存在问题。 首先，查找max_minutes对应的max_account: 12345max_minutes=0for account_key,total_minutes in total_minutes_by_account.items(): if total_minutes&gt;max_minutes: max_minutes=total_minutes max_account=account_key 在一周参与记录中查找该账户对应的记录以及该账户的注册信息： 1234for engagement in paid_engagement_in_first_week: if engagement[&apos;account_key&apos;]==max_account: print engagementprint paid_student[max_account] 从输出结果看到条目远远大于7条，即收集的参与数据包含了学员最新一次注册之前的所有数据，检查可发现，是检测参与时间是否在注册一周内的函数出现问题，加以更正： 123def within_one_week(join_date, engagement_date): time_delta = engagement_date - join_date return time_delta.days &lt; 7 and time_delta.days&gt;=0 重新运行其下代码，在新的数据集中进行简要分析，得到结果： 1234Mean: 306.708326753Standard deviation: 412.996933409Min: 0.0Max: 3564.7332645 虽然Max值接近60小时，但较为可信。 可以执行本节中前两段代码再次检测最大值对应用户的参与记录，核验是否仍存在问题。 探索课程完成情况怎样探索学员第一周完成的课程数量？ 利用目前已经得到的某学员一周内参与记录，我们可以用类似得到上课时长的方式得到完成课程数量。 由于代码相似度非常高，可以定义几个函数，完成类似的内容，如： 123456789101112131415def sum_grouped_items(grouped_data, field_name): summed_data = &#123;&#125; for account_key,engagement_for_student in grouped_data.items(): total=0 for engagement_record in engagement_for_student: total+=engagement_record[field_name] summed_data[account_key]=total return summed_data import numpy as npdef describe_data(data): print &apos;Mean:&apos;, np.mean(data) print &apos;Standard deviation:&apos;, np.std(data) print &apos;Minimum:&apos;, np.min(data) print &apos;Maximum:&apos;, np.max(data) 得到某学员某项指标总值的函数，与简要分析包含所有学员某项总值的列表的函数 进行分析: 1234total_minutes_by_account=sum_grouped_items(engagement_by_account,&apos;total_minutes_visited&apos;)lessons_completed=sum_grouped_items(engagement_by_account,&apos;lessons_completed&apos;)describe_data(total_minutes_by_account.values())describe_data(lessons_completed.values()) 为增强运算灵活性，也可以在这里将按学员account_key分组得到某学员参与数据的代码重写为函数并执行： 12345678from collections import defaultdict #允许设置默认值的字典def group_data(data,key_name): grouped_data=defaultdict(list) #当在字典中寻找商不存在的关键字，会得到一个空列表。 for data_point in data: key=data_point[key_name] grouped_data[key].append(data_point) return grouped_dataengagement_by_account=group_data(paid_engagement_in_first_week,&apos;account_key&apos;) 探索一周访问天数怎样可以得到学员每周访问课程的天数？ 从记录中可以看出，学员某日记录的‘num_courses_visited’字段不同。大于0说明当日访问了课程，等于0则说明没有访问，我们可以为记录新增加’has_visited’字段，来记录当天是否访问，再调用上节的各个函数，就可以计算一周内访问天数的总和以及对所有访问天数进行分析了。 为paid_engagement（所有注册时间长于一周的学员的记录）添加’has_visited’字段： 12345for engagement in paid_engagement: if engagement[&apos;num_courses_visited&apos;]&gt;0: engagement[&apos;has_visited&apos;]=1 else: engagement[&apos;has_visited&apos;]=0 进行统计和分析： 12total_visited=sum_grouped_items(engagement_by_account,&apos;has_visited&apos;)describe_data(total_visited.values()) 划分学员参与记录现在继续正题，以是否通过第一个项目为标准，将学员划分为两组，同时也将参与记录划分为两组。 首先查看一条提交记录： 1234567paid_submissions[0]:&#123;u&apos;account_key&apos;: u&apos;256&apos;, u&apos;assigned_rating&apos;: u&apos;UNGRADED&apos;, u&apos;completion_date&apos;: datetime.datetime(2015, 1, 16, 0, 0), u&apos;creation_date&apos;: datetime.datetime(2015, 1, 14, 0, 0), u&apos;lesson_key&apos;: u&apos;3176718735&apos;, u&apos;processing_state&apos;: u&apos;EVALUATED&apos;&#125; ‘lesson_key’字段指示提交项目的代码 ‘assigned_rating’字段指示项目是否通过 而首个项目的’lesson_key’为’746169184’ 或 ‘3176718735’ 表示通过项目的状态为：’PASSED’或’DISTINCTION’ 可以执行以下代码，将所有通过第一个项目的学员的account_key 加入pass_subway_project列表： 123456789subway_project_lesson_keys = [&apos;746169184&apos;, &apos;3176718735&apos;]rating_passed=[&apos;PASSED&apos;,&apos;DISTINCTION&apos;]pass_subway_project = set()for submission in paid_submissions: account_key=submission[&apos;account_key&apos;] lesson_key=submission[&apos;lesson_key&apos;] assigned_rating=submission[&apos;assigned_rating&apos;] if (lesson_key in subway_project_lesson_keys) and (assigned_rating in rating_passed): pass_subway_project.add(account_key) 再以account_key是否在pass_subway_project列表中为标准划分参与记录： 1234567passing_engagement = []non_passing_engagement = []for engagement_record in paid_engagement_in_first_week: if engagement_record[&apos;account_key&apos;] in pass_subway_project: passing_engagement.append(engagement_record) else: non_passing_engagement.append(engagement_record) 比较两组学员现在可以调用之间group_data与统计、分析函数，新创建两个包含某学员参与数据的字典并进行通过与未通过项目的学员参与数据之间统计与比较： 1234567891011121314151617181920212223242526272829passing_engagement_by_account = group_data(passing_engagement,&apos;account_key&apos;)non_passing_engagement_by_account = group_data(non_passing_engagement,&apos;account_key&apos;)print &apos;total_minutes_visited:&apos;print &apos;non-passing students:&apos;non_passing_minutes = sum_grouped_items(non_passing_engagement_by_account,&apos;total_minutes_visited&apos;)describe_data(non_passing_minutes.values())print &apos;\\npassing students:&apos;passing_minutes = sum_grouped_items(passing_engagement_by_account,&apos;total_minutes_visited&apos;)describe_data(passing_minutes.values())print &apos;\\n\\nlessons_completed:&apos;print &apos;non-passing students:&apos;non_passing_lessons = sum_grouped_items(non_passing_engagement_by_account,&apos;lessons_completed&apos;)describe_data(non_passing_lessons.values())print &apos;\\npassing students:&apos;passing_lessons = sum_grouped_items(passing_engagement_by_account,&apos;lessons_completed&apos;)describe_data(passing_lessons.values())print &apos;\\n\\nhas_visited:&apos;print &apos;non-passing students:&apos;non_passing_visits = sum_grouped_items(non_passing_engagement_by_account,&apos;has_visited&apos;)describe_data(non_passing_visits.values())print &apos;\\npassing students:&apos;passing_visits = sum_grouped_items(passing_engagement_by_account,&apos;has_visited&apos;)describe_data(passing_visits.values()) 输出结果： 12345678910111213141516171819202122232425262728293031323334353637383940total_minutes_visited:non-passing students:Mean: 143.326474267Standard deviation: 269.538619011Minimum: 0.0Maximum: 1768.52274933passing students:Mean: 394.586046484Standard deviation: 448.499519327Minimum: 0.0Maximum: 3564.7332645lessons_completed:non-passing students:Mean: 0.862068965517Standard deviation: 2.54915994183Minimum: 0Maximum: 27passing students:Mean: 2.05255023184Standard deviation: 3.14222705558Minimum: 0Maximum: 36has_visited:non-passing students:Mean: 1.90517241379Standard deviation: 1.90573144136Minimum: 0Maximum: 7passing students:Mean: 3.38485316847Standard deviation: 2.25882147092Minimum: 0Maximum: 7 可对以上分析结果自行进行比较探究。 创建直方图对于上述6个统计结果，可以创建图表加以描述。 jupyter notebook创建直方图： 1234data = [1, 2, 1, 3, 3, 1, 4, 2]%matplotlib inline #设定图表在notebook内部输出import matplotlib.pyplot as pltplt.hist(data) 得到以下图表： 关于hist参数 参照示例创建描述及绘图函数： 123456789101112%matplotlib inlineimport matplotlib.pyplot as pltimport numpy as npdef describe_data(data,bins,x_label,y_label,title): print &apos;Mean:&apos;, np.mean(data) print &apos;Standard deviation:&apos;, np.std(data) print &apos;Minimum:&apos;, np.min(data) print &apos;Maximum:&apos;, np.max(data) plt.hist(data,bins) plt.xlabel(x_label) plt.ylabel(y_label) plt.title(title) 首先对比两组学生的听课总时长分布： 1234describe_data(non_passing_minutes.values(),50, &apos;Time/minutes&apos;,&apos;Number of people&apos;,r&apos;non_passing_students_total_minutes&apos;)describe_data(passing_minutes.values(),50, &apos;Time/minutes&apos;,&apos;Number of people&apos;,r&apos;passing_students_total_minutes&apos;) 得到图像： 对比发现，分布形状大致类似，但通过项目的学生组持续较长听课时间的人数要明显多于未通过组。 然后对比两组完成课程数量的分布： 1234describe_data(non_passing_lessons.values(),54, &apos;Lessons&apos;,&apos;Number of people&apos;,r&apos;non_passing_students_lessons&apos;)describe_data(passing_lessons.values(),72, &apos;Lessons&apos;,&apos;Number of people&apos;,r&apos;non_passing_students_lessons&apos;) 发现：虽然第一周两组内都是未听课的人数居多，但通过组听课量保持在1-10节的人数明显多于未通过组。 最后对比两组访问课程天数： 1234describe_data(non_passing_visits.values(),20, &apos;Visits/days&apos;,&apos;Number of people&apos;,r&apos;non_passing_students_visits&apos;)describe_data(passing_visits.values(),20, &apos;Visits/days&apos;,&apos;Number of people&apos;,r&apos;passing_students_visits&apos;) 这次的图形外形差异很大： 对与未通过组，随着天数增多，人数逐渐减少，而通过组，各个访问天数内人数相差不大。 是否能得到结论？通过以上对变量间联系的探究，我们可以做出一些预测或初步结论： 如：通过第一个项目的学生上课分钟数要多于未通过的学生。 但是，两组间的差异是真实存在的差异还是偶然造成（或受一些隐含因素影响）的呢？ 要得到确切的结论，还需要严格的统计学检查，否则得到的只是未经证实的试验性结论。 关于应用统计学分析以上数据集得出更严格的结论，可以看下一篇博客： 关于udacity学生数据集的t检验分析 关于相关性与因果性相关性：通过第一个项目的学生更愿意在第一周内多听课 因果性：在第一周内更多上课可以使学生通过第一个项目 两者之间有非常大的不同，如果想要验证因果关系，可以进行A/B测试 基于众多特征进行预测通过以上的整理分析，现在可以尝试预测：哪些学生更可能通过首个项目？ 可以运用启发式方法（heuristics） 也可以利用机器学习，自动进行较为准确的预测。 改善图形分享心得在得出结论和做出预测后，需要分享研究结果、进行交流。 但如果想以可视化的方法呈现结果，最好可以对图形加以优化，使其更美观、更能表现图表想要传达的内容。 如，可以添加坐标轴名称，图表标题，改变bins参数设置直方图所使用的分组数量等优化图表 还可以使用 seaborn 库自动美化 matplotlib 图形。 即在代码中导入此库： 1import seaborn as sns 在此后创建的图形就会自动进行美化。 优化后的效果 至此，我们较为完整地完成了一个数据分析的流程，但还有许多不完善的地方有待加以补充。","tags":[]},{"title":"MySQL-学习笔记-3","date":"2017-04-16T01:23:42.000Z","path":"2017/04/16/MySQL-学习笔记-3/","text":"MySQL约束及修改数据表外键约束的要求解析约束 根据约束针对字段多少： 针对一个字段——列级约束 针对多个字段——表级约束 外键约束 注：当外键列无索引时，MySQL自动创建索引，而当参照列无索引时，不会自动创建。 查看MySQL当前提供引擎 1mysql&gt; show engines; 查看MySQL默认存储引擎 123456789mysql&gt; show variables like &apos;%storage_engine%&apos;;+----------------------------------+--------+| Variable_name | Value |+----------------------------------+--------+| default_storage_engine | InnoDB || default_tmp_storage_engine | InnoDB || disabled_storage_engines | || internal_tmp_disk_storage_engine | InnoDB |+----------------------------------+--------+ 创建父表： 1234mysql&gt; CREATE TABLE provinces( -&gt; id SMALLINT UNSIGNED PRIMARY KEY AUTO_INCREMENT, -&gt; pname VARCHAR(20) NOT NULL -&gt; ); 查看创建父表时的存储引擎： 12345678910mysql&gt; SHOW CREATE TABLE provinces;+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| provinces | CREATE TABLE `provinces` ( `id` smallint(5) unsigned NOT NULL AUTO_INCREMENT, `pname` varchar(20) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=latin1 |+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 创建子表（错误示例） 1234567mysql&gt; CREATE TABLE user( -&gt; id SMALLINT UNSIGNED PRIMARY KEY AUTO_INCREMENT, -&gt; username VARCHAR(10) NOT NULL, -&gt; pid BIGINT, -&gt; FOREIGN KEY (pid) REFERENCES provinces (id) -&gt; );ERROR 1215 (HY000): Cannot add foreign key constraint 可见，外键列必须与主键保持相同数据类型 创建子表： 123456mysql&gt; CREATE TABLE users( -&gt; id SMALLINT UNSIGNED PRIMARY KEY AUTO_INCREMENT, -&gt; username VARCHAR(10) NOT NULL, -&gt; pid SMALLINT UNSIGNED, -&gt; FOREIGN KEY (pid) REFERENCES provinces (id) -&gt; ); 查看子表创建时的引擎和外键的创建： 12345678910111213mysql&gt; SHOW CREATE TABLE users;+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| users | CREATE TABLE `users` ( `id` smallint(5) unsigned NOT NULL AUTO_INCREMENT, `username` varchar(10) NOT NULL, `pid` smallint(5) unsigned DEFAULT NULL, PRIMARY KEY (`id`), KEY `pid` (`pid`), CONSTRAINT `users_ibfk_1` FOREIGN KEY (`pid`) REFERENCES `provinces` (`id`)) ENGINE=InnoDB DEFAULT CHARSET=latin1 |+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 查看父表索引： 123456789101112131415mysql&gt; SHOW INDEXES FROM provinces\\G*************************** 1. row *************************** Table: provinces Non_unique: 0 Key_name: PRIMARY Seq_in_index: 1 Column_name: id Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: Index_type: BTREE Comment: Index_comment: 可见：参照列 ‘id’ 为主键，已自动创建索引 \\G表示以网格形式显示 查看子表索引 1234567891011121314151617181920212223242526272829mysql&gt; SHOW INDEXES FROM users\\G*************************** 1. row *************************** Table: users Non_unique: 0 Key_name: PRIMARY Seq_in_index: 1 Column_name: id Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: Index_type: BTREE Comment: Index_comment: *************************** 2. row *************************** Table: users Non_unique: 1 Key_name: pid Seq_in_index: 1 Column_name: pid Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE Comment: Index_comment: 可见，子表存在两个索引：子表主键的索引以及外键的索引 外键约束的参照操作 创建子表，指定外键的参照操作： 123456mysql&gt; CREATE TABLE user1( -&gt; id SMALLINT UNSIGNED PRIMARY KEY AUTO_INCREMENT, -&gt; username VARCHAR(10) NOT NULL, -&gt; pid SMALLINT UNSIGNED, -&gt; FOREIGN KEY (pid) REFERENCES provinces (id) ON DELETE CASCADE -&gt; ); 在父表中插入记录 1234567891011121314151617mysql&gt; INSERT provinces(pname) VALUE(&apos;A&apos;);Query OK, 1 row affected (0.06 sec)mysql&gt; INSERT provinces(pname) VALUE(&apos;B&apos;);Query OK, 1 row affected (0.04 sec)mysql&gt; INSERT provinces(pname) VALUE(&apos;C&apos;);Query OK, 1 row affected (0.09 sec)mysql&gt; SELECT * FROM provinces;+----+-------+| id | pname |+----+-------+| 1 | A || 2 | B || 3 | C |+----+-------+ 在子表中插入记录 12mysql&gt; INSERT user1(username,pid) VALUES(&apos;Tom&apos;,3);Query OK, 1 row affected (0.03 sec) 12mysql&gt; INSERT user1(username,pid) VALUES(&apos;John&apos;,5);ERROR 1452 (23000): Cannot add or update a child row: a foreign key constraint fails (`lcyDB`.`user1`, CONSTRAINT `user1_ibfk_1` FOREIGN KEY (`pid`) REFERENCES `provinces` (`id`) ON DELETE CASCADE) 可见：外键列不能设为父表中参照列不存在的值 123456789101112mysql&gt; INSERT user1(username,pid) VALUES(&apos;John&apos;,1);Query OK, 1 row affected (0.05 sec)mysql&gt; INSERT user1(username,pid) VALUES(&apos;Rose&apos;,2);Query OK, 1 row affected (0.05 sec)mysql&gt; SELECT * FROM user1;+----+----------+------+| id | username | pid |+----+----------+------+| 1 | Tom | 3 || 4 | John | 1 || 5 | Rose | 2 |+----+----------+------+ 删除父表中某记录，查看子表相应行是否改变： 123456789mysql&gt; DELETE FROM provinces WHERE id=3;Query OK, 1 row affected (0.05 sec)mysql&gt; SELECT * FROM user1;+----+----------+------+| id | username | pid |+----+----------+------+| 4 | John | 1 || 5 | Rose | 2 |+----+----------+------+ 可见：子表已自动删除id为３的记录 但因为物理的外键约束只有INNODB引擎才支持，在实际的开发过程中，我们很少使用物理的外键约束，大多使用逻辑的外键约束。所以说，我们在实际的项目开发中，一般定义逻辑的外键，指的是在定义两张表结构时，按照存在的某种结构的方式去定义，但是不使用FOREIGN KEY这个关键词 表级约束和列级约束 如之前创建的外键约束为列级约束，可以在列定义时声明 在实际开发中，用列级约束比较多，表级约束很少用。 修改数据表——添加\\删除列添加单列 ALTER TABLE table_name ADD [COLUMN] col_name colume_definition [FIRST|AFTER col_name] 如： 123456789101112mysql&gt; ALTER TABLE user1 ADD age TINYINT UNSIGNED NOT NULL DEFAULT 10;Query OK, 0 rows affected (0.68 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; SHOW COLUMNS FROM user1;+----------+----------------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+----------+----------------------+------+-----+---------+----------------+| id | smallint(5) unsigned | NO | PRI | NULL | auto_increment || username | varchar(10) | NO | | NULL | || pid | smallint(5) unsigned | YES | MUL | NULL | || age | tinyint(3) unsigned | NO | | 10 | |+----------+----------------------+------+-----+---------+----------------+ 指定AFTER: 1234567891011121314mysql&gt; ALTER TABLE user1 ADD password VARCHAR(20) NOT NULL AFTER username;Query OK, 0 rows affected (0.76 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; SHOW COLUMNS FROM user1;+----------+----------------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+----------+----------------------+------+-----+---------+----------------+| id | smallint(5) unsigned | NO | PRI | NULL | auto_increment || username | varchar(10) | NO | | NULL | || password | varchar(20) | NO | | NULL | || pid | smallint(5) unsigned | YES | MUL | NULL | || age | tinyint(3) unsigned | NO | | 10 | |+----------+----------------------+------+-----+---------+----------------+5 rows in set (0.00 sec) 指定FIRST: 123456789101112131415mysql&gt; ALTER TABLE user1 ADD truename VARCHAR(20) NOT NULL FIRST;Query OK, 0 rows affected (0.96 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; SHOW COLUMNS FROM user1;+----------+----------------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+----------+----------------------+------+-----+---------+----------------+| truename | varchar(20) | NO | | NULL | || id | smallint(5) unsigned | NO | PRI | NULL | auto_increment || username | varchar(10) | NO | | NULL | || password | varchar(20) | NO | | NULL | || pid | smallint(5) unsigned | YES | MUL | NULL | || age | tinyint(3) unsigned | NO | | 10 | |+----------+----------------------+------+-----+---------+----------------+6 rows in set (0.00 sec) 添加多列：（不能指定位置关系） 1ALTER TABLE table_name ADD [COLUMN] (col_name column_definition,...) 删除列 1ALTER TABLE table_name DROP [COLUMN] (col_name) 如： 12345678910111213mysql&gt; ALTER TABLE user1 DROp truename;Query OK, 0 rows affected (0.69 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; SHOW COLUMNS FROM user1;+----------+----------------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+----------+----------------------+------+-----+---------+----------------+| id | smallint(5) unsigned | NO | PRI | NULL | auto_increment || username | varchar(10) | NO | | NULL | || password | varchar(20) | NO | | NULL | || pid | smallint(5) unsigned | YES | MUL | NULL | || age | tinyint(3) unsigned | NO | | 10 | |+----------+----------------------+------+-----+---------+----------------+ 删除多列 1ALTER TABLE table_name DROP col_name1,DROP col_name2,... 删除同时添加 1ALTER TABLE table_name DROP col_name1,ADD col_name2,... 修改数据表——添加\\删除约束**添加主键约束： 1ALTER TABLE table_name ADD[CONSTRAINT[symbol]] PRIMARY KEY [index_type] (index_col_name,...) 可选项：CONSTRAINT[symbol] 可以为主键设定名字，index_type指定索引类型 如： 1234567891011121314mysql&gt; CREATE TABLE user2( -&gt; username VARCHAR(10) NOT NULL, -&gt; pid SMALLINT UNSIGNED -&gt; );mysql&gt; ALTER TABLE user2 ADD id SMALLINT UNSIGNED;mysql&gt; ALTER TABLE user2 ADD CONSTRAINT PK_user2_id PRIMARY KEY(id);mysql&gt; SHOW COLUMNS FROM user2;+----------+----------------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+----------------------+------+-----+---------+-------+| username | varchar(10) | NO | | NULL | || pid | smallint(5) unsigned | YES | | NULL | || id | smallint(5) unsigned | NO | PRI | NULL | |+----------+----------------------+------+-----+---------+-------+ 添加唯一约束 1ALTER TABLE table_name ADD[CONSTRAINT[symbol]] UNIQUE [INDEX|KEY] [index_name][index_type] (index_col_name,...) 如： 123456789mysql&gt; ALTER TABLE user2 ADD UNIQUE (username);mysql&gt; SHOW COLUMNS FROM user2;+----------+----------------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+----------------------+------+-----+---------+-------+| username | varchar(10) | NO | UNI | NULL | || pid | smallint(5) unsigned | YES | | NULL | || id | smallint(5) unsigned | NO | PRI | NULL | |+----------+----------------------+------+-----+---------+-------+ 添加外键约束 1ALTER TABLE table_name ADD[CONSTRAINT[symbol]] FOREIGN KEY [index_name] (index_col_name,...) reference_definition 如： 1234567891011mysql&gt; ALTER TABLE user2 ADD FOREIGN KEY (pid) PEFERENCES provinces(id);mysql&gt; SHOW CREATE TABLE user2;| user2 | CREATE TABLE `user2` ( `username` varchar(10) NOT NULL, `pid` smallint(5) unsigned DEFAULT NULL, `id` smallint(5) unsigned NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `username` (`username`), KEY `pid` (`pid`), CONSTRAINT `user2_ibfk_1` FOREIGN KEY (`pid`) REFERENCES `provinces` (`id`)) ENGINE=InnoDB DEFAULT CHARSET=latin1 | 添加/删除默认约束 1ALTER TABLE table_name ALTER[COLUMN] col_name &#123;SET DEFAULT literal|DROP DEFAULT&#125; 如添加约束： 1234567891011mysql&gt; ALTER TABLE user2 ADD age TINYINT UNSIGNED NOT NULL;mysql&gt; ALTER TABLE user2 ALTER age SET DEFAULT 15;mysql&gt; SHOW COLUMNS FROM user2;+----------+----------------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+----------------------+------+-----+---------+-------+| username | varchar(10) | NO | UNI | NULL | || pid | smallint(5) unsigned | YES | MUL | NULL | || id | smallint(5) unsigned | NO | PRI | NULL | || age | tinyint(3) unsigned | NO | | 15 | |+----------+----------------------+------+-----+---------+-------+ 删除约束： 12345678910mysql&gt; ALTER TABLE user2 ALTER age DROP DEFAULT;mysql&gt; SHOW COLUMNS FROM user2;+----------+----------------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+----------------------+------+-----+---------+-------+| username | varchar(10) | NO | UNI | NULL | || pid | smallint(5) unsigned | YES | MUL | NULL | || id | smallint(5) unsigned | NO | PRI | NULL | || age | tinyint(3) unsigned | NO | | NULL | |+----------+----------------------+------+-----+---------+-------+ 删除主键约束 1ALTER TABLE table_name DROP PRIMARY KEY 如： 12345678910mysql&gt; ALTER TABLE user2 DROP PRIMARY KEY;mysql&gt; SHOW COLUMNS FROM user2;+----------+----------------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+----------------------+------+-----+---------+-------+| username | varchar(10) | NO | PRI | NULL | || pid | smallint(5) unsigned | YES | MUL | NULL | || id | smallint(5) unsigned | NO | | NULL | || age | tinyint(3) unsigned | NO | | NULL | |+----------+----------------------+------+-----+---------+-------+ 删除唯一约束 1ALTER TABLE table_name DROP &#123;INDEX|KEY&#125; index_name 指定索引名称是因为一张表可以有多个唯一约束，需要删除字段上的约束而非字段本身就要知道约束名称加以限定。 查看索引名称： 1mysql&gt; SHOW INDEXES FROM user2\\G 得到 唯一约束column的Key_name: username 删除约束： 1mysql&gt; ALTER TABLE user2 DROP INDEX username; 查看数据表： 123456789mysql&gt; SHOW COLUMNS FROM user2;+----------+----------------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+----------------------+------+-----+---------+-------+| username | varchar(10) | NO | | NULL | || pid | smallint(5) unsigned | YES | MUL | NULL | || id | smallint(5) unsigned | NO | | NULL | || age | tinyint(3) unsigned | NO | | NULL | |+----------+----------------------+------+-----+---------+-------+ 唯一约束已删除 删除外键约束 1mysql&gt; ALTER TABLE user2 DROP FOREIGN KEY fk_symbol; 需查看外键的fk_symbol（系统指定）： 1234mysql&gt; SHOW CREATE TABLE user2;得到：KEY `pid` (`pid`), CONSTRAINT `user2_ibfk_1` FOREIGN KEY (`pid`) REFERENCES `provinces` (`id`) 即：fk_symbol为‘user2_ibfk_1’ 删除约束： 1mysql&gt; ALTER TABLE user2 DROP FOREIGN KEY user2_ibfk_1; 还可以继续删除索引： 123mysql&gt; ALTER TABLE user2 DROP INDEX pid;mysql&gt; SHOW INDEXES FROM user2\\GEmpty set (0.00 sec) 修改列定义和更名数据表修改列定义 1ALTER TABLE table_name MODIFY col_name column_definition [FIRST|AFTER col_name] 备选项：可以修改字段位置，如： 12345678910mysql&gt; ALTER TABLE user2 MODIFY id SMALLINT UNSIGNED NOT NULL FIRST;mysql&gt; SHOW COLUMNS FROM user2;+----------+----------------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+----------------------+------+-----+---------+-------+| id | smallint(5) unsigned | NO | | NULL | || username | varchar(10) | NO | | NULL | || pid | smallint(5) unsigned | YES | | NULL | || age | tinyint(3) unsigned | NO | | NULL | |+----------+----------------------+------+-----+---------+-------+ 修改字段数据类型： 12345678910mysql&gt; ALTER TABLE user2 MODIFY id TINYINT UNSIGNED NOT NULL;mysql&gt; SHOW COLUMNS FROM user2;+----------+----------------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+----------------------+------+-----+---------+-------+| id | tinyint(3) unsigned | NO | | NULL | || username | varchar(10) | NO | | NULL | || pid | smallint(5) unsigned | YES | | NULL | || age | tinyint(3) unsigned | NO | | NULL | |+----------+----------------------+------+-----+---------+-------+ 但注意：将数据类型修改为更小类型时可能会造成数据丢失。 修改列名称： 1ALTER TABLE table_name CHANGE old_col_name new_col_name column_definition [FIRST|AFTER col_name] 即可修改列名称，也可修改列定义。 如： 12345678910mysql&gt; ALTER TABLE user2 CHANGE pid p_id TINYINT UNSIGNED NOT NULL;mysql&gt; SHOW COLUMNS FROM user2;+----------+---------------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+---------------------+------+-----+---------+-------+| id | tinyint(3) unsigned | NO | | NULL | || username | varchar(10) | NO | | NULL | || p_id | tinyint(3) unsigned | NO | | NULL | || age | tinyint(3) unsigned | NO | | NULL | |+----------+---------------------+------+-----+---------+-------+ 数据表更名 123ALTER TABLE table_name RENAME[TO|AS] new_table_name或：RENAME TABLE table_name TO new_table_name 如： 1234567891011121314151617181920212223242526272829303132mysql&gt; ALTER TABLE user2 RENAME user3;mysql&gt; SHOW TABLES;+-----------------+| Tables_in_lcyDB |+-----------------+| provinces || tb1 || tb2 || tb3 || tb4 || tb5 || tb6 || user1 || user3 || users |+-----------------+mysql&gt; ALTER TABLE user3 RENAME user2;mysql&gt; SHOW TABLES;+-----------------+| Tables_in_lcyDB |+-----------------+| provinces || tb1 || tb2 || tb3 || tb4 || tb5 || tb6 || user1 || user2 || users |+-----------------+ 注意： 要尽量少使用列和表的更名，如果之前创建了索引或视图，引用了表名或列名，修改名称可能会导致视图或存储过程无法正常工作。","tags":[]},{"title":"MySQL-学习笔记-2","date":"2017-04-13T10:12:57.000Z","path":"2017/04/13/MySQL-学习笔记-2/","text":"数据类型及数据表操作数据类型数据类型决定存储格式 要根据实际应用来选择最合适的数据类型 整型 浮点型 日期时间型 通常用数字类型取代日期时间或进行时间戳转换 字符型 注： CHAR(M)为定长类型 VARCHAR 为变长类型 L+1或L+2里多出来的字节用来保存数据值长度 ENUM表示从枚举的值中选择其一 SET表示从成员中进行排列组合形成其值 数据表操作创建数据表数据表是数据库中其他对象的基础 关系性数据库——二维表格——数据表 行——记录 列——字段 打开数据库 USE 数据库名称; SELECT DATABASE()； 显示当前使用的数据库 创建数据表 CREATE TABLE table_name( ​ column_name data_type, ​ …… ) e.g. 123456&gt; mysql&gt; CREATE TABLE tb1(&gt; -&gt; username VARCHAR(20),&gt; -&gt; age TINYINT UNSIGNED,&gt; -&gt; salary FLOAT(8,2) UNSIGNED &gt; -&gt; );&gt; &gt; Query OK, 0 rows affected (0.35 sec) 查看数据表 **SHOW TABLES[FROM db_name] [LIKE ‘pattern’| WHERE expr] 注：此命令可查询其他数据库中的表且不改变当前所处数据库，如： mysql&gt; show tables from mysql; +—————————+ | Tables_in_mysql |+—————————+| columns_priv || db || engine_cost || event | 第二行涉及通配符，暂略 查看数据表结构 SHOW COLUMNS FROM table_name; 如： mysql&gt; show columns from tb1;+———-+———————+——+—–+———+——-+| Field | Type | Null | Key | Default | Extra |+———-+———————+——+—–+———+——-+| username | varchar(20) | YES | | NULL | || age | tinyint(3) unsigned | YES | | NULL | || salary | float(8,2) unsigned | YES | | NULL | |+———-+———————+——+—–+———+——-+3 rows in set (0.01 sec) 记录插入与查找插入记录 INSERT [INTO] tb1_name [(col_name,…)] VALUES(val,…) 如： mysql&gt; INSERT tb1 VALUES(‘TOM’,25,7825.37);Query OK, 1 row affected (0.07 sec) mysql&gt; INSERT tb1(username,salary) VALUES(‘Jphn’,6825.37);Query OK, 1 row affected (0.04 sec) 查找记录 SELECT expr,… FROM table_name 如： mysql&gt; SELECT * FROM tb1;+———-+——+———+| username | age | salary |+———-+——+———+| TOM | 25 | 7825.37 || Jphn | NULL | 6825.37 |+———-+——+———+ 注：*是对字段的过滤而非对记录的过滤 空值与非空创建表格时指定某字段的值是否可以为空： NULL,字段值可以为空 NOT NULL，字段值禁止为空 mysql&gt; CREATE TABLE tb2(-&gt; username VARCHAR(20) NOT NULL,-&gt; age TINYINT UNSIGNED NULL-&gt; ); mysql&gt; SHOW COLUMNS FROM tb2;+———-+———————+——+—–+———+——-+| Field | Type | Null | Key | Default | Extra |+———-+———————+——+—–+———+——-+| username | varchar(20) | NO | | NULL | || age | tinyint(3) unsigned | YES | | NULL | |+———-+———————+——+—–+———+——-+ mysql&gt; INSERT tb2 VALUES(NULL,26);ERROR 1048 (23000): Column ‘username’ cannot be null 自动编号保证某条记录的唯一性——记录自动编号 即自动编号字段必须定义为数值类型，且定义为主键 如设计错误的定义： mysql&gt; CREATE TABLE tb3( -&gt; -&gt;id SMALLINT UNSIGNED AUTO_INCREMENT, -&gt; username VARCHAR(30) NOT NULL -&gt;); ERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key 初涉主键约束 如修改上述记录： 123mysql&gt; CREATE TABLE tb3(-&gt; id SMALLINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,-&gt; username VARCHAR(30) NOT NULL -&gt; ); mysql&gt; SHOW COLUMNS FROM tb3;+———-+———————-+——+—–+———+—————-+| Field | Type | Null | Key | Default | Extra |+———-+———————-+——+—–+———+—————-+| id | smallint(5) unsigned | NO | PRI | NULL | auto_increment || username | varchar(30) | NO | | NULL | |+———-+———————-+——+—–+———+—————-+ 自动编号示例： mysql&gt; INSERT tb3(username) VALUES(‘XueXue’);Query OK, 1 row affected (0.05 sec) mysql&gt; INSERT tb3(username) VALUES(‘TuTu’);Query OK, 1 row affected (0.06 sec) mysql&gt; INSERT tb3(username) VALUES(‘JingJing’);Query OK, 1 row affected (0.02 sec) mysql&gt; INSERT tb3(username) VALUES(‘GaiGai’);Query OK, 1 row affected (0.05 sec) mysql&gt; SELECT * FROM tb3;+—-+———-+| id | username |+—-+———-+| 1 | XueXue || 2 | TuTu || 3 | JingJing || 4 | GaiGai |+—-+———-+ 主键不一定要定义为自动编号，如： mysql&gt; CREATE TABLE tb4(-&gt; id SMALLINT UNSIGNED PRIMARY KEY,-&gt; username VARCHAR(20) NOT NULL-&gt; );Query OK, 0 rows affected (0.36 sec) mysql&gt; SHOW COLUMNS FROM tb4;+———-+———————-+——+—–+———+——-+| Field | Type | Null | Key | Default | Extra |+———-+———————-+——+—–+———+——-+| id | smallint(5) unsigned | NO | PRI | NULL | || username | varchar(20) | NO | | NULL | |+———-+———————-+——+—–+———+——-+ 主键字段具有唯一性： mysql&gt; INSERT tb4 VALUES(22,’Tom’);Query OK, 1 row affected (0.05 sec) mysql&gt; INSERT tb4 VALUES(22,’John’);ERROR 1062 (23000): Duplicate entry ‘22’ for key ‘PRIMARY’ 初涉唯一约束 第二三条看似相悖，实际上一张表只允许存在一个值为空的唯一约束字段 创建示例： 12345mysql&gt; CREATE TABLE tb5(-&gt; id SMALLINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,-&gt; username VARCHAR(20) NOT NULL UNIQUE KEY,-&gt; age TINYINT UNSIGNED-&gt; ); mysql&gt; SHOW COLUMNS FROM tb5;+———-+———————-+——+—–+———+—————-+| Field | Type | Null | Key | Default | Extra |+———-+———————-+——+—–+———+—————-+| id | smallint(5) unsigned | NO | PRI | NULL | auto_increment || username | varchar(20) | NO | UNI | NULL | || age | tinyint(3) unsigned | YES | | NULL | |+———-+———————-+——+—–+———+—————-+ 验证唯一性： mysql&gt; INSERT tb5(username,age) VALUES(‘John’,22);Query OK, 1 row affected (0.05 sec) mysql&gt; INSERT tb5(username,age) VALUES(‘John’,22);ERROR 1062 (23000): Duplicate entry ‘John’ for key ‘username’ 初涉默认约束DEFAULT 创建： mysql&gt; CREATE TABLE tb6(-&gt; id SMALLINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,-&gt; username VARCHAR(20) NOT NULL UNIQUE KEY,-&gt; sex ENUM(‘1’,’2’,’3’) DEFAULT ‘3’-&gt; ); 查看： mysql&gt; SHOW COLUMNS FROM tb6;+———-+———————-+——+—–+———+—————-+| Field | Type | Null | Key | Default | Extra |+———-+———————-+——+—–+———+—————-+| id | smallint(5) unsigned | NO | PRI | NULL | auto_increment | username varchar(20) NO UNI NULL | sex | enum(‘1’,’2’,’3’) | YES | | 3 | |+———-+———————-+——+—–+———+—————-+ 验证默认约束： mysql&gt; INSERT tb6(username) VALUES(‘Tom’);Query OK, 1 row affected (0.05 sec) mysql&gt; SELECT * FROM tb6;+—-+———-+——+| id | username | sex |+—-+———-+——+| 1 | Tom | 3 |+—-+———-+——+","tags":[]},{"title":"MySQL-学习笔记-1","date":"2017-04-10T16:17:20.000Z","path":"2017/04/11/MySQL-学习笔记-1/","text":"MySQL自启动sudo chkconfig –add mysql 添加服务 netstat -na | grep 3306 看到有监听说明服务启动 或：ps -ef | grep mysqld 检查MySQL服务器是否启动 另： sudo /etc/init.d/mysql start 启动MySQL 服务器 sudo /etc/init.d/mysql stop 或 ./mysqladmin -u root -p shutdown 关闭目前运行的 MySQL 服务器（/usr/bin 目录下） MySQL添加用户、删除用户与授权登录MYSQL： >mysql -u root -p 创建用户： >CREATE USER ‘username’@’localhost’ IDENTIFIED BY ‘password’; 或 INSERT INTO user (host, user,ssl_cipher,x509_issuer,x509_subject) VALUES (‘localhost’, ‘test’,’’,’’,’’); use mysql; UPDATE user SET authentication_string=PASSWORD(‘*‘) WHERE user=’test’; FLUSH PRIVILEGES; 为用户授权: 以ROOT身份登录 >mysql -u root -p 为用户创建一个数据库: >create database *DB; 授权新用户拥有***DB数据库的所有权限: >grant all privileges on ***DB. to \\**@localhost identified by ‘*(password)’; 刷新系统权限表: >flush privileges; 授权新用户拥有所有数据库的某些权限： >grant select,delete,update,create,drop on . to ***@localhost identified by “*“; 查看MYSQL数据库中所有用户: > SELECT DISTINCT CONCAT(‘User: ‘’’,user,’’’@’’’,host,’’’;’) AS query FROM mysql.user; 查看数据库中具体某个用户的权限: >show grants for ‘*‘@’localhost’; 修改MySQL提示符：登录时： mysql -u -p –prompt 提示符 已连接客户端时: >prompt 提示符; 部分参数： 常用命令及语法规范 操作数据库创建数据库： >CREATE DATABASE db_name; 全部参数： 存在warning信息时查看： >SHOW WARNINGS; 查看创建时编码方式： >SHOW CREATE DATABASE db_name; 查看（当前服务器下）数据库列表： >SHOW DATABASES; 修改数据库： 删除数据库： >DROP DATABASE db_name;","tags":[]},{"title":"t检验笔记","date":"2017-04-09T08:17:21.000Z","path":"2017/04/09/t检验笔记/","text":"t检验_第二部分效应量（effect size）实验性研究中，表示处理效应的大小 非实验性研究中，表示变量之间关系强度 t/z检验中效应量衡量指标——均值差异（mean difference） 效应量测量类型差异度量​ 均值差异 ​ 标准化差异度量（standardized differens）——cohen‘s d 相关度量​ $$r^2$$ 表示某个变量的变化比例与另一个变量的关系（一个变量能够“解释”另一个变量的特定变化的比例） 统计显著性在统计学中，“显著性”是指—— rejected the null results are not likely due to chance（sampling error） 即：我们在解释结果时，排除了随机因素/抽样错误 判断调查研究结果是否有意义： 变量是否具有实际意义或社会、理论意义 效应量大小 能否排除抽样错误 能否排除结果对立解释（排除潜在变量影响） Cohen‘s d【标准化均值差异（standardized mean difference）】$$d=\\frac{\\bar x-\\mu}{s}$$ $$x 表示样本均值 ,s表示样本标准差$$ $r^2$ （确定系数）​ 相关度量 表示两个变量之间的关系程度 ​ $$r^2$$ 范围：0~1（数值越大相关性越强） ​ t检验 中计算 $$r^2$$ : ​ $$r^2=\\frac{t^2}{t^2+ df}$$ t为在t检验中获得的值，df为自由度 报告结果描述统计量（文，图，表） 推论统计量（假设检验、置信区间） APA style ​ $$t（df）=x.xx,p=x.xx,direction$$ ​ e.g. $$t(24)=-2.50,p&lt;0.05,one-tailed$$ 报告置信区间：（注明是什么的置信区间） ​ e.g. confidence interval on the mean difference;95% CI=(4,6) 报告效应量： e.g. $$d=x.xx$$ ​ $$r^2=.xx$$ (通常不写0) 完整的单样本t检验t检验置信区间Cohen‘s dr^2公式： ​ $$df=n-1$$ ​ $$SEM(均值标准误差)=\\frac{S}{\\sqrt{n}}$$ ​ $$t=\\frac{x-\\mu}{SEM}$$ ​ $$CI =x+/-margin\\ of\\ error $$ ​ $$margin\\ of\\ error=t_crttical*SEM$$ ​ $$Cohen’s\\ d=\\frac{x-\\mu}{S}$$ ​ $$r^2=\\frac{t^2}{t^2+ df}$$ 过程 自变量、因变量 处理方式 零假设、对立假设 哪种尾检验 自由度 t临界值$$(\\alpha 水平)$$ SEM 均值误差（$$x-\\mu$$） t统计量 t是否在临界区 p值范围（是否大于置信水平） 统计显著性（是/否） 结果是否有意义 Cohen’s d $$r^2$$ 误差范围 置信区间 ​","tags":[]},{"title":"Hello World","date":"2017-04-08T16:00:00.000Z","path":"2017/04/09/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]