---
title: 机器学习入门(1) 朴素贝叶斯
date: 2017-05-19 10:07:47
tags:
categories: 机器学习
---

#### 简介

机器学习实际应用：

- Netflix影片推荐
- 信用卡欺诈预防
- Google搜索服务、页面排名、语音识别、翻译、无人驾驶等
- 精准购物推荐

<!---more--->

### 朴素贝叶斯

#### 关于监督分类（supervised classification）

案例：无人驾驶

supervised——有正确的样本用于监督学习、训练

训练——不通过编写规则来完成训练，而是向学习对象展示正确的示例使之完成模仿——复制智能行为

#### 监督分类示例

Machine Learning：从实例中进行学习

对于一个分类方式不给出其确切概念，而是给出大量案例，从案例的许多特征和属性中挑出正确的分类特征，对新的案例进行分类。

可以用监督分类方式解决的问题：

- 从许多加标记的照片中，识别某个特定的人
- 推荐系统，通过分析用户喜欢的事物，找出这些事物的特征、属性，来推荐更多具有类似属性的事物

不属于监督分类范畴的问题：

- 分析银行数据，找出涉嫌诈骗的交易（未给出异常交易的明确定义，没有例子来描述其特征）
- 聚类：对学生学习风格进行分类（不知道具体有多少个类型和已明确知道类型的样本数据）

#### 特征和标签

在机器学习中，通常会把特征作为输入，然后尝试生成标签

类似听一段音乐，提取所谓的特征，如：流派，歌曲节奏、强度等信息，大脑将其处理为两个类别：喜欢和不喜欢

#### Stanley地形分类

运用监督学习训练驾驶机器人的速度控制——通过观察某些特征来完成速度的调节

特征选取：地形坡度、地形平整性

可以利用二维散点图将路况条件转化为独立的散点，并找到对应的位置

#### 定义决策面（decision surface）

决策面通常位于两个不同的类之间的某个位置上，将用来预测数据点属于哪个分类。（分类——>泛化）

良好的线性决策面：决策面为直线，且可以适用于之后所有情况，帮助确定之后所有数据的分类。

#### 朴素贝叶斯（naive bayes）

常见的寻找决策面的算法

可以使用 Python scikit-learn（sklearn）库 高斯朴素贝叶斯（GaussianNB）编写分类器

```
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
Y = np.array([1, 1, 1, 2, 2, 2])
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB() #创建分类器
clf.fit(X, Y)  #提供训练数据（特征值x和标签y），进行训练
clf.predict([[-0.8, -1]])  #让已完成训练的分类器对新数据进行预测
```

Out：

```
array([1])
```

评估朴素贝叶斯分类器效果——计算预测的准确率

- 对比预测结果集与实际测试集特征值进行比较：

```
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(pred,labels_test)
```

- 直接使用分类器的方法计算：

```
clf.score(features_test, labels_test)
```

#### 贝叶斯法则（Bayes rule）【概率推理】

##### 先验概率与后验概率

例：

假设有一种癌症，发生率为人口的1%.

若患癌，检查结果90%呈阳性。若未患癌，检查结果90%呈阴性。

若进行检查结果呈阳性，计算患这种癌症的可能性。

先验概率（prior probability）检验之前得到的概率 （如人群中的癌症发病率）

后验概率  （posterior probability） 经过测试中某些证据的引导后得到的概率 （如经过检查后呈阴性或阳性，此时的患病概率）

Prior：

$$P(C) =0.01 \ \  P(ℸC)=0.99$$

检测敏感性：

$$P(Pos|C)=0.9$$

$$ P(Pos|ℸC)=0.1$$

Joint(联合概率)：

$$P(C, Pos) = P(C) • P(Pos|C)=0.009$$

$$P(ℸC, Pos) = P(ℸC)• P(Pos|ℸC)=0.099$$

归一化：

$$P(Pos)=P(C, Pos) +P(ℸC, Pos) =0.108$$

Posterior:

$$P(C|Pos)=\frac{P(C, Pos)}{P(Pos)}=0.0833$$

$$P(ℸC|Pos)=\frac{P(ℸC, Pos)}{P(Pos)}=0.9167$$

全概率：

$$P(C|Pos)+P(ℸC|Pos)=1$$

#### 用于分类的贝叶斯法则

如用于Text Learning  朴素贝叶斯

例：有两个人Chris和Sara，他们发送邮件的主题均分别为”love“，”Deal“，”life“，频率为：

Chris：

love	  0.1		deal  0.8		life  0.1

Sara:

love	  0.5		deal  0.2		life  0.3

现在一封邮件，提到Deal和life，预测这封邮件由谁发出。

假定先验概率两人各为50%

Joint:

$$P(Chris,(deal,life))=P(Chris) • P((deal,life)|Chris)=0.5*0.8*0.1=0.04$$

$$P(Sara,(deal,life))=P(Sara) • P((deal,life)|Sara)=0.5*0.2*0.3=0.03$$

Posterior:

$$P(Chris|(deal,life))=\frac{P(Chris,(deal,life))}{P(deal,life)}=0.04/0.07=0.571$$

$$P(Sara|(deal,life))=\frac{P(Sara,(deal,life))}{P(deal,life)}=0.03/0.07=0.429$$

#### "naive"的来源

naive bayes：

已知目标标签的词语使用频率，提供一些证据（使用了哪些词汇），计算得出词汇使用者应属于哪个标签的概率

之所以称为naive，是因为计算概率时，忽略了词序。（并没有真正理解文本，只将词频作为分类方式）

#### 朴素贝叶斯的优势和劣势

优点：易于执行、特征空间大（容纳更多词汇数据样本）

不足：有间断（break）有多个单词组成且意义明显不同的短语不适用